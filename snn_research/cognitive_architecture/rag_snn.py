# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/rag_snn.py
#
# Phase 3: RAG-SNN (Retrieval-Augmented Generation) ã‚·ã‚¹ãƒ†ãƒ 
#
# æ”¹å–„ç‚¹ (v2):
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º3ã€Œå› æœãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã€ã«åŸºã¥ãã€
#   `add_causal_relationship`ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚
#   ã“ã‚Œã«ã‚ˆã‚Šã€ã€ŒA causes Bã€ã®ã‚ˆã†ãªå› æœé–¢ä¿‚ã‚’ã‚ˆã‚Šæ˜ç¢ºã«è¡¨ç¾ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

import os
from typing import List, Optional
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class RAGSystem:
    """
    å¤–éƒ¨çŸ¥è­˜ï¼ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼‰ã¨å†…éƒ¨è¨˜æ†¶ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ­ã‚°ï¼‰ã‚’æ¤œç´¢ã—ã€
    æ€è€ƒã®ãŸã‚ã®æ–‡è„ˆã‚’æä¾›ã™ã‚‹RAGã‚·ã‚¹ãƒ†ãƒ ã€‚
    ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã¨ã—ã¦ã®æ©Ÿèƒ½ã‚‚ä½µã›æŒã¤ã€‚
    """
    def __init__(self, vector_store_path: str = "runs/vector_store"):
        self.vector_store_path = vector_store_path
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2"
        )
        self.vector_store: Optional[FAISS] = self._load_vector_store()

    def _load_vector_store(self) -> Optional[FAISS]:
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        if os.path.exists(self.vector_store_path):
            print(f"ğŸ“š æ—¢å­˜ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {self.vector_store_path}")
            return FAISS.load_local(self.vector_store_path, self.embedding_model, allow_dangerous_deserialization=True)
        return None

    def setup_vector_store(self, knowledge_dir: str = "doc", memory_file: str = "runs/agent_memory.jsonl"):
        """
        çŸ¥è­˜æºã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ãƒ»ä¿å­˜ã™ã‚‹ã€‚
        """
        print("ğŸ› ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ã‚’é–‹å§‹ã—ã¾ã™...")
        
        doc_loader = DirectoryLoader(knowledge_dir, glob="**/*.md", loader_cls=TextLoader, silent_errors=True)
        txt_loader = DirectoryLoader(knowledge_dir, glob="**/*.txt", loader_cls=TextLoader, silent_errors=True)
        docs = doc_loader.load() + txt_loader.load()

        if os.path.exists(memory_file):
            memory_loader = TextLoader(memory_file)
            docs.extend(memory_loader.load())
        
        if not docs:
            print("âš ï¸ çŸ¥è­˜æºã¨ãªã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        split_docs = text_splitter.split_documents(docs)

        print(f"ğŸ“„ {len(split_docs)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™...")
        self.vector_store = FAISS.from_documents(split_docs, self.embedding_model)
        
        os.makedirs(os.path.dirname(self.vector_store_path), exist_ok=True)
        self.vector_store.save_local(self.vector_store_path)
        print(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®æ§‹ç¯‰ãŒå®Œäº†ã—ã€'{self.vector_store_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def search(self, query: str, k: int = 3) -> List[str]:
        """
        ã‚¯ã‚¨ãƒªã«æœ€ã‚‚é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’æ¤œç´¢ã™ã‚‹ã€‚
        """
        if self.vector_store is None:
            print("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…ˆã« setup_vector_store() ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            self.setup_vector_store()
            if self.vector_store is None:
                 return ["ã‚¨ãƒ©ãƒ¼: ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"]

        results = self.vector_store.similarity_search(query, k=k)
        return [doc.page_content for doc in results]

    def add_relationship(self, source_concept: str, relation: str, target_concept: str):
        """
        æ¦‚å¿µé–“ã®é–¢ä¿‚æ€§ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ï¼‰ã«è¿½åŠ ã™ã‚‹ã€‚
        """
        if self.vector_store is None:
            print("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
            self.vector_store = FAISS.from_texts([], self.embedding_model)

        relationship_text = f"Concept Relation: {source_concept} {relation} {target_concept}."
        
        doc = Document(page_content=relationship_text, metadata={"source": "internal_knowledge"})
        self.vector_store.add_documents([doc])
        
        self.vector_store.save_local(self.vector_store_path)
        print(f"ğŸ“ˆ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ›´æ–°: ã€Œ{relationship_text}ã€")

    # --- â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†“ã“ã“ã‹ã‚‰ãŒé‡è¦â†“â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸ ---
    def add_causal_relationship(self, cause: str, effect: str, condition: Optional[str] = None):
        """
        æ¦‚å¿µé–“ã®å› æœé–¢ä¿‚ã‚’ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã«è¿½åŠ ã™ã‚‹ã€‚

        Args:
            cause (str): åŸå› ã¨ãªã£ãŸã‚¤ãƒ™ãƒ³ãƒˆè¨˜è¿°ã€‚
            effect (str): çµæœã¨ã—ã¦ç”Ÿã˜ãŸã‚¤ãƒ™ãƒ³ãƒˆè¨˜è¿°ã€‚
            condition (Optional[str]): å› æœé–¢ä¿‚ãŒæˆç«‹ã—ãŸæ–‡è„ˆãƒ»çŠ¶æ³ã€‚
        """
        if self.vector_store is None:
            # ãƒ©ã‚¤ãƒ–ã§ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ç©ºã®ã‚¹ãƒˆã‚¢ã‚’åˆæœŸåŒ–
            self.vector_store = FAISS.from_texts([], self.embedding_model)

        if condition:
            causal_text = f"Causal Relation: Under condition '{condition}', the event '{cause}' leads to the effect '{effect}'."
        else:
            causal_text = f"Causal Relation: The event '{cause}' directly leads to the effect '{effect}'."
        
        doc = Document(page_content=causal_text, metadata={"source": "causal_inference"})
        self.vector_store.add_documents([doc])
        self.vector_store.save_local(self.vector_store_path)
        print(f"ğŸ”— å› æœé–¢ä¿‚ã‚’è¨˜éŒ²: ã€Œ{causal_text}ã€")
    # --- â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â†‘ã“ã“ã¾ã§ãŒé‡è¦â†‘â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸â—¾ï¸ ---