# **SNN5 高度化ロードマップ (v10.2): SOTA精度と生物学的パラダイムの融合**

## **I. 基本方針**

SNN5プロジェクトをSOTA（State-of-the-Art）レベルのAIフレームワークへと強化する。

プロジェクトの設計思想である「非GPU依存」「高効率」を追求しつつ、ANN（人工ニューラルネットワーク）に匹敵する「高精度」を達成するため、以下の戦略的実装計画を推進する。

1. **学習基盤 (SG Track):** 実証済みの「代理Gradient（Surrogate Gradient: SG）法」を標準学習基盤として採用し、エンドツーエンドの直接訓練を可能にする (強化案 1.2)。  
2. **アーキテクチャ (SG Track):** ResNet, TransformerといったSOTAアーキテクチャをSNNネイティブに実装し、精度を追求する (強化案 2.1, 5.1)。  
3. **効率化 (All Tracks):** 精度を維持しつつ、SNN固有の利点（低レイテンシ・低消費電力）を最大化するため、スパース化、量子化、プルーニング、動的推論を徹底的に実装する (強化案 4.1-4.4, 6.1)。  
4. **ベンチマーク (All Tracks):** 評価基盤をSpikingJelly 24 に準拠させ、ニューロモーフィック・データセット（CIFAR10-DVS, SHD）での性能を標準指標とする (強化案 7.1)。  
5. **生物学的パラダイム (Bio Track):** ユーザー提供の実験コード (rsnn\_experiments\_fixed.py 等) で有効性が示された「微分フリー」の生物学的学習則（R-STDP, Homeostasis, LSM）を、PyTorch (nn.Module) ベースで正式に実装し、SG Trackと並行して開発する。

## **II. 実装ロードマップ (v10.2)**

### **フェーズ1: 高精度アーキテクチャの実装 (SG Track)**

SNNの深層化と、Transformerアーキテクチャの導入により、ANN SOTAモデルに匹敵する精度基盤を構築する。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P1.1** | **深層残差SNNの実装** | 勾配消失を防ぎ、100層以上の深層SNNを可能にする残差接続（Residual Connection）を実装する。 | **SEW-ResNet** 9 | 2.1 | 計画中 |
| **P1.2** | **イベント駆動型Transformer (HW Track)** | SEW-ResNetが持つ「非スパイク計算」の問題を解決する、純粋なイベント駆動型Transformerを実装する。 | **Spikingformer** 9 | 5.1 | 計画中 |
| **P1.3** | **ハイブリッドTransformer (GPU Track)** | ANNのAttentionとSNNのFFNを組み合わせた、迅速デプロイ可能なハイブリッドTransformerを実装する。 | **HsVT** 53 | 5.2 | 計画中 |
| **P1.4** | **時空間アテンション** | SNN固有の時間情報を活用するため、スパイクベースの時空間アテンション・モジュールを実装する。 | **DTA-SNN** 10, **STAA-SNN** 11 | 2.2 | 計画中 |

### **フェーズ2: ニューロン・ダイナミクスとコーディングの強化 (All Tracks)**

ニューロンモデル自体と入力コーディング方式を高度化し、ネットワークの表現力と効率を向上させる。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P2.1** | **学習可能なニューロン (PLIF)** | 膜時定数($\\tau$)を学習可能なパラメータとし、各ニューロンが最適な時間スケールを自動獲得できるようにする。 | **PLIF** 7 | 3.1 | 計画中 |
| **P2.2** | **学習可能なニューロン (GLIF)** | $\\tau$や閾値を入力依存のゲートで動的に制御する、より高表現力なニューロンモデルを実装する。 | **GLIF** 8 | 3.1 | 計画中 |
| **P2.3** | **時間コーディングの最適化** | レートコーディングから、高速・高効率なTTFS（Time-to-First-Spike）コーディングへ移行する。 **根拠:** rsnn\_latency\_burst\_comparison.json で、時間符号化がレート符号化に対し、**精度低下を最小限に抑えつつ発火率を50%以上削減**できる高効率性が実証されたため。 | **TTFS** 30, 84 | 3.2 | 計画中 |

### **フェーズ3: SOTAモデルのHPOチューニング (SG Track) (v10.2 新規)**

フェーズ1で実装したSOTAアーキテクチャの精度を最大化（目標①: 95-97%）するため、徹底的なパラメータ探索を行う。

| ID | タスク | 概要 | SOTA技術 | 関連(SNN5) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P3.1** | **SOTAアーキテクチャのHPO** | フェーズ1 (P1.1, P1.2等) で実装したモデルに対し、run\_hpo.py を用いて大規模なハイパーパラメータ最適化（学習率、膜時定数、代理勾配の形状 $\\alpha$ など）を実行する。 | **Optuna** | run\_hpo.py | 計画中 |

### **フェーズ4: 省エネ化（エネルギー効率）の徹底追求 (SG Track)**

「高精度」と「高スパース性（低スパイク率）」「低タイムステップ」を両立させ、真の省エネモデルを実現する。(旧P3)

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P4.1** | **スパース性正則化** | 損失関数にL1正則化や平均スパイク率ペナルティを導入し、スパースな発火（省エネ）を強制する。 | スパース性正則化 12, 13 | 4.2 | 計画中 |
| **P4.2** | **量子化アウェア学習 (QAT)** | 2ビットや4ビットなどの低ビット精度で学習・推論を行い、メモリと演算コストを劇的に削減する。 | **QUEST** 14 | 4.3 | 計画中 |
| **P4.3** | **ワンショット・プルーニング** | ヘッセ行列を用いて再学習不要（ワンショット）でネットワークを剪定し、開発サイクルを高速化する。 | **SBC** (Spiking Brain Compression) 15 | 4.4 | 計画中 |

### **フェーズ5: デプロイメントと実用化 (All Tracks)**

ロボティクス等の実応用で不可欠な「リアルタイム性」と「ハードウェア互換性」を確保する。(旧P4)

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P5.1** | **動的推論 (SNN Cutoff)** | 推論の途中で確信度を監視し、閾値を超えた時点で処理を早期終了させ、レイテンシを動的に削減する。 | **SNN Cutoff** 16, 58 | 6.1 | 計画中 |
| **P5.2** | **HW協調設計 (Lava)** | P1.2 (Spikingformer) や P4 の最適化済みモデルを、Intel Loihi 2 向けのLavaフレームワークへ移植する。 | **Lava Framework** 61 | 6.2 | 計画中 |
| **P5.3** | **HW協調設計 (SpiNNaker)** | P1.2 (Spikingformer) や P4 の最適化済みモデルを、SpiNNaker 向けのsPyNNakerフレームワークへ移植する。 | **sPyNNaker** 69 | 6.2 | 計画中 |

### **フェーズ6: 統合的効率化検証 (All Tracks) (v10.2 新規)**

目標②（効率: ANN比 1/50）を達成するため、ここまでの効率化技術を全て統合したモデルの性能を検証する。

| ID | タスク | 概要 | SOTA技術 | 関連(SNN5) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P6.1** | **統合効率化モデルの構築** | P3でチューニングしたSOTAモデルに対し、P2.3 (時間コーディング), P4 (省エネ化), P5.1 (SNN Cutoff) の技術を**すべて適用**した最終モデルを構築する。 | 複合技術 | pruning.py, quantization.py, deployment.py | 計画中 |
| **P6.2** | **最終効率ベンチマーク** | P6.1で構築した「最終モデル」を P7.3 (SNNBench) で評価し、精度と消費電力（SynOps）を測定。ANN比1/50の目標達成度を最終判定する。 | **SNNBench** 79 | run\_benchmark\_suite.py | 計画中 |

### **フェーズ7: ベンチマークと基盤整備 (All Tracks)**

開発基盤をSOTAフレームワークに準拠させ、客観的な評価体制を構築する。(旧P5)

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P7.1** | **開発基盤の移行** | プロジェクトの開発・評価基盤として、SpikingJellyフレームワークを全面的に採用する。 | **SpikingJelly** 24 | 7.1 | 計画中 |
| **P7.2** | **ニューロモーフィック・データセット対応** | 静的データセットに加え、CIFAR10-DVS, SHD, N-Caltech101 等のイベントベースデータセットを標準でサポートする。 | DVS/SHD Datasets 74 | 7.1 | 計画中 |
| **P7.3** | **統合ベンチマークの確立** | 評価指標として「精度」に加え、「SynOps (シナプス演算数)」と「スパイクレート」を必須とし、CI/CDに統合する。 | **SNNBench** 79 | 7.1 | 計画中 |

### **フェーズ8: 生物学的パラダイムの強化 (Bio Track)**

ユーザー提供の numpy ベース実験コードで有効性が実証された微分フリーの学習則を、SNN5プロジェクトの PyTorch (nn.Module) 基盤に正式に統合する。(旧P6)

| ID | タスク | 概要 | 参照コード | 関連(SNN5) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P8.1** | **LSMベースラインの実装** | prototype\_rsnn.py に基づき、(STDP訓練リザバー \+ Ridge回帰リードアウト) というLSMベースラインを PyTorch で実装し、SpikingSSM 等の性能と比較する。 | prototype\_rsnn.py | spiking\_ssm.py | 計画中 |
| **P8.2** | **E/Iバランスの実装** | rsnn\_experiments\_fixed.py (RSNN\_EI) に基づき、デールの法則（E/I分離）を snn\_research/bio\_models/simple\_network.py に正式に実装し、学習の安定性を評価する。 | rsnn\_experiments\_fixed.py | simple\_network.py | 計画中 |
| **P8.3** | **恒常性（適応的閾値）の実装** | rsnn\_experiments\_fixed.py (RSNN\_Homeo) に基づき、適応的発火閾値による恒常性維持メカニズムを BioSNN に実装し、BCMLearningRule と比較検証する。 | rsnn\_experiments\_fixed.py | bcm\_rule.py | 計画中 |
| **P8.4** | **Bio Track 再現性の安定化 (v10.2 新規)** | 目標③(再現性95%)のため、Bio Track (R-STDP等) の確率的挙動（ポアソン入力、確率的STDP）を分析し、シード値固定や決定論的STDPの実装検討により学習の再現性を確保する計画を策定する。 | prototype\_rsnn.py | learning\_rules/stdp.py | 計画中 |

