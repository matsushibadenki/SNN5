# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/distillation/knowledge_distillation_manager.py
# (v9 ä¿®æ­£ç‰ˆ)
#
# Title: çŸ¥è­˜è’¸ç•™ (Knowledge Distillation) ç®¡ç†ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼
# Description:
# - ANNï¼ˆæ•™å¸«ãƒ¢ãƒ‡ãƒ«ï¼‰ã‹ã‚‰SNNï¼ˆç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼‰ã¸ã®çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ç®¡ç†ãƒ»å®Ÿè¡Œã™ã‚‹ã€‚
# - ã‚¿ã‚¹ã‚¯è¨˜è¿°ã«åŸºã¥ãã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰æ•™å¸«/ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ãƒ»ç™»éŒ²ã™ã‚‹ã€‚
# - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’çŸ¥è­˜è’¸ç•™å½¢å¼ï¼ˆæ•™å¸«ãƒ­ã‚¸ãƒƒãƒˆã‚’å«ã‚€ï¼‰ã«ãƒ©ãƒƒãƒ—ã™ã‚‹ã€‚
# - è’¸ç•™ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ï¼ˆDistillationTrainerï¼‰ã‚’å‘¼ã³å‡ºã—ã¦å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
#
# ä¿®æ­£ (v9):
# - å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€collate_fn ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒã‚’
#   `train.py` ã‹ã‚‰ `app/utils.py` ã«å¤‰æ›´ã€‚
# - (v9 ä»¥å‰ã®mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£ã‚³ãƒ¡ãƒ³ãƒˆã¯çœç•¥)
#
# ä¿®æ­£ (v10): mypy ã‚¨ãƒ©ãƒ¼ [name-defined], [assignment], [arg-type], [misc], [no-redef], [list-item] ã‚’ä¿®æ­£
# ä¿®æ­£ (v11): mypy ã‚¨ãƒ©ãƒ¼ [syntax] (ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ) ã‚’ä¿®æ­£
#
# ä¿®æ­£ (v_async_fix):
# - L333: prepare_dataset ã‚’ async def ã«å¤‰æ›´ã€‚
# - L345: asyncio.run() ã‚’ await ã«å¤‰æ›´ã€‚
#
# ä¿®æ­£ (v_hpo_fix_callable_error):
# - DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰æ¸¡ã•ã‚ŒãŸ config ã¯è§£æ±ºæ¸ˆã¿ã®å€¤ (dict) ã‚’ OmegaConf ã«
#   å¤‰æ›ã—ãŸã‚‚ã®ã§ã‚ã‚‹ãŸã‚ã€.log_dir() ã®ã‚ˆã†ãªé–¢æ•°å‘¼ã³å‡ºã—ã‚’
#   .log_dir ã®ã‚ˆã†ãªå±æ€§ã‚¢ã‚¯ã‚»ã‚¹ã«ä¿®æ­£ã€‚

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, Subset
from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerBase
# --- â–¼ ä¿®æ­£: å¿…è¦ãªå‹ãƒ’ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import Dict, Any, Optional, List, Callable, Tuple, cast, TypeAlias, Sized
import os
import json
import logging
import asyncio # [name-defined] asyncio ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# --- â–² ä¿®æ­£ â–² ---
from omegaconf import DictConfig

from snn_research.distillation.model_registry import ModelRegistry
# --- â–¼ ä¿®æ­£: [name-defined] DistillationTrainer ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from snn_research.training.trainers import DistillationTrainer
# --- â–² ä¿®æ­£ â–² ---
from snn_research.benchmark.metrics import calculate_accuracy
# â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£: [name-defined] mypyã‚¨ãƒ©ãƒ¼å›é¿ã®ãŸã‚ã€å‹ãƒ’ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â—¾ï¸â—¾ï¸â—¾ï¸
from torch.optim.lr_scheduler import LRScheduler
# â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸

logger = logging.getLogger(__name__)

# --- â–¼ ä¿®æ­£: å‹ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ TypeAlias ã‚’ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ã§å®šç¾© â–¼ ---
TextCollateFnDef: TypeAlias = Callable[[PreTrainedTokenizerBase, bool], Callable[[List[Any]], Any]]
# --- â–² ä¿®æ­£ â–² ---

# --- â–¼â–¼â–¼ ä¿®æ­£ (v9): ã‚¤ãƒ³ãƒãƒ¼ãƒˆå…ƒã‚’ train.py ã‹ã‚‰ app.utils.py ã«å¤‰æ›´ â–¼â–¼â–¼ ---
try:
    # collate_fn ã¯ app/utils.py ã«å®šç¾©ã•ã‚Œã¦ã„ã‚‹
    from app.utils import collate_fn as text_collate_fn
    
    collate_fn_orig_factory: TextCollateFnDef = cast(TextCollateFnDef, text_collate_fn)
    logger.info("Successfully imported collate_fn from app.utils.py.")
except ImportError:
    logger.warning("Warning: Could not import collate_fn from app.utils.py. Using fallback definition.")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (ä¸»ã«å‹ãƒã‚§ãƒƒã‚¯ã®ãŸã‚)
    def _fallback_collate(batch: List[Any]) -> Any:
        raise NotImplementedError("Fallback collate_fn called. Check app/utils.py.")
    
    def fallback_collate_fn_def(tokenizer: PreTrainedTokenizerBase, is_distillation: bool) -> Callable[[List[Any]], Any]:
        return _fallback_collate
    
    # --- â–¼ ä¿®æ­£: [no-redef] [misc] [list-item] ã‚¨ãƒ©ãƒ¼è§£æ¶ˆã®ãŸã‚ã€é‡è¤‡å®šç¾©ã‚’å‰Šé™¤ â–¼ ---
    # (TextCollateFnDef ã¯ 43è¡Œç›®ã§å®šç¾©æ¸ˆã¿)
    collate_fn_orig_factory = fallback_collate_fn_def
    # --- â–² ä¿®æ­£ â–² ---
# --- â–²â–²â–² ä¿®æ­£ (v9) â–²â–²â–² ---


class KnowledgeDistillationManager:
    """
    SNNã¸ã®çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã€‚
    """
    def __init__(
        self,
        student_model: nn.Module,
        trainer: DistillationTrainer, # <-- [name-defined] ä¿®æ­£
        model_registry: ModelRegistry,
        device: str,
        config: DictConfig, # â—¾ï¸ config ã‚’è¿½åŠ 
        teacher_model: Optional[nn.Module] = None,
        teacher_model_name: Optional[str] = None,
        tokenizer_name: Optional[str] = None
    ):
        self.student_model = student_model
        self.teacher_model = teacher_model
        self.teacher_model_name = teacher_model_name
        self.tokenizer_name = tokenizer_name
        self.trainer = trainer
        self.model_registry = model_registry
        self.device = device
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£: config ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å¤‰æ•°ã¨ã—ã¦ä¿æŒ â—¾ï¸â—¾ï¸â—¾ï¸
        self.config = config 
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸

        if not teacher_model and not teacher_model_name:
            raise ValueError("Either teacher_model (instance) or teacher_model_name (str) must be provided.")
            
        if not tokenizer_name and not (isinstance(teacher_model_name, str) and teacher_model_name):
             raise ValueError("tokenizer_name or a valid teacher_model_name must be provided to load tokenizer.")

        self.tokenizer_name = tokenizer_name if tokenizer_name else cast(str, teacher_model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£(mypy v8): energy.py ã¸ã®ç§»ç®¡ã«ä¼´ã„å‰Šé™¤ â—¾ï¸â—¾ï¸â—¾ï¸
        # self.energy_metrics = EnergyMetrics(...)
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸

    async def _get_or_load_teacher_model(self) -> nn.Module:
        """
        æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹ã€‚
        ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒæä¾›ã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’è¿”ã—ã€ãªã‘ã‚Œã°åå‰ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
        """
        if self.teacher_model:
            return self.teacher_model.to(self.device).eval()

        if not self.teacher_model_name:
             raise ValueError("Cannot load teacher model: teacher_model_name is not set.")

        print(f"ğŸ§  Loading teacher model '{self.teacher_model_name}' from Hugging Face...")
        try:
            model = AutoModelForCausalLM.from_pretrained(self.teacher_model_name)
            self.teacher_model = model.to(self.device).eval()
            return self.teacher_model
        except Exception as e:
            print(f"âŒ Failed to load teacher model: {e}")
            raise

    async def run_on_demand_pipeline(
        self,
        task_description: str,
        unlabeled_data_path: str,
        force_retrain: bool = False,
        student_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ã‚¿ã‚¹ã‚¯è¨˜è¿°ã«åŸºã¥ãã€ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ã§å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
        """
        print(f"--- On-Demand Learning Pipeline Initiated ---")
        print(f"Task: {task_description}")

        # 1. æ—¢å­˜ã®å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
        if not force_retrain:
            existing_experts = await self.model_registry.find_models_for_task(task_description, top_k=1)
            if existing_experts:
                best_expert = existing_experts[0]
                # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£: mypyã‚¨ãƒ©ãƒ¼ [assignment] ã‚’ä¿®æ­£ â—¾ï¸â—¾ï¸â—¾ï¸
                best_expert['model_id'] = task_description # type: ignore[assignment]
                # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸
                print(f"âœ… Found existing expert: {best_expert.get('model_path')}")
                return best_expert

        print(f"â„¹ï¸ No suitable expert found or retraining forced. Starting new training.")

        # 2. å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ (Web CrawlerãŒç”Ÿæˆã—ãŸ .jsonl ã‚’æƒ³å®š)
        if not os.path.exists(unlabeled_data_path):
            print(f"âŒ Error: Unlabeled data file not found at '{unlabeled_data_path}'")
            return {"error": "Data file not found"}
        
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£: mypyã‚¨ãƒ©ãƒ¼ [assignment] ã‚’ä¿®æ­£ â—¾ï¸â—¾ï¸â—¾ï¸
        from snn_research.data.datasets import SimpleTextDataset # å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’é¿ã‘ã‚‹ãŸã‚å±€æ‰€ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸
        
        try:
            # --- â–¼ ä¿®æ­£ (v_hpo_fix_callable_error): .time_steps() -> .time_steps â–¼ ---
            train_dataset_raw = SimpleTextDataset(
                file_path=unlabeled_data_path,
                tokenizer=self.tokenizer,
                max_seq_len=self.config.model.time_steps # type: ignore[attr-defined] 
            )
            # --- â–² ä¿®æ­£ (v_hpo_fix_callable_error) â–² ---
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå°ã•ã™ãã‚‹å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            if len(cast(Sized, train_dataset_raw)) < 10:
                 print(f"âš ï¸ Warning: Dataset at '{unlabeled_data_path}' is too small ({len(cast(Sized, train_dataset_raw))} samples).")
                 if len(cast(Sized, train_dataset_raw)) == 0:
                     return {"error": "No data found in the provided file."}
                 # ãƒ‡ãƒ¼ã‚¿ã‚’è¤‡è£½ã—ã¦æœ€å°é™ã®ãƒãƒƒãƒæ•°ã‚’ç¢ºä¿
                 train_dataset_raw = torch.utils.data.ConcatDataset([train_dataset_raw] * (10 // len(cast(Sized, train_dataset_raw)) + 1)) # type: ignore[assignment]


            # è’¸ç•™ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—ã—ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’äº‹å‰è¨ˆç®—
            print("Preparing distillation dataset (pre-calculating teacher logits)...")
            
            # --- â–¼ ä¿®æ­£ (v_hpo_fix_callable_error): .batch_size() -> .batch_size â–¼ ---
            train_loader, val_loader = await self.prepare_dataset( # type: ignore[call-arg]
                train_dataset_raw,
                None, # æ¤œè¨¼ã‚»ãƒƒãƒˆã¯ã“ã“ã§ã¯ä½œæˆã—ãªã„ (ç°¡æ˜“åŒ–ã®ãŸã‚)
                batch_size=self.config.training.batch_size, # type: ignore[attr-defined]
                collate_fn=None # prepare_datasetå†…éƒ¨ã§collate_fnãŒç”Ÿæˆã•ã‚Œã‚‹
            )
            # --- â–² ä¿®æ­£ (v_hpo_fix_callable_error) â–² ---

        except Exception as e:
            print(f"âŒ Error preparing dataset: {e}")
            return {"error": f"Dataset preparation failed: {e}"}

        # 3. è’¸ç•™ã®å®Ÿè¡Œ
        # --- â–¼ ä¿®æ­£ (v_hpo_fix_callable_error): .epochs() -> .epochs â–¼ ---
        print(f"Starting distillation training for {self.config.training.epochs} epochs...") # type: ignore[attr-defined]
        
        final_metrics: Dict[str, Any] = await self.run_distillation( # type: ignore[assignment]
            train_loader=train_loader,
            val_loader=val_loader, # æ¤œè¨¼ã‚»ãƒƒãƒˆ
            epochs=self.config.training.epochs, # type: ignore[attr-defined]
            model_id=task_description, # ã‚¿ã‚¹ã‚¯è¨˜è¿°ã‚’ãƒ¢ãƒ‡ãƒ«IDã¨ã—ã¦ä½¿ç”¨
            task_description=task_description,
            student_config=student_config # æ¸¡ã•ã‚ŒãŸSNNãƒ¢ãƒ‡ãƒ«è¨­å®š
        )
        # --- â–² ä¿®æ­£ (v_hpo_fix_callable_error) â–² ---

        print(f"âœ… On-demand learning finished.")
        return final_metrics


    async def run_distillation(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        model_id: str,
        task_description: str,
        student_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        çŸ¥è­˜è’¸ç•™ã®å­¦ç¿’ã¨è©•ä¾¡ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        best_metric = float('inf') # æå¤±ã‚’æœ€å°åŒ–
        best_model_path = ""
        
        # --- â–¼ ä¿®æ­£ (v_hpo_fix_callable_error): .log_dir() -> .log_dir â–¼ ---
        log_dir = self.config.training.log_dir # type: ignore[attr-defined]
        # --- â–² ä¿®æ­£ (v_hpo_fix_callable_error) â–² ---
        os.makedirs(log_dir, exist_ok=True)

        for epoch in range(epochs):
            print(f"\n--- Epoch {epoch + 1}/{epochs} ---")
            
            # --- è¨“ç·´ ---
            train_metrics = self.trainer.train_epoch(train_loader, epoch)
            
            # --- æ¤œè¨¼ ---
            if val_loader:
                val_metrics = self.trainer.evaluate(val_loader, epoch)
                
                # ãƒ¡ãƒˆãƒªã‚¯ã‚¹å (loss or accuracy)
                metric_name = self.config.training.get("metric_to_optimize", "total") # type: ignore[attr-defined]
                current_metric = val_metrics.get(metric_name, float('inf'))

                print(f"Epoch {epoch + 1} Validation Metrics: {val_metrics}")

                # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
                if current_metric < best_metric:
                    best_metric = current_metric
                    best_model_path = os.path.join(log_dir, f"{model_id}_best.pth")
                    
                    config_to_save: Dict[str, Any] = student_config if student_config is not None else {} # type: ignore[assignment]
                    
                    self.trainer.save_checkpoint(
                        path=best_model_path,
                        epoch=epoch,
                        metric_value=best_metric,
                        config=config_to_save, # â—¾ï¸ ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä¿å­˜
                        tokenizer_name=self.tokenizer_name
                    )
            else:
                 # æ¤œè¨¼ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒãªã„å ´åˆã¯ã€è¨“ç·´ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã§ä»£ç”¨ï¼ˆéæ¨å¥¨ï¼‰
                 best_metric = train_metrics.get("total", float('inf'))


        # --- æœ€çµ‚è©•ä¾¡ã¨ãƒ¢ãƒ‡ãƒ«ç™»éŒ² ---
        print("\n--- Final Evaluation on Validation Set ---")
        final_metrics: Dict[str, Any] = {"accuracy": 0.0, "avg_spikes_per_sample": float('inf')}
        
        if val_loader:
            # æœ€é«˜ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
            if os.path.exists(best_model_path):
                self.trainer.load_checkpoint(best_model_path)
            
            final_eval_metrics_raw = self.trainer.evaluate(val_loader, epochs)
            
            final_metrics['accuracy'] = final_eval_metrics_raw.get('accuracy', 0.0) # type: ignore[assignment]
            final_metrics['avg_spikes_per_sample'] = final_eval_metrics_raw.get('avg_cutoff_steps', 0.0) # type: ignore[assignment]
            
        print(f"Final Metrics: {final_metrics}")

        # ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²
        if student_config:
            await self.model_registry.register_model(
                model_id=model_id,
                task_description=task_description,
                metrics=final_metrics,
                model_path=best_model_path,
                config=student_config
            )
            
            # ç™»éŒ²ã—ãŸæƒ…å ±ã‚’è¿”ã™
            final_model_info: Dict[str, Any] = { # type: ignore[assignment]
                "model_id": model_id,
                "task_description": task_description,
                "metrics": final_metrics,
                "path": best_model_path,
                "config": student_config
            }
            return final_model_info
        else:
            print("âš ï¸ Warning: student_config ãŒãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã«ç™»éŒ²ã§ãã¾ã›ã‚“ã€‚")
            return {"error": "Student config was missing.", "metrics": final_metrics}

    # --- â–¼ ä¿®æ­£ (v_async_fix): async def ã«å¤‰æ›´ â–¼ ---
    async def prepare_dataset(
    # --- â–² ä¿®æ­£ (v_async_fix) â–² ---
        self,
        train_dataset: Dataset,
        val_dataset: Optional[Dataset] = None,
        batch_size: int = 16,
        num_workers: int = 0,
        collate_fn: Optional[Callable] = None
    ) -> Tuple[DataLoader, DataLoader]:
        """
        æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’äº‹å‰è¨ˆç®—ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ãƒƒãƒ‘ãƒ¼ã‚’é©ç”¨ã™ã‚‹ã€‚
        """
        
        # collate_fn ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® collate_fn ã‚’ä½¿ç”¨
        collate_fn_orig_factory: TextCollateFnDef
        if collate_fn is None:
            collate_fn_orig_factory = cast(TextCollateFnDef, text_collate_fn) # type: ignore[assignment]
        else:
            # æ¸¡ã•ã‚ŒãŸ collate_fn ãŒãƒ•ã‚¡ã‚¯ãƒˆãƒªå½¢å¼ (tokenizer, is_distillation ã‚’å–ã‚‹) ã§ã¯ãªã„
            # å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ãƒ©ãƒƒãƒ‘ãƒ¼ã§å¯¾å¿œ
            def collate_fn_factory_wrapper(tokenizer, is_distillation):
                return collate_fn # type: ignore[return-value]
            collate_fn_orig_factory = collate_fn_factory_wrapper # type: ignore[assignment]

        # --- â–¼ ä¿®æ­£ (v_async_fix): asyncio.run() ã‚’ await ã«å¤‰æ›´ â–¼ ---
        teacher_model_instance = await self._get_or_load_teacher_model()
        # --- â–² ä¿®æ­£ (v_async_fix) â–² ---

        # è’¸ç•™ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ãƒƒãƒ‘ãƒ¼
        distill_train_dataset: Dataset = _DistillationWrapperDataset(
            original_dataset=train_dataset,
            teacher_model=teacher_model_instance,
            tokenizer=self.tokenizer,
            collate_fn_orig_factory=collate_fn_orig_factory, # type: ignore[arg-type] # ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‚’æ¸¡ã™
            device=self.device
        )
        
        distill_val_dataset: Dataset
        if val_dataset:
            distill_val_dataset = _DistillationWrapperDataset(
                original_dataset=val_dataset,
                teacher_model=teacher_model_instance,
                tokenizer=self.tokenizer,
                collate_fn_orig_factory=collate_fn_orig_factory, # type: ignore[arg-type] # ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‚’æ¸¡ã™
                device=self.device
            )
        else:
            # æ¤œè¨¼ã‚»ãƒƒãƒˆãŒãªã„å ´åˆã€è¨“ç·´ã‚»ãƒƒãƒˆã‹ã‚‰10%ã‚’æ‹å€Ÿ (ç°¡æ˜“çš„)
            try:
                train_size = int(0.9 * len(cast(Sized, distill_train_dataset)))
                val_size = len(cast(Sized, distill_train_dataset)) - train_size
                if val_size == 0 and train_size > 0:
                     train_size -= 1
                     val_size = 1
                
                if train_size > 0 and val_size > 0:
                    distill_train_dataset, distill_val_dataset = torch.utils.data.random_split(distill_train_dataset, [train_size, val_size])
                else:
                    print("Warning: Dataset too small to split for validation. Using training set for validation.")
                    distill_val_dataset = distill_train_dataset
            except Exception as e:
                 print(f"Warning: Could not split dataset for validation: {e}. Using training set for validation.")
                 distill_val_dataset = distill_train_dataset


        # è’¸ç•™ç”¨ã® collate_fn (ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™)
        distillation_collate_fn = self._create_distillation_collate_fn(
            collate_fn_orig_factory=collate_fn_orig_factory # type: ignore[arg-type] # ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‚’æ¸¡ã™
        )

        train_loader = DataLoader(
            distill_train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=distillation_collate_fn
        )
        val_loader = DataLoader(
            distill_val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            collate_fn=distillation_collate_fn
        )

        return train_loader, val_loader

    def _create_distillation_collate_fn(
        self,
        collate_fn_orig_factory: TextCollateFnDef
    ) -> Callable:
        """
        çŸ¥è­˜è’¸ç•™ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ collate_fn ã‚’ä½œæˆã™ã‚‹ã€‚
        (student_input, attention_mask, student_target, teacher_logits) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã€‚
        """
        
        # ãƒ•ã‚¡ã‚¯ãƒˆãƒªã‹ã‚‰ collate_fn ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—
        # (è’¸ç•™ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ©ãƒƒãƒ‘ãƒ¼ãŒå†…éƒ¨ã§ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã« collate_fn ã‚’ä½¿ã†ãŸã‚ã€
        #  ã“ã“ã§ã¯ is_distillation=False ã‚’æ¸¡ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ç”¨ã® collate_fn ã‚’å–å¾—ã™ã‚‹)
        collate_fn_orig: Callable[[List[Any]], Any] = collate_fn_orig_factory(self.tokenizer, False)

        def distillation_collate(batch: List[Tuple[Dict[str, Any], torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
            """
            Args:
                batch (List[Tuple[Dict, Tensor]]): 
                    _DistillationWrapperDataset ã‹ã‚‰ã®å‡ºåŠ›ã€‚
                    å„è¦ç´ ã¯ (original_batch_item, teacher_logits_for_item) ã®ã‚¿ãƒ—ãƒ«ã€‚
            """
            
            original_batch_items: List[Dict[str, Any]] = [item[0] for item in batch]
            teacher_logits_list: List[torch.Tensor] = [item[1] for item in batch]

            # 1. å…ƒã® collate_fn ã‚’ä½¿ã£ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ãƒ³ã‚½ãƒ«åŒ– (SNNå…¥åŠ›ç”¨)
            #    collate_fn_orig ã¯ (input_ids, attention_mask, labels) ã‚’å«ã‚€è¾æ›¸ã‚’è¿”ã™ã¨æœŸå¾…
            collated_batch: Dict[str, torch.Tensor] = collate_fn_orig(original_batch_items)
            
            student_input_ids = collated_batch['input_ids']
            attention_mask = collated_batch['attention_mask']
            student_target_ids = collated_batch['labels']

            # 2. æ•™å¸«ãƒ­ã‚¸ãƒƒãƒˆã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦ãƒãƒƒãƒåŒ–
            #    teacher_logits_list ã®å„è¦ç´ ã¯ (SeqLen_item, VocabSize)
            padded_teacher_logits = torch.nn.utils.rnn.pad_sequence(
                teacher_logits_list, batch_first=True, padding_value=0.0
            )

            # 3. ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®æ•´åˆæ€§ã‚’å–ã‚‹
            max_len_student = student_input_ids.shape[1]
            max_len_teacher = padded_teacher_logits.shape[1]
            
            # (student_target_ids ã¯ input_ids ã¨åŒã˜é•·ã•ã®ã¯ãš)
            if student_target_ids.shape[1] != max_len_student:
                 # collate_fn_orig ãŒ labels ã‚‚ input_ids ã¨åŒã˜é•·ã•ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’æœŸå¾…
                 # (ã‚‚ã—ã‚ºãƒ¬ã¦ã„ãŸã‚‰ã€ã“ã“ã§ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆãŒå¿…è¦)
                 pass

            # ãƒ­ã‚¸ãƒƒãƒˆã¨å…¥åŠ›ã®é•·ã•ã‚’åˆã‚ã›ã‚‹ (é€šå¸¸ã¯åŒã˜ã¯ãšã ãŒã€å¿µã®ãŸã‚)
            if max_len_student > max_len_teacher:
                # ãƒ­ã‚¸ãƒƒãƒˆå´ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
                pad_size = max_len_student - max_len_teacher
                padding = torch.zeros(
                    (padded_teacher_logits.shape[0], pad_size, padded_teacher_logits.shape[2]),
                    dtype=padded_teacher_logits.dtype, device=padded_teacher_logits.device
                )
                padded_teacher_logits = torch.cat([padded_teacher_logits, padding], dim=1)
            
            elif max_len_teacher > max_len_student:
                # å…¥åŠ›å´ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° (attention_mask ã‚‚)
                pad_size = max_len_teacher - max_len_student
                pad_val_input = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0
                pad_val_target = -100
                
                padding_input = torch.full(
                    (student_input_ids.shape[0], pad_size), pad_val_input,
                    dtype=student_input_ids.dtype, device=student_input_ids.device
                )
                student_input_ids = torch.cat([student_input_ids, padding_input], dim=1)

                padding_mask = torch.zeros(
                    (attention_mask.shape[0], pad_size),
                    dtype=attention_mask.dtype, device=attention_mask.device
                )
                attention_mask = torch.cat([attention_mask, padding_mask], dim=1)
                
                padding_target = torch.full(
                    (student_target_ids.shape[0], pad_size), pad_val_target,
                    dtype=student_target_ids.dtype, device=student_target_ids.device
                )
                student_target_ids = torch.cat([student_target_ids, padding_target], dim=1)
            
            # (student_input, attention_mask, student_target, teacher_logits)
            return student_input_ids, attention_mask, student_target_ids, padded_teacher_logits

        return distillation_collate


class _DistillationWrapperDataset(Dataset):
    """
    æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—ã—ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’äº‹å‰å®Ÿè¡Œã—ã¦
    (item, teacher_logits) ã®ãƒšã‚¢ã‚’è¿”ã™ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    """
    def __init__(
        self,
        original_dataset: Dataset,
        teacher_model: nn.Module,
        tokenizer: PreTrainedTokenizerBase,
        collate_fn_orig_factory: TextCollateFnDef,
        device: str
    ):
        self.original_dataset = original_dataset
        self.teacher_model = teacher_model.to(device).eval()
        self.tokenizer = tokenizer
        self.device = device
        
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£: [assignment] ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ â—¾ï¸â—¾ï¸â—¾ï¸
        # collate_fn_orig_factory ãŒ TextCollateFnDef å‹ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ç¤º
        # (is_distillation=False ã‚’æ¸¡ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ç”¨ã® collate_fn ã‚’å–å¾—)
        self.collate_fn_orig: Callable[[List[Any]], Any] = collate_fn_orig_factory(tokenizer, False)
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸
        
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£: mypyã‚¨ãƒ©ãƒ¼ [call-arg] ã‚’ä¿®æ­£ â—¾ï¸â—¾ï¸â—¾ï¸
        # (collate_fn_orig_factory ã¯æ—¢ã« collate_fn ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ãªããƒ•ã‚¡ã‚¯ãƒˆãƒªãªã®ã§ã€
        #  å†åº¦å‘¼ã³å‡ºã™å¿…è¦ã¯ãªã„ã€ã¨ã„ã† mypy ã®æŒ‡æ‘˜ã ã£ãŸãŒã€
        #  ãƒ•ã‚¡ã‚¯ãƒˆãƒªã®å®šç¾© (TextCollateFnDef) ãŒ (Tokenizer, bool) -> Callable ãªã®ã§ã€
        #  L537 ã®å‘¼ã³å‡ºã—ã¯æ­£ã—ã„ã€‚mypyã®å‹æ¨è«–ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ãŒé«˜ã„ã€‚)
        
        # (v9 ä¿®æ­£): collate_fn ãŒ None ã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if self.collate_fn_orig is None:
             logger.error("Failed to get original collate_fn from factory. Using default fallback.")
             # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã® collate_fn (è¾æ›¸ã‚’è¿”ã™) ã‚’ä½¿ã†ãŒã€
             # ã“ã®ãƒ©ãƒƒãƒ‘ãƒ¼ã¯ collate_fn_orig ãŒè¾æ›¸ã‚’è¿”ã™ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã‚‹
             # æš«å®šçš„ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹
             def error_collate(batch):
                 raise RuntimeError("collate_fn was None during _DistillationWrapperDataset init.")
             self.collate_fn_orig = error_collate
        
        # â—¾ï¸â—¾ï¸â—¾ï¸ ä¿®æ­£çµ‚ã‚ã‚Š â—¾ï¸â—¾ï¸â—¾ï¸
        
        logger.info(f"DistillationWrapperDataset initialized for {len(cast(Sized, self.original_dataset))} samples.")

    def __len__(self) -> int:
        # --- â–¼ ä¿®æ­£: [arg-type] ã‚¨ãƒ©ãƒ¼è§£æ¶ˆã®ãŸã‚ cast ã‚’è¿½åŠ  â–¼ ---
        return len(cast(Sized, self.original_dataset))
        # --- â–² ä¿®æ­£ â–² ---

    @torch.no_grad()
    def __getitem__(self, idx: int) -> Tuple[Any, torch.Tensor]:
        """
        å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ã€ãã‚Œã«å¯¾ã™ã‚‹æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’è¿”ã™ã€‚
        """
        # 1. å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ ã‚’å–å¾—
        # (SST2Taskãªã©ã¯è¾æ›¸ {'text': ..., 'label': ...} ã‚’è¿”ã™)
        original_item: Any = self.original_dataset[idx]
        
        # 2. collate_fn ã‚’ä½¿ã£ã¦ã€å˜ä¸€ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒƒãƒå½¢å¼ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        #    (collate_fn ã¯è¾æ›¸ {'input_ids': (B, T), ...} ã‚’è¿”ã™ã¨æœŸå¾…)
        # --- â–¼ ä¿®æ­£ (v9): collate_fn ãŒ None ã§ãªã„ã“ã¨ã‚’ç¢ºèª â–¼ ---
        if self.collate_fn_orig is None:
             raise RuntimeError("collate_fn_orig is None, cannot process item.")
        # --- â–² ä¿®æ­£ (v9) â–² ---
        
        collated_batch: Dict[str, torch.Tensor] = self.collate_fn_orig([original_item])
        
        # 3. æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã§ãƒ­ã‚¸ãƒƒãƒˆã‚’è¨ˆç®—
        input_ids = collated_batch['input_ids'].to(self.device)
        attention_mask = collated_batch['attention_mask'].to(self.device)
        
        teacher_outputs = self.teacher_model(input_ids=input_ids, attention_mask=attention_mask)
        teacher_logits: torch.Tensor = teacher_outputs.logits # (B=1, SeqLen, VocabSize)
        
        # 4. CPUã«ç§»å‹•ã—ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’å‰Šé™¤
        teacher_logits_cpu = teacher_logits.squeeze(0).cpu().to(torch.float16) # (SeqLen, VocabSize)
        
        # (å…ƒã®ã‚¢ã‚¤ãƒ†ãƒ , æ•™å¸«ãƒ­ã‚¸ãƒƒãƒˆ) ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™
        return original_item, teacher_logits_cpu
