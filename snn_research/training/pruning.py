# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/pruning.py
# (æ”¹ä¿®: SBC ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£… + æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°è¿½åŠ )
# Title: æ§‹é€ çš„ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (SBC & Spatio-Temporal)
# Description:
# - doc/SNNé–‹ç™ºï¼šåŸºæœ¬è¨­è¨ˆæ€æƒ³.md (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.4, å¼•ç”¨[15]) ã«åŸºã¥ãã€
#   ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSBCï¼‰ã‚’å®Ÿè£…ã™ã‚‹ã€‚
# - SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
#   æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã®ã‚¹ã‚¿ãƒ–ã‚’è¿½åŠ ã€‚
#
# æ”¹å–„ (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1 å¯¾å¿œ):
# - _calculate_temporal_redundancy ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…ã‚’ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«
#   åŸºã¥ãé£½å’Œåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã®ã‚¹ã‚¿ãƒ–ï¼‰ã«æ”¹å–„ã€‚
#
# mypy --strict æº–æ‹ ã€‚
#
# æ”¹å–„ (v2):
# - SBC (å¼•ç”¨[15]) ã®æ ¸å¿ƒã§ã‚ã‚‹ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®è¨ˆç®—ã¨é‡ã¿è£œæ­£ã®
#   ã€Œãƒ€ãƒŸãƒ¼å®Ÿè£…ã€ã‚’ã€Œè¿‘ä¼¼å®Ÿè£… (Optimal Brain Damage)ã€ã«æ”¹å–„ã€‚

import torch
import torch.nn as nn
# --- â–¼ ä¿®æ­£: å¿…è¦ãªå‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import List, Tuple, Dict, Any, cast, Optional, Type, Iterator
import logging 
# --- â–² ä¿®æ­£ â–² ---
# --- â–¼ ä¿®æ­£: SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1 å¯¾å¿œ â–¼ ---
from snn_research.core.neurons import AdaptiveLIFNeuron, IzhikevichNeuron
import torch.nn.functional as F
# --- â–² ä¿®æ­£ â–² ---


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- â–¼â–¼â–¼ æ”¹å–„ (v2): SBC ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®è§£æ¶ˆ â–¼â–¼â–¼ ---
def _get_model_input_keys(model: nn.Module) -> List[str]:
    """ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚¿ã‚¤ãƒ—ã‹ã‚‰å…¥åŠ›ã‚­ãƒ¼ã‚’æ¨æ¸¬ã™ã‚‹ (ç°¡æ˜“ç‰ˆ)"""
    if hasattr(model, 'config') and hasattr(model.config, 'architecture_type'):
        arch_type = model.config.architecture_type
        if arch_type in ["spiking_cnn", "sew_resnet", "hybrid_cnn_snn"]:
            return ["input_images"]
        if arch_type == "tskips_snn":
            return ["input_sequence"]
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (Transformer, SSM, RWKV ãªã©)
    return ["input_ids"]

def _compute_hessian_diag(
    model: nn.Module, 
    loss_fn: nn.Module, 
    dataloader: Any,
    max_samples: int = 64 # ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
) -> Dict[str, torch.Tensor]:
    """
    (æ”¹å–„ v2) ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®å¯¾è§’æˆåˆ† (H_ii = d^2 L / d w_i^2) ã‚’è¿‘ä¼¼è¨ˆç®—ã™ã‚‹ã€‚
    SBC (å¼•ç”¨[15]) ã«åŸºã¥ãã€‚
    """
    logger.info("Computing Hessian matrix diagonal (Approximate)...")
    
    # 1. è¨ˆç®—å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (é‡ã¿) ã‚’ç‰¹å®š
    params_to_compute: List[nn.Parameter] = []
    param_names: List[str] = []
    for name, param in model.named_parameters():
        if "weight" in name and param.requires_grad and param.dim() > 1:
            params_to_compute.append(param)
            param_names.append(name)
            
    if not params_to_compute:
        logger.warning("No parameters found for Hessian computation.")
        return {}

    # 2. æå¤±ã®å‹¾é… (dL/dw) ã‚’è¨ˆç®— (autograd.grad ã‚’ä½¿ã†ãŸã‚)
    
    # (SNNCoreãƒ©ãƒƒãƒ‘ãƒ¼ã‚’æƒ³å®š)
    input_keys: List[str] = _get_model_input_keys(model)
    
    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
    data_iterator: Iterator = iter(dataloader)
    hessian_diag_avg: Dict[str, torch.Tensor] = {
        name: torch.zeros_like(param, device=param.device) 
        for name, param in zip(param_names, params_to_compute)
    }
    samples_processed: int = 0
    device: torch.device = next(model.parameters()).device

    while samples_processed < max_samples:
        try:
            batch: Any = next(data_iterator)
            
            # (SNN/ANNãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®collate_fnå‡ºåŠ›ã‚’æƒ³å®š)
            if not isinstance(batch, dict) or "labels" not in batch:
                logger.warning("Skipping batch: Invalid data format for Hessian computation.")
                continue
                
            labels: torch.Tensor = batch["labels"].to(device)
            inputs: Dict[str, torch.Tensor] = {
                k: v.to(device) for k, v in batch.items() if k in input_keys
            }

            if not inputs:
                logger.warning("Skipping batch: No valid input keys found.")
                continue

            # (ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒHessianè¨ˆç®—ã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã€ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«è¨ˆç®—)
            current_batch_size: int = labels.shape[0]
            
            for i in range(current_batch_size):
                if samples_processed >= max_samples:
                    break
                
                # ã‚µãƒ³ãƒ—ãƒ« i ã®ã¿æŠ½å‡º
                sample_inputs: Dict[str, torch.Tensor] = {
                    k: v[i].unsqueeze(0) for k, v in inputs.items()
                }
                sample_label: torch.Tensor = labels[i].unsqueeze(0)

                # --- æå¤± L ã‚’è¨ˆç®— ---
                model.zero_grad()
                outputs: Tuple[torch.Tensor, ...] = model(**sample_inputs)
                logits: torch.Tensor = outputs[0] if isinstance(outputs, tuple) else outputs
                
                loss: torch.Tensor
                # (SNN/ANNãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æå¤±ã‚’æƒ³å®š)
                if logits.dim() == 3: # (B, S, V)
                    loss = loss_fn(logits.view(-1, logits.size(-1)), sample_label.view(-1))
                else: # (B, V)
                    loss = loss_fn(logits, sample_label)

                # --- 1æ¬¡å‹¾é… (dL/dw) ã‚’è¨ˆç®— ---
                first_grads: Tuple[torch.Tensor, ...] = torch.autograd.grad(
                    loss, params_to_compute, create_graph=True
                )
                
                # --- 2æ¬¡å‹¾é… (H_ii) ã‚’è¨ˆç®— ---
                # (Hessian-vector product (H*v) ã® v ã‚’ (1, 1, ...) ã«è¨­å®šã—ã€
                #  dL/dw (first_grads) ã¨ã®å†…ç©ã‚’å–ã‚‹ã“ã¨ã§å¯¾è§’æˆåˆ†ã‚’è¿‘ä¼¼)
                #
                #  (ã‚ˆã‚Šå˜ç´”ãªæ–¹æ³•: d(dL/dw)/dw ã‚’è¨ˆç®—)
                
                for j, (name, param) in enumerate(zip(param_names, params_to_compute)):
                    if first_grads[j] is None:
                        continue
                        
                    # (dL/dw)^2 ã‚’ H_ii ã®è¿‘ä¼¼ã¨ã—ã¦ä½¿ç”¨ (Fisheræƒ…å ±è¡Œåˆ—ã®å¯¾è§’ã®è¿‘ä¼¼)
                    # H_ii â‰ˆ E[(dL/dw_i)^2]
                    # (SBC (å¼•ç”¨[15]) ã¯ãƒ˜ãƒƒã‚»è¡Œåˆ— (d^2 L / dw^2) ã‚’è¦æ±‚ã™ã‚‹ãŒã€
                    #  å¤šãã®å®Ÿè£…ã§ã¯è¨ˆç®—ã®å®¹æ˜“ã•ã‹ã‚‰Fisherã®å¯¾è§’ã§ä»£ç”¨ã™ã‚‹)
                    
                    # (ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã®å‹¾é…ã®äºŒä¹—ã‚’åŠ ç®—)
                    hessian_diag_avg[name] += (first_grads[j] ** 2)

                samples_processed += 1
                
        except StopIteration:
            break # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼çµ‚äº†
        except Exception as e:
            logger.error(f"Error during Hessian computation: {e}", exc_info=True)
            break # ã‚¨ãƒ©ãƒ¼åœæ­¢

    if samples_processed == 0:
        logger.error("Hessian computation failed: No samples processed.")
        return {}

    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã§å¹³å‡
    for name in hessian_diag_avg:
        hessian_diag_avg[name] /= samples_processed
        # (SBCã¯ d^2 L / dw^2 ãŒè² ã«ãªã‚‹ã“ã¨ã‚‚è¨±å®¹ã™ã‚‹ãŒã€
        #  Fisherè¿‘ä¼¼ (dL/dw)^2 ã¯å¸¸ã«æ­£ã€‚ã“ã“ã§ã¯ 1e-8 ã‚’åŠ ãˆã¦å®‰å®šåŒ–)
        hessian_diag_avg[name] += 1e-8 

    logger.info(f"Hessian diagonal (Fisher approx.) computed for {len(hessian_diag_avg)} layers (using {samples_processed} samples).")
    return hessian_diag_avg

# --- â–²â–²â–² æ”¹å–„ (v2): SBC ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®è§£æ¶ˆ â–²â–²â–² ---

def _compute_saliency(param: torch.Tensor, hessian_diag: torch.Tensor) -> torch.Tensor:
    """
    SBC (Optimal Brain Compression) ã«åŸºã¥ãé‡ã¿ã®é‡è¦åº¦ï¼ˆSaliencyï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    Saliency = (1/2) * (w^2) * (H_ii)
    """
    return 0.5 * (param.data ** 2) * hessian_diag

@torch.no_grad()
def _prune_and_update_weights(
    module: nn.Module,
    param_name: str,
    saliency: torch.Tensor,
    hessian_diag: torch.Tensor, # æ”¹å–„ v2: ãƒ˜ãƒƒã‚»è¡Œåˆ—ã‚’å—ã‘å–ã‚‹
    amount: float
) -> Tuple[int, int]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã€‚
    (æ”¹å–„ v2): Optimal Brain Compression (OBC) ã®é‡ã¿è£œæ­£ã¯ã‚ªãƒ•ãƒ€ã‚¤ã‚¢ã‚´ãƒŠãƒ«é … H_ij ãŒ
               å¿…è¦ã§ã‚ã‚Šã€ã“ã®å®Ÿè£…ï¼ˆå¯¾è§’é … H_ii ã®ã¿ï¼‰ã§ã¯ä¸å¯èƒ½ã€‚
               ã“ã“ã§ã¯ã€SBCè«–æ–‡ (å¼•ç”¨[15]) ã® Saliency (é‡è¦åº¦) ã«åŸºã¥ã
               é‡ã¿ã‚’å‰Šé™¤ã™ã‚‹ã€ŒOptimal Brain Damage (OBD)ã€ç›¸å½“ã®å‡¦ç†ã‚’è¡Œã†ã€‚
               é‡ã¿è£œæ­£ (Update) ã¯è¡Œã‚ãªã„ã€‚
    """
    param: torch.Tensor = getattr(module, param_name)
    
    # 1. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã‚’æ±ºå®š
    num_to_prune = int(param.numel() * amount)
    if num_to_prune == 0:
        return 0, param.numel()
        
    # Saliency ãŒ *æœ€å°* ã®ã‚‚ã®ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã¨ã™ã‚‹
    threshold = torch.kthvalue(saliency.view(-1), k=num_to_prune).values
    
    # Saliency > threshold ã®é‡ã¿ã‚’ *æ®‹ã™* (ãƒã‚¹ã‚¯)
    mask = saliency > threshold
    
    # --- æ”¹å–„ v2: é‡ã¿è£œæ­£ (Update) ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å‰Šé™¤ ---
    # (ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯ã¯ä¸æ­£ç¢ºã§ã‚ã‚Šã€OBD (å¯¾è§’é …ã®ã¿) ã§ã¯
    #  é‡ã¿è£œæ­£ã¯è¡Œã‚ãªã„ã®ãŒä¸€èˆ¬çš„ã§ã‚ã‚‹ãŸã‚)
    
    # 3. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒã‚¹ã‚¯ã‚’é©ç”¨)
    param.data *= mask.float()
    
    original_count = param.numel()
    pruned_count = original_count - mask.sum().item()
    return int(pruned_count), original_count
# --- â–²â–²â–² æ”¹å–„ (v2): SBC ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®è§£æ¶ˆ â–²â–²â–² ---

def apply_sbc_pruning(
    model: nn.Module,
    amount: float,
    dataloader_stub: Any, # ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)
    loss_fn_stub: nn.Module # æå¤±é–¢æ•° (ã‚¹ã‚¿ãƒ–)
) -> nn.Module:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã€SBC (Spiking Brain Compression) ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã€‚
    (æ”¹å–„ v2: å®Ÿè£…ã¯ OBD (Optimal Brain Damage) ç›¸å½“)

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚
        amount (float): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã®å‰²åˆ (0.0ã‹ã‚‰1.0ã®é–“)ã€‚
        dataloader_stub (Any): ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€‚
        loss_fn_stub (nn.Module): æå¤±é–¢æ•°ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    if not (0.0 < amount < 1.0):
        logger.warning(f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é‡ãŒç„¡åŠ¹ã§ã™ ({amount})ã€‚0.0ã‹ã‚‰1.0ã®é–“ã®å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return model

    logger.info(f"--- ğŸ§  Spiking Brain Compression (SBC/OBD) é–‹å§‹ (Amount: {amount:.1%}) ---")

    # 1. ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆå¯¾è§’æˆåˆ†ï¼‰ã‚’è¨ˆç®— (æ”¹å–„ v2)
    hessian_diagonals = _compute_hessian_diag(model, loss_fn_stub, dataloader_stub)
    
    if not hessian_diagonals:
        logger.error("--- âŒ SBC å¤±æ•—: ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ ---")
        return model
    
    total_pruned = 0
    total_params = 0

    # 2. å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    target_modules: List[Tuple[nn.Module, str]] = []
    
    # (SNNCoreãƒ©ãƒƒãƒ‘ãƒ¼ã‚’è€ƒæ…®ã—ã€å†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—)
    model_to_prune: nn.Module = model
    if isinstance(model, SNNCore) and hasattr(model, 'model'):
        model_to_prune = model.model
    
    for module in model_to_prune.modules():
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
            if hasattr(module, 'weight'): # weight ãŒã‚ã‚‹ã‹ç¢ºèª
                target_modules.append((module, 'weight'))

    if not target_modules:
        logger.warning("ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return model

    logger.info(f"SBCå¯¾è±¡ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {len(target_modules)}")
    
    for module, param_name in target_modules:
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’å–å¾— (mypyäº’æ›ã®ãŸã‚ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨)
        full_param_name: str = ""
        for name, mod in model_to_prune.named_modules(): # model_to_prune ã‚’æ¢ç´¢
             if mod is module:
                 full_param_name = f"{name}.{param_name}"
                 break
        
        if not full_param_name:
             logger.warning(f"  - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ (Module: {type(module)})")
             continue

        if full_param_name in hessian_diagonals:
            param: torch.Tensor = getattr(module, param_name)
            hessian_diag = hessian_diagonals[full_param_name]
            
            # 3. é‡è¦åº¦ã‚’è¨ˆç®—
            saliency = _compute_saliency(param, hessian_diag)
            
            # 4. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (é‡ã¿è£œæ­£ãªã—) (æ”¹å–„ v2)
            pruned, total = _prune_and_update_weights(
                module, param_name, saliency, hessian_diag, amount
            )
            total_pruned += pruned
            total_params += total
            logger.info(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': {pruned}/{total} ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (OBDãƒ™ãƒ¼ã‚¹)ã€‚")
        else:
            logger.warning(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': ãƒ˜ãƒƒã‚»è¡Œåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"--- âœ… SBC (OBD) å®Œäº† ---")
        logger.info(f"  - åˆè¨ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ç‡: {actual_sparsity:.2%} ({total_pruned} / {total_params})")
    else:
        logger.error("--- âŒ SBC å¤±æ•—: å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ0ã§ã—ãŸ ---")

    return model

# --- â–¼â–¼â–¼ SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ãè¿½åŠ å®Ÿè£… â–¼â–¼â–¼ ---

@torch.no_grad()
def _calculate_temporal_redundancy(
    model: nn.Module, 
    dataloader: Any, 
    time_steps: int,
    target_layer_names: Optional[List[str]] = None, # ç›£è¦–å¯¾è±¡ã®LIFå±¤ãªã©
    kl_threshold: float = 0.01 # é£½å’Œã¨ã¿ãªã™KLç™ºæ•£ã®é–¾å€¤
) -> Dict[str, int]:
    """
    (æ”¹å–„) SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€‚
    KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ç›£è¦–ã—ã€æƒ…å ±ãŒé£½å’Œã—ãŸå†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹ã€‚
    
    (æ³¨: å®Ÿéš›ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ã¯è¤‡é›‘ãªãƒ•ãƒƒã‚¯ã¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ä¼´ã†ãŸã‚ã€
     ã“ã“ã§ã¯ãã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œçµæœã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¿ãƒ–ã‚’å®Ÿè£…ã—ã¾ã™)

    Returns:
        Dict[str, int]: ãƒ¬ã‚¤ãƒ¤ãƒ¼åã¨ã€ãã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‰Šæ¸›å¯èƒ½ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¾æ›¸ã€‚
    """
    logger.info(f"Calculating temporal redundancy (KL divergence method, threshold={kl_threshold})...")
    
    # --- (ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®æ”¹å–„) ---
    # å®Ÿéš›ã«ã¯ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã€ãƒ•ãƒƒã‚¯ã‚’ä½¿ã£ã¦
    # å„ `target_layer_names` ã®ã‚¹ãƒ‘ã‚¤ã‚¯å‡ºåŠ› (T, B, F) ã‚’åé›†ã™ã‚‹ã€‚
    #
    # for t in range(time_steps - 1):
    #   p_t = spike_history[t].mean(dim=(0, 1)) # (F,)
    #   p_t_plus_1 = spike_history[t+1].mean(dim=(0, 1))
    #   # ã‚¼ãƒ­ã‚’é¿ã‘ã‚‹ãŸã‚ã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    #   p_t = (p_t + 1e-6) / (1.0 + 1e-6 * F)
    #   p_t_plus_1 = (p_t_plus_1 + 1e-6) / (1.0 + 1e-6 * F)
    #   
    #   kl_div = F.kl_div(p_t_plus_1.log(), p_t, reduction='sum')
    #   
    #   if kl_div < kl_threshold:
    #       redundant_start_step = t + 1
    #       break
    
    # (ã“ã“ã§ã¯ã€ãã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œçµæœã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹)
    
    # KLé–¾å€¤ãŒå°ã•ã„ã»ã©ã€é£½å’Œæ¤œå‡ºãŒå³ã—ããªã‚Šã€å†—é•·ã‚¹ãƒ†ãƒƒãƒ—ã¯å°‘ãªããªã‚‹
    # KLé–¾å€¤ãŒå¤§ãã„ã»ã©ã€é£½å’Œæ¤œå‡ºãŒç·©ããªã‚Šã€å†—é•·ã‚¹ãƒ†ãƒƒãƒ—ã¯å¤šããªã‚‹
    
    # å†—é•·ãªé–‹å§‹ã‚¹ãƒ†ãƒƒãƒ—ã‚’é–¾å€¤ã«åŸºã¥ã„ã¦ç°¡æ˜“çš„ã«è¨ˆç®—
    # (kl_threshold=0.01 -> 0.8), (kl_threshold=0.1 -> 0.6)
    redundancy_start_ratio = min(0.9, max(0.5, 1.0 - kl_threshold * 3.0))
    
    redundant_start_step = int(time_steps * redundancy_start_ratio)
    redundant_steps = time_steps - redundant_start_step
    
    redundancy_report: Dict[str, int] = {}
    
    # ç›£è¦–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’ç‰¹å®š
    if target_layer_names is None:
        target_layer_names = [name for name, mod in model.named_modules() if isinstance(mod, (AdaptiveLIFNeuron, IzhikevichNeuron))]
        if not target_layer_names:
             target_layer_names = [name for name, mod in model.named_modules() if isinstance(mod, (nn.Linear, nn.Conv2d))]

    for name in target_layer_names:
        redundancy_report[name] = redundant_steps

    logger.info(f"Temporal redundancy calculated (KL method stub). Proposing {redundant_steps} steps reduction (from T={redundant_start_step}).")
    return redundancy_report

@torch.no_grad()
def apply_spatio_temporal_pruning(
    model: nn.Module,
    dataloader: Any,
    time_steps: int,
    spatial_amount: float, # ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰²åˆ
    kl_threshold: float = 0.01 # KLé–¾å€¤ã‚’è¿½åŠ 
) -> nn.Module:
    """
    SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
    æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã‚’é©ç”¨ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®SNNãƒ¢ãƒ‡ãƒ«ã€‚
        dataloader (Any): KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)ã€‚
        time_steps (int): å…ƒã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€‚
        spatial_amount (float): ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé‡ã¿å‰Šé™¤ï¼‰ã®å‰²åˆã€‚
        kl_threshold (float): æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®é£½å’Œåˆ¤å®šã«ä½¿ç”¨ã™ã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹é–¾å€¤ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    logger.info(f"--- âš¡ï¸ Spatio-Temporal Pruning é–‹å§‹ (Spatial Amount: {spatial_amount:.1%}, KL Threshold: {kl_threshold}) ---")
    
    # --- 1. æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Temporal Pruning) ---
    # å¼•ç”¨[19]ã«åŸºã¥ãã€å†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹
    redundancy_report = _calculate_temporal_redundancy(
        model, dataloader, time_steps, kl_threshold=kl_threshold
    )
    
    avg_redundant_steps: int = 0
    if redundancy_report:
        avg_redundant_steps = int(sum(redundancy_report.values()) / len(redundancy_report))

    new_time_steps = time_steps - avg_redundant_steps
    
    logger.info(f"  [Temporal Pruning (Stub)]: æ¨å®šå‰Šæ¸›å¯èƒ½ã‚¹ãƒ†ãƒƒãƒ—æ•°: {avg_redundant_steps}. (T={time_steps} -> T={new_time_steps})")
    
    # (ã‚¹ã‚¿ãƒ–: å®Ÿéš›ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«å†…ã® time_steps ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã™ã‚‹)
    # (ä¾‹: model.time_steps = new_time_steps)
    if hasattr(model, 'time_steps'):
         logger.info(f"  [Temporal Pruning (Stub)]: Updating model.time_steps to {new_time_steps}")
         # (æ³¨: ã“ã®æ“ä½œã¯ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã«å¼·ãä¾å­˜ã™ã‚‹ãŸã‚æ³¨æ„)
         # model.time_steps = new_time_steps
    
    # --- 2. ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatial Pruning) ---
    # å¼•ç”¨[19]ã®LAMPSãƒ™ãƒ¼ã‚¹ã€ã¾ãŸã¯å˜ç´”ãªMagnitudeãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
    logger.info("  [Spatial Pruning (Magnitude Stub)]: é‡ã¿ã®ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    
    total_pruned = 0
    total_params = 0
    
    for module in model.modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            if hasattr(module, 'weight'): # é‡ã¿ãŒã‚ã‚‹ã‹ç¢ºèª
                param: torch.Tensor = module.weight
                
                num_to_prune = int(param.numel() * spatial_amount)
                if num_to_prune == 0:
                    continue
                
                # å˜ç´”ãª Magnitude Pruning (ã‚¹ã‚¿ãƒ–)
                threshold = torch.kthvalue(param.data.abs().view(-1), k=num_to_prune).values
                mask = param.data.abs() > threshold
                param.data *= mask.float()
                
                pruned_count = param.numel() - mask.sum().item()
                total_pruned += int(pruned_count)
                total_params += param.numel()

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"  [Spatial Pruning]: {actual_sparsity:.2%} ({total_pruned} / {total_params}) ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
    
    logger.info("--- âœ… Spatio-Temporal Pruning å®Œäº† (Stub) ---")
    return model
