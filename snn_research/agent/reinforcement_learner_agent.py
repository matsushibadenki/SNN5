# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/agent/reinforcement_learner_agent.py
# (æ›´æ–°)
# æ”¹å–„ç‚¹:
# - `learn`ãƒ¡ã‚½ãƒƒãƒ‰ã«`causal_credit`å¼•æ•°ã‚’è¿½åŠ ã€‚
# - ã“ã®å¼•æ•°ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã€é€šå¸¸ã®å ±é…¬ã‚ˆã‚Šã‚‚å„ªå…ˆã—ã€
#   ã‚ˆã‚Šå¤§ããªå­¦ç¿’ç‡ã§é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã€‚
#
# æ”¹å–„ (v2):
# - doc/The-flow-of-brain-behavior.md ã¨ã®æ•´åˆæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€
#   ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãŸå­¦ç¿’ãƒ«ãƒ¼ãƒ« (R-STDP) ã‚’å‰Šé™¤ã€‚
# - __init__ ãŒ `synaptic_rule` ã¨ `homeostatic_rule` ã‚’
#   å¤–éƒ¨ã‹ã‚‰ï¼ˆDIã‚³ãƒ³ãƒ†ãƒŠçµŒç”±ã§ï¼‰å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
# - ã“ã‚Œã«ã‚ˆã‚Šã€snn_research/bio_models/simple_network.py (v2) ã®
#   å®‰å®šåŒ–æ©Ÿæ§‹ã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

import torch
# --- â–¼ æ”¹å–„ (v2): å¿…è¦ãªå‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ  â–¼ ---
from typing import Dict, Any, List, Optional
# --- â–² æ”¹å–„ (v2) â–² ---

from snn_research.bio_models.simple_network import BioSNN
# --- â–¼ æ”¹å–„ (v2): BioLearningRule ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from snn_research.learning_rules.base_rule import BioLearningRule
# --- â–² æ”¹å–„ (v2) â–² ---
from snn_research.communication import SpikeEncoderDecoder

class ReinforcementLearnerAgent:
    """
    BioSNNã¨å ±é…¬å¤‰èª¿å‹STDPã‚’ç”¨ã„ã€ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®å› æœã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã§å­¦ç¿’ãŒå¤‰èª¿ã•ã‚Œã‚‹å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    """
    # --- â–¼ æ”¹å–„ (v2): __init__ ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’å¤‰æ›´ â–¼ ---
    def __init__(
        self, 
        input_size: int, 
        output_size: int, 
        device: str,
        synaptic_rule: BioLearningRule, # å¤–éƒ¨ã‹ã‚‰æ³¨å…¥
        homeostatic_rule: Optional[BioLearningRule] = None # å¤–éƒ¨ã‹ã‚‰æ³¨å…¥
    ):
    # --- â–² æ”¹å–„ (v2) â–² ---
        self.device = device
        
        # --- â–¼ æ”¹å–„ (v2): ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå­¦ç¿’ãƒ«ãƒ¼ãƒ«ã‚’å‰Šé™¤ â–¼ ---
        # learning_rule = RewardModulatedSTDP(
        #     learning_rate=0.005, a_plus=1.0, a_minus=1.0,
        #     tau_trace=20.0, tau_eligibility=50.0
        # )
        # --- â–² æ”¹å–„ (v2) â–² ---
        
        hidden_size = (input_size + output_size) * 2
        layer_sizes = [input_size, hidden_size, output_size]
        
        self.model = BioSNN(
            layer_sizes=layer_sizes,
            neuron_params={'tau_mem': 10.0, 'v_threshold': 1.0, 'v_reset': 0.0, 'v_rest': 0.0},
            # --- â–¼ æ”¹å–„ (v2): æ³¨å…¥ã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«ã‚’ä½¿ç”¨ â–¼ ---
            synaptic_rule=synaptic_rule,
            homeostatic_rule=homeostatic_rule
            # --- â–² æ”¹å–„ (v2) â–² ---
        ).to(device)

        self.encoder = SpikeEncoderDecoder(num_neurons=input_size, time_steps=1)
        self.experience_buffer: List[List[torch.Tensor]] = []

    def get_action(self, state: torch.Tensor) -> int:
        """
        ç¾åœ¨ã®çŠ¶æ…‹ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«ã‚ˆã£ã¦å˜ä¸€ã®è¡Œå‹•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®šã™ã‚‹ã€‚
        """
        self.model.eval()
        with torch.no_grad():
            input_spikes = (torch.rand_like(state) < (state * 0.5 + 0.5)).float()
            output_spikes, hidden_spikes_history = self.model(input_spikes)
            self.experience_buffer.append([input_spikes] + hidden_spikes_history)
            action = torch.argmax(output_spikes).item()
            return int(action)

    # --- â–¼ ä¿®æ­£ â–¼ ---
    def learn(self, reward: float, causal_credit: float = 0.0):
        """
        å—ã‘å–ã£ãŸå ±é…¬ä¿¡å·ã¾ãŸã¯å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã€‚
        """
        if not self.experience_buffer:
            return

        self.model.train()
        
        # å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãã‚Œã‚’å„ªå…ˆã—ã€å­¦ç¿’ã‚’å¢—å¹…ã•ã›ã‚‹
        if causal_credit > 0:
            # ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã¯é€šå¸¸ã®å ±é…¬ã‚ˆã‚Šã‚‚å¼·åŠ›ãªå­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼ã¨ã™ã‚‹
            final_reward_signal = reward + causal_credit * 10.0 
            print(f"ğŸ§  ã‚·ãƒŠãƒ—ã‚¹å­¦ç¿’å¢—å¼·ï¼ (Causal Credit: {causal_credit})")
        else:
            final_reward_signal = reward
            
        optional_params = {"reward": final_reward_signal}
        
        for step_spikes in self.experience_buffer:
            self.model.update_weights(
                all_layer_spikes=step_spikes,
                optional_params=optional_params
            )
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚äº†ã€ã¾ãŸã¯å¼·åŠ›ãªå­¦ç¿’ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸã‚‰ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
        if reward != -0.05 or causal_credit > 0:
            self.experience_buffer = []
    # --- â–² ä¿®æ­£ â–² ---