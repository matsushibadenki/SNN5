# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/run_cifar10_experiment.py
# (æ–°è¦ä½œæˆ)
# Title: CIFAR-10 ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯å®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description:
# snn_4_ann_parity_plan.mdã®ã€Œå„ªå…ˆå®Ÿé¨“æ¡ˆã€ã«åŸºã¥ãã€CIFAR-10ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§
# ANNãƒ¢ãƒ‡ãƒ«ã¨SNNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãƒ»è©•ä¾¡ã‚’è¡Œã„ã€æ€§èƒ½ã‚’ç›´æŽ¥æ¯”è¼ƒã™ã‚‹ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
# ä¿®æ­£(mypy): [import-untyped], [name-defined]ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã€‚

import argparse
import time
import pandas as pd  # type: ignore
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
from pathlib import Path
import sys
from typing import Dict

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent))

from snn_research.benchmark import TASK_REGISTRY
from app.utils import get_auto_device
from transformers import AutoTokenizer

def train_and_evaluate_model(
    model_type: str,
    task,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: str,
    epochs: int,
    learning_rate: float
) -> Dict:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®è¨“ç·´ã¨è©•ä¾¡ã‚’è¡Œã†ã€‚
    """
    print("\n" + "="*20 + f" ðŸš€ Starting Experiment for: {model_type} " + "="*20)
    
    # vocab_sizeã¯ç”»åƒã‚¿ã‚¹ã‚¯ã§ã¯ä½¿ç”¨ã—ãªã„ãŒã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’åˆã‚ã›ã‚‹ãŸã‚ã«æ¸¡ã™
    model = task.build_model(model_type, vocab_size=10).to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    
    best_accuracy = 0.0
    
    for epoch in range(epochs):
        # è¨“ç·´ãƒ«ãƒ¼ãƒ—
        model.train()
        train_loss = 0.0
        train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [{model_type} Training]")
        for batch in train_progress:
            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
            labels = batch['labels'].to(device)

            optimizer.zero_grad()
            
            # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›å½¢å¼ã«åˆã‚ã›ã¦logitsã‚’å–å¾—
            outputs = model(**inputs)
            logits = outputs[0] if isinstance(outputs, tuple) else outputs
            
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            train_progress.set_postfix({"loss": f"{loss.item():.4f}"})
            
    # è©•ä¾¡
    print(f"\n--- Evaluating {model_type} model ---")
    start_time = time.time()
    metrics = task.evaluate(model, val_loader)
    duration = time.time() - start_time
    
    metrics["model"] = model_type
    metrics["eval_time_sec"] = duration
    
    print(f"  - Results: {metrics}")
    return metrics


def main():
    parser = argparse.ArgumentParser(description="SNN vs ANN CIFAR-10 Experiment")
    parser.add_argument("--epochs", type=int, default=3, help="Number of training epochs for demonstration.")
    parser.add_argument("--batch_size", type=int, default=32, help="Training and validation batch size.")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate for the optimizer.")
    args = parser.parse_args()

    device = get_auto_device()
    print(f"Using device: {device}")
    
    # CIFAR-10ã‚¿ã‚¹ã‚¯ã®æº–å‚™
    TaskClass = TASK_REGISTRY["cifar10"]
    # tokenizerã¯ä¸è¦ã ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’åˆã‚ã›ã‚‹ãŸã‚ã«ãƒ€ãƒŸãƒ¼ã‚’æ¸¡ã™
    task = TaskClass(tokenizer=AutoTokenizer.from_pretrained("gpt2"), device=device, hardware_profile={})
    
    train_dataset, val_dataset = task.prepare_data()
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=task.get_collate_fn(), shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=task.get_collate_fn())

    all_results = []
    
    # ANNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
    ann_metrics = train_and_evaluate_model(
        'ANN', task, train_loader, val_loader, device, args.epochs, args.learning_rate
    )
    all_results.append(ann_metrics)
    
    # SNNãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨è©•ä¾¡
    snn_metrics = train_and_evaluate_model(
        'SNN', task, train_loader, val_loader, device, args.epochs, args.learning_rate
    )
    all_results.append(snn_metrics)

    # æœ€çµ‚æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆã®è¡¨ç¤º
    print("\n\n" + "="*25 + " ðŸ† Final Comparison Summary " + "="*25)
    df = pd.DataFrame(all_results)
    
    snn_energy = df[df['model'] == 'SNN']['estimated_energy_j'].iloc[0]
    ann_energy = df[df['model'] == 'ANN']['estimated_energy_j'].iloc[0]
    if ann_energy > 0 and snn_energy is not None:
        efficiency_gain = (1 - (snn_energy / ann_energy)) * 100
        df['efficiency_gain_%'] = [f"{efficiency_gain:.2f}%" if m == 'SNN' else '-' for m in df['model']]

    print(df.to_string())
    print("="*90)


if __name__ == "__main__":
    main()