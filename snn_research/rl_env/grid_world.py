# snn_research/rl_env/grid_world.py
# (æ–°è¦ä½œæˆ)
# Title: Grid World ç’°å¢ƒ
# Description: ROADMAPãƒ•ã‚§ãƒ¼ã‚º2ã€ŒéšŽå±¤çš„å› æžœå­¦ç¿’ã€ã®æ¤œè¨¼ã®ãŸã‚ã€
#              è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®è¡Œå‹•é¸æŠžã‚’å¿…è¦ã¨ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªè¿·è·¯æŽ¢ç´¢ã‚¿ã‚¹ã‚¯ã‚’æä¾›ã™ã‚‹ã€‚

import torch
from typing import Tuple

class GridWorldEnv:
    """
    ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚´ãƒ¼ãƒ«ã‚’ç›®æŒ‡ã™ã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ç’°å¢ƒã€‚
    """
    def __init__(self, size: int = 5, max_steps: int = 50, device: str = 'cpu'):
        self.size = size
        self.max_steps = max_steps
        self.device = device
        
        self.agent_pos = torch.zeros(2, device=self.device, dtype=torch.long)
        self.goal_pos = torch.zeros(2, device=self.device, dtype=torch.long)
        
        self.current_step = 0
        self.reset()

    def _get_state(self) -> torch.Tensor:
        """ç¾åœ¨ã®çŠ¶æ…‹ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½ç½®ã¨ã‚´ãƒ¼ãƒ«ä½ç½®ï¼‰ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¿”ã™ã€‚"""
        # çŠ¶æ…‹ã‚’æ­£è¦åŒ–ã—ã¦ [-1, 1] ã®ç¯„å›²ã«ã™ã‚‹
        state = torch.cat([
            (self.agent_pos / (self.size - 1)) * 2 - 1,
            (self.goal_pos / (self.size - 1)) * 2 - 1
        ]).float()
        return state

    def reset(self) -> torch.Tensor:
        """ç’°å¢ƒã‚’ãƒªã‚»ãƒƒãƒˆã—ã€æ–°ã—ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã‚´ãƒ¼ãƒ«ã®ä½ç½®ã‚’è¨­å®šã™ã‚‹ã€‚"""
        self.agent_pos = torch.randint(0, self.size, (2,), device=self.device)
        self.goal_pos = torch.randint(0, self.size, (2,), device=self.device)
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã‚´ãƒ¼ãƒ«ãŒåŒã˜å ´æ‰€ã‹ã‚‰å§‹ã¾ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹
        while torch.equal(self.agent_pos, self.goal_pos):
            self.goal_pos = torch.randint(0, self.size, (2,), device=self.device)
        
        self.current_step = 0
        
        # print(f"ðŸŒ New Grid World: Agent at {self.agent_pos.cpu().numpy()}, Goal at {self.goal_pos.cpu().numpy()}")
        return self._get_state()

    def step(self, action: int) -> Tuple[torch.Tensor, float, bool]:
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’å—ã‘å–ã‚Šã€(æ¬¡ã®çŠ¶æ…‹, å ±é…¬, å®Œäº†ãƒ•ãƒ©ã‚°) ã‚’è¿”ã™ã€‚

        Args:
            action (int): 0:ä¸Š, 1:ä¸‹, 2:å·¦, 3:å³

        Returns:
            Tuple[torch.Tensor, float, bool]: (æ¬¡ã®çŠ¶æ…‹, å ±é…¬, å®Œäº†ãƒ•ãƒ©ã‚°)ã€‚
        """
        self.current_step += 1

        # è¡Œå‹•ã«åŸºã¥ã„ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç§»å‹•
        if action == 0: # ä¸Š
            self.agent_pos[1] += 1
        elif action == 1: # ä¸‹
            self.agent_pos[1] -= 1
        elif action == 2: # å·¦
            self.agent_pos[0] -= 1
        elif action == 3: # å³
            self.agent_pos[0] += 1
        
        # ã‚°ãƒªãƒƒãƒ‰ã®å¢ƒç•Œå†…ã«åŽã‚ã‚‹
        self.agent_pos = torch.clamp(self.agent_pos, 0, self.size - 1)

        # å ±é…¬ã®è¨ˆç®—
        if torch.equal(self.agent_pos, self.goal_pos):
            reward = 1.0  # ã‚´ãƒ¼ãƒ«ã«åˆ°é”
            done = True
        else:
            reward = -0.05  # ç§»å‹•ã‚³ã‚¹ãƒˆ
            done = False

        # æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
        if self.current_step >= self.max_steps:
            done = True
        
        next_state = self._get_state()

        return next_state, reward, done