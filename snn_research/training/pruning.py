# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/pruning.py
# (æ”¹ä¿®: SBC ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£…)
# Title: æ§‹é€ çš„ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (SBC - Spiking Brain Compression)
# Description:
# doc/SNNé–‹ç™ºï¼šåŸºæœ¬è¨­è¨ˆæ€æƒ³.md (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.4, å¼•ç”¨[15]) ã«åŸºã¥ãã€
# é«˜ã‚³ã‚¹ãƒˆãªåå¾©ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ãªãã€ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆæå¤±ã®äºŒæ¬¡å¾®åˆ†ï¼‰ã‚’åˆ©ç”¨ã—ãŸ
# ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSBCï¼‰ã‚’å®Ÿè£…ã™ã‚‹ã€‚
#
# å®Ÿè£…æ¦‚è¦ (ã‚¹ã‚¿ãƒ–):
# 1. (apply_sbc_pruning): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®å±¤ã‚’ç‰¹å®šã™ã‚‹ã€‚
# 2. (_compute_hessian_diag): æå¤±ã®äºŒæ¬¡å¾®åˆ†ï¼ˆãƒ˜ãƒƒã‚»è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚(ãƒ€ãƒŸãƒ¼å®Ÿè£…)
# 3. (_compute_saliency): ãƒ˜ãƒƒã‚»è¡Œåˆ—ã«åŸºã¥ãã€å„é‡ã¿ã®é‡è¦åº¦ï¼ˆSaliencyï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
# 4. (prune_and_update_weights): é‡è¦åº¦ãŒä½ã„é‡ã¿ã‚’å‰Šé™¤ï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã—ã€
#    æ®‹ã£ãŸé‡ã¿ã‚’è£œæ­£ï¼ˆUpdateï¼‰ã—ã¦æå¤±ã®å¢—åŠ ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã€‚
#
# è¿½åŠ  (v2):
# - SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
#   æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã®ã‚¹ã‚¿ãƒ–ã‚’è¿½åŠ ã€‚
#
# æ”¹å–„ (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1 å¯¾å¿œ):
# - _calculate_temporal_redundancy ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…ã‚’ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«
#   åŸºã¥ãé£½å’Œåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã®ã‚¹ã‚¿ãƒ–ï¼‰ã«æ”¹å–„ã€‚

import torch
import torch.nn as nn
# --- â–¼ ä¿®æ­£: å¿…è¦ãªå‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import List, Tuple, Dict, Any, cast, Optional, Type
import logging 
# --- â–² ä¿®æ­£ â–² ---
# --- â–¼ ä¿®æ­£: SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1 å¯¾å¿œ â–¼ ---
from snn_research.core.neurons import AdaptiveLIFNeuron, IzhikevichNeuron
import torch.nn.functional as F
# --- â–² ä¿®æ­£ â–² ---


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _compute_hessian_diag(model: nn.Module, loss_fn: nn.Module, dataloader: Any) -> Dict[str, torch.Tensor]:
    """
    ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‚’è¨ˆç®—ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚
    å®Ÿéš›ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã„ã€
    ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’2å›è¡Œã†ãªã©ã®æ‰‹æ³•ï¼ˆä¾‹: L-BFGSï¼‰ãŒå¿…è¦ã€‚
    """
    logger.info("Computing Hessian matrix diagonal (Stub)...")
    hessian_diag: Dict[str, torch.Tensor] = {}
    
    # --- ãƒ€ãƒŸãƒ¼å®Ÿè£… ---
    # å®Ÿéš›ã«ã¯ã“ã“ã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æ•°ãƒãƒƒãƒå›ã—ã€
    # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® (d^2 L / d w^2) ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    for name, param in model.named_parameters():
        if "weight" in name and param.requires_grad and param.dim() > 1:
            # å¯¾è§’æˆåˆ†ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨åŒã˜å½¢çŠ¶ã‚’æŒã¤
            # ãƒ€ãƒŸãƒ¼ã¨ã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§ãã•ã«å¿œã˜ãŸãƒ©ãƒ³ãƒ€ãƒ ãªæ­£ã®å€¤ã‚’è¨­å®š
            hessian_diag[name] = torch.rand_like(param) * 0.1 + (param.data.abs() * 0.5) + 1e-6
    # --- ãƒ€ãƒŸãƒ¼å®Ÿè£…çµ‚äº† ---
    
    logger.info(f"Hessian diagonal computed (dummy) for {len(hessian_diag)} layers.")
    return hessian_diag

def _compute_saliency(param: torch.Tensor, hessian_diag: torch.Tensor) -> torch.Tensor:
    """
    SBC (Optimal Brain Compression) ã«åŸºã¥ãé‡ã¿ã®é‡è¦åº¦ï¼ˆSaliencyï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    Saliency = (1/2) * (w^2) * (H_ii)
    """
    return 0.5 * (param.data ** 2) * hessian_diag

@torch.no_grad()
def _prune_and_update_weights(
    module: nn.Module,
    param_name: str,
    saliency: torch.Tensor,
    amount: float
) -> Tuple[int, int]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€é‡ã¿ã‚’è£œæ­£ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚
    """
    param: torch.Tensor = getattr(module, param_name)
    
    # 1. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã‚’æ±ºå®š
    num_to_prune = int(param.numel() * amount)
    if num_to_prune == 0:
        return 0, param.numel()
        
    threshold = torch.kthvalue(saliency.view(-1), k=num_to_prune).values
    mask = saliency > threshold
    
    # 2. é‡ã¿è£œæ­£ (SBCã®æ ¸å¿ƒéƒ¨ - ãƒ€ãƒŸãƒ¼å®Ÿè£…)
    # å®Ÿéš›ã«ã¯ã€SBCã¯å‰Šé™¤ã™ã‚‹é‡ã¿ (w_j) ãŒæ®‹ã‚Šã®é‡ã¿ (w_i) ã«
    # ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ (H_ij) ã‚’è€ƒæ…®ã—ã¦ w_i ã‚’æ›´æ–°ã™ã‚‹ã€‚
    # delta_w_i = - (H_ii)^-1 * H_ij * w_j
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€æ®‹ã£ãŸé‡ã¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    
    # ç°¡æ˜“è£œæ­£: å‰Šé™¤ã•ã‚Œã‚‹é‡ã¿ã®ç·å’Œã‚’æ®‹ã‚Šã®é‡ã¿ã§å‰²ã£ãŸå€¤ã‚’ã€
    # å­¦ç¿’ç‡ã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦åŠ ç®—ã™ã‚‹ï¼ˆç”Ÿç‰©å­¦çš„å¯å¡‘æ€§ã«è¿‘ã„ãƒ€ãƒŸãƒ¼è£œæ­£ï¼‰
    # update_factor = (param.data * ~mask).sum() / (param.data * mask).sum().clamp(min=1e-6)
    # param.data[mask] += param.data[mask] * update_factor * 0.01 # 1%è£œæ­£
    
    # 3. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒã‚¹ã‚¯ã‚’é©ç”¨)
    param.data *= mask.float()
    
    original_count = param.numel()
    pruned_count = original_count - mask.sum().item()
    return int(pruned_count), original_count

def apply_sbc_pruning(
    model: nn.Module,
    amount: float,
    dataloader_stub: Any, # ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)
    loss_fn_stub: nn.Module # æå¤±é–¢æ•° (ã‚¹ã‚¿ãƒ–)
) -> nn.Module:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã€SBC (Spiking Brain Compression) ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã€‚

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚
        amount (float): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã®å‰²åˆ (0.0ã‹ã‚‰1.0ã®é–“)ã€‚
        dataloader_stub (Any): ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ç¾åœ¨ã¯æœªä½¿ç”¨)ã€‚
        loss_fn_stub (nn.Module): æå¤±é–¢æ•° (ç¾åœ¨ã¯æœªä½¿ç”¨)ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    if not (0.0 < amount < 1.0):
        logger.warning(f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é‡ãŒç„¡åŠ¹ã§ã™ ({amount})ã€‚0.0ã‹ã‚‰1.0ã®é–“ã®å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return model

    logger.info(f"--- ğŸ§  Spiking Brain Compression (SBC) é–‹å§‹ (Amount: {amount:.1%}) ---")

    # 1. ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆå¯¾è§’æˆåˆ†ï¼‰ã‚’è¨ˆç®— (ã‚¹ã‚¿ãƒ–)
    hessian_diagonals = _compute_hessian_diag(model, loss_fn_stub, dataloader_stub)
    
    total_pruned = 0
    total_params = 0

    # 2. å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡ã¿è£œæ­£ã‚’å®Ÿè¡Œ
    # (ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ãªãã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«æŒ‡å®šã•ã‚ŒãŸå‰²åˆã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°)
    target_modules: List[Tuple[nn.Module, str]] = []
    for module in model.modules():
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
            target_modules.append((module, 'weight'))

    if not target_modules:
        logger.warning("ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return model

    logger.info(f"SBCå¯¾è±¡ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {len(target_modules)}")
    
    for module, param_name in target_modules:
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’å–å¾— (mypyäº’æ›ã®ãŸã‚ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨)
        full_param_name: str = ""
        for name, mod in model.named_modules():
             if mod is module:
                 full_param_name = f"{name}.{param_name}"
                 break
        
        if not full_param_name:
             logger.warning(f"  - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
             continue

        if full_param_name in hessian_diagonals:
            param: torch.Tensor = getattr(module, param_name)
            hessian_diag = hessian_diagonals[full_param_name]
            
            # 3. é‡è¦åº¦ã‚’è¨ˆç®—
            saliency = _compute_saliency(param, hessian_diag)
            
            # 4. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡ã¿è£œæ­£ (ã‚¹ã‚¿ãƒ–)
            pruned, total = _prune_and_update_weights(module, param_name, saliency, amount)
            total_pruned += pruned
            total_params += total
            logger.info(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': {pruned}/{total} ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (è£œæ­£å®Ÿè¡Œæ¸ˆã‚¹ã‚¿ãƒ–)ã€‚")
        else:
            logger.warning(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': ãƒ˜ãƒƒã‚»è¡Œåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"--- âœ… SBC å®Œäº† ---")
        logger.info(f"  - åˆè¨ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ç‡: {actual_sparsity:.2%} ({total_pruned} / {total_params})")
    else:
        logger.error("--- âŒ SBC å¤±æ•—: å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ0ã§ã—ãŸ ---")

    return model

# --- â–¼â–¼â–¼ SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ãè¿½åŠ å®Ÿè£… â–¼â–¼â–¼ ---

@torch.no_grad()
def _calculate_temporal_redundancy(
    model: nn.Module, 
    dataloader: Any, 
    time_steps: int,
    target_layer_names: Optional[List[str]] = None, # ç›£è¦–å¯¾è±¡ã®LIFå±¤ãªã©
    kl_threshold: float = 0.01 # é£½å’Œã¨ã¿ãªã™KLç™ºæ•£ã®é–¾å€¤
) -> Dict[str, int]:
    """
    (æ”¹å–„) SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€‚
    KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ç›£è¦–ã—ã€æƒ…å ±ãŒé£½å’Œã—ãŸå†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹ã€‚
    
    (æ³¨: å®Ÿéš›ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ã¯è¤‡é›‘ãªãƒ•ãƒƒã‚¯ã¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ä¼´ã†ãŸã‚ã€
     ã“ã“ã§ã¯ãã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œçµæœã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¿ãƒ–ã‚’å®Ÿè£…ã—ã¾ã™)

    Returns:
        Dict[str, int]: ãƒ¬ã‚¤ãƒ¤ãƒ¼åã¨ã€ãã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‰Šæ¸›å¯èƒ½ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¾æ›¸ã€‚
    """
    logger.info(f"Calculating temporal redundancy (KL divergence method, threshold={kl_threshold})...")
    
    # --- (ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®æ”¹å–„) ---
    # å®Ÿéš›ã«ã¯ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã€ãƒ•ãƒƒã‚¯ã‚’ä½¿ã£ã¦
    # å„ `target_layer_names` ã®ã‚¹ãƒ‘ã‚¤ã‚¯å‡ºåŠ› (T, B, F) ã‚’åé›†ã™ã‚‹ã€‚
    #
    # for t in range(time_steps - 1):
    #   p_t = spike_history[t].mean(dim=(0, 1)) # (F,)
    #   p_t_plus_1 = spike_history[t+1].mean(dim=(0, 1))
    #   # ã‚¼ãƒ­ã‚’é¿ã‘ã‚‹ãŸã‚ã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    #   p_t = (p_t + 1e-6) / (1.0 + 1e-6 * F)
    #   p_t_plus_1 = (p_t_plus_1 + 1e-6) / (1.0 + 1e-6 * F)
    #   
    #   kl_div = F.kl_div(p_t_plus_1.log(), p_t, reduction='sum')
    #   
    #   if kl_div < kl_threshold:
    #       redundant_start_step = t + 1
    #       break
    
    # (ã“ã“ã§ã¯ã€ãã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œçµæœã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹)
    
    # KLé–¾å€¤ãŒå°ã•ã„ã»ã©ã€é£½å’Œæ¤œå‡ºãŒå³ã—ããªã‚Šã€å†—é•·ã‚¹ãƒ†ãƒƒãƒ—ã¯å°‘ãªããªã‚‹
    # KLé–¾å€¤ãŒå¤§ãã„ã»ã©ã€é£½å’Œæ¤œå‡ºãŒç·©ããªã‚Šã€å†—é•·ã‚¹ãƒ†ãƒƒãƒ—ã¯å¤šããªã‚‹
    
    # å†—é•·ãªé–‹å§‹ã‚¹ãƒ†ãƒƒãƒ—ã‚’é–¾å€¤ã«åŸºã¥ã„ã¦ç°¡æ˜“çš„ã«è¨ˆç®—
    # (kl_threshold=0.01 -> 0.8), (kl_threshold=0.1 -> 0.6)
    redundancy_start_ratio = min(0.9, max(0.5, 1.0 - kl_threshold * 3.0))
    
    redundant_start_step = int(time_steps * redundancy_start_ratio)
    redundant_steps = time_steps - redundant_start_step
    
    redundancy_report: Dict[str, int] = {}
    
    # ç›£è¦–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’ç‰¹å®š
    if target_layer_names is None:
        target_layer_names = [name for name, mod in model.named_modules() if isinstance(mod, (AdaptiveLIFNeuron, IzhikevichNeuron))]
        if not target_layer_names:
             target_layer_names = [name for name, mod in model.named_modules() if isinstance(mod, (nn.Linear, nn.Conv2d))]

    for name in target_layer_names:
        redundancy_report[name] = redundant_steps

    logger.info(f"Temporal redundancy calculated (KL method stub). Proposing {redundant_steps} steps reduction (from T={redundant_start_step}).")
    return redundancy_report

@torch.no_grad()
def apply_spatio_temporal_pruning(
    model: nn.Module,
    dataloader: Any,
    time_steps: int,
    spatial_amount: float, # ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰²åˆ
    kl_threshold: float = 0.01 # KLé–¾å€¤ã‚’è¿½åŠ 
) -> nn.Module:
    """
    SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
    æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã‚’é©ç”¨ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®SNNãƒ¢ãƒ‡ãƒ«ã€‚
        dataloader (Any): KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)ã€‚
        time_steps (int): å…ƒã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€‚
        spatial_amount (float): ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé‡ã¿å‰Šé™¤ï¼‰ã®å‰²åˆã€‚
        kl_threshold (float): æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®é£½å’Œåˆ¤å®šã«ä½¿ç”¨ã™ã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹é–¾å€¤ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    logger.info(f"--- âš¡ï¸ Spatio-Temporal Pruning é–‹å§‹ (Spatial Amount: {spatial_amount:.1%}, KL Threshold: {kl_threshold}) ---")
    
    # --- 1. æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Temporal Pruning) ---
    # å¼•ç”¨[19]ã«åŸºã¥ãã€å†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹
    redundancy_report = _calculate_temporal_redundancy(
        model, dataloader, time_steps, kl_threshold=kl_threshold
    )
    
    avg_redundant_steps: int = 0
    if redundancy_report:
        avg_redundant_steps = int(sum(redundancy_report.values()) / len(redundancy_report))

    new_time_steps = time_steps - avg_redundant_steps
    
    logger.info(f"  [Temporal Pruning (Stub)]: æ¨å®šå‰Šæ¸›å¯èƒ½ã‚¹ãƒ†ãƒƒãƒ—æ•°: {avg_redundant_steps}. (T={time_steps} -> T={new_time_steps})")
    
    # (ã‚¹ã‚¿ãƒ–: å®Ÿéš›ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«å†…ã® time_steps ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã™ã‚‹)
    # (ä¾‹: model.time_steps = new_time_steps)
    if hasattr(model, 'time_steps'):
         logger.info(f"  [Temporal Pruning (Stub)]: Updating model.time_steps to {new_time_steps}")
         # (æ³¨: ã“ã®æ“ä½œã¯ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã«å¼·ãä¾å­˜ã™ã‚‹ãŸã‚æ³¨æ„)
         # model.time_steps = new_time_steps
    
    # --- 2. ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatial Pruning) ---
    # å¼•ç”¨[19]ã®LAMPSãƒ™ãƒ¼ã‚¹ã€ã¾ãŸã¯å˜ç´”ãªMagnitudeãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
    logger.info("  [Spatial Pruning (Magnitude Stub)]: é‡ã¿ã®ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    
    total_pruned = 0
    total_params = 0
    
    for module in model.modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            if hasattr(module, 'weight'): # é‡ã¿ãŒã‚ã‚‹ã‹ç¢ºèª
                param: torch.Tensor = module.weight
                
                num_to_prune = int(param.numel() * spatial_amount)
                if num_to_prune == 0:
                    continue
                
                # å˜ç´”ãª Magnitude Pruning (ã‚¹ã‚¿ãƒ–)
                threshold = torch.kthvalue(param.data.abs().view(-1), k=num_to_prune).values
                mask = param.data.abs() > threshold
                param.data *= mask.float()
                
                pruned_count = param.numel() - mask.sum().item()
                total_pruned += int(pruned_count)
                total_params += param.numel()

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"  [Spatial Pruning]: {actual_sparsity:.2%} ({total_pruned} / {total_params}) ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
    
    logger.info("--- âœ… Spatio-Temporal Pruning å®Œäº† (Stub) ---")
    return model
