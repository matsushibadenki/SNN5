# matsushibadenki/snn_research/conversion/ann_to_snn_converter.py
# (æ›´æ–°)
# GGUF/Safetensorså½¢å¼ã®ANNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰SNNã¸ã®å¤‰æ›ãƒ»è’¸ç•™ã‚’è¡Œã†ã‚³ãƒ³ãƒãƒ¼ã‚¿
#
# æ©Ÿèƒ½:
# - [æ”¹å–„ v3] å …ç‰¢ãªå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã€‚BatchNorm Folding, å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼,
#   ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ­ã‚®ãƒ³ã‚°ã‚’å°å…¥ã€‚
# - [æ”¹å–„ v3] LLMå¤‰æ›ã®éç¾å®Ÿæ€§ã‚’æ˜ç¢ºåŒ–ã—ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from safetensors.torch import load_file
from tqdm import tqdm
from typing import Dict, Any, Optional
import logging
from transformers import AutoModelForCausalLM

from snn_research.core.snn_core import AdaptiveLIFNeuron
from .conversion_utils import safe_copy_weights, calibrate_thresholds_by_percentile
from .fold_bn import fold_all_batchnorms

# GGUFã®ä¾å­˜é–¢ä¿‚ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã™ã‚‹
try:
    from gguf import GGUFReader
    GGUF_AVAILABLE = True
except ImportError:
    GGUF_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def _load_gguf(path: str) -> Dict[str, torch.Tensor]:
    """GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€PyTorchã®state_dictã‚’è¿”ã™ã€‚"""
    if not GGUF_AVAILABLE:
        raise ImportError("GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ `gguf` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚`pip install gguf` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    logging.info(f"GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {path}")
    reader = GGUFReader(path, 'r')
    state_dict = {tensor.name: torch.from_numpy(tensor.data.copy()) for tensor in reader.tensors}
    logging.info(f"âœ… GGUFã‹ã‚‰ {len(state_dict)} å€‹ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
    return state_dict

class AnnToSnnConverter:
    """
    æ—¢å­˜ã®ANNãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SNNãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚
    """
    def __init__(self, snn_model: nn.Module, model_config: Dict[str, Any]):
        self.snn_model = snn_model
        self.model_config = model_config
        self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        self.snn_model.to(self.device)

    def _load_ann_weights(self, ann_model_path: str, is_llm: bool = False) -> Dict[str, torch.Tensor]:
        """ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        logging.info(f"ğŸ’¾ ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {ann_model_path}")
        if ann_model_path.endswith(".safetensors"):
            return load_file(ann_model_path, device=self.device)
        elif ann_model_path.endswith(".gguf"):
            return _load_gguf(ann_model_path)
        elif is_llm:
            try:
                model = AutoModelForCausalLM.from_pretrained(ann_model_path)
                return model.state_dict()
            except Exception as e:
                logging.error(f"Hugging Faceãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                raise
        else:
            try:
                return torch.load(ann_model_path, map_location=self.device)
            except Exception as e:
                logging.error(f"PyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                raise

    def convert_llm_weights(
        self,
        ann_model_name_or_path: str,
        output_path: str,
        calibration_loader: Optional[Any] = None
    ) -> None:
        """
        Hugging Faceã®LLMã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æ­£è¦åŒ–ã¨é«˜åº¦ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã£ã¦SNNã«å¤‰æ›ã™ã‚‹ã€‚
        """
        logging.info(f"--- ğŸš€ é«˜å¿ å®Ÿåº¦LLMå¤‰æ›é–‹å§‹: {ann_model_name_or_path} ---")
        
        # 1. ANNãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        ann_model = AutoModelForCausalLM.from_pretrained(ann_model_name_or_path).to(self.device)
        ann_model.eval()

        #
        # è­¦å‘Š: Transformerå…¨ä½“ã®å®Œå…¨ãªã‚¹ãƒ‘ã‚¤ã‚¯åŒ–ã¯éå¸¸ã«å›°é›£ã§ã™ã€‚
        # ç‰¹ã«ã€LayerNormã‚„Softmaxã‚’å«ã‚€è‡ªå·±æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®ç›´æ¥å¤‰æ›ã¯ã€
        # å¤§å¹…ãªç²¾åº¦ä½ä¸‹ã‚„ã€è†¨å¤§ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’è¦æ±‚ã™ã‚‹åŸå› ã¨ãªã‚Šã¾ã™ã€‚
        #
        # ç¾å®Ÿçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã§ã™:
        # 1. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«: Attentionãªã©è¨ˆç®—ãŒè¤‡é›‘ãªéƒ¨åˆ†ã¯ã‚¢ãƒŠãƒ­ã‚°ã®ã¾ã¾æ®‹ã—ã€
        #    FFNå±¤ãªã©ä¸€éƒ¨ã®ã¿ã‚’SNNã«ç½®ãæ›ãˆã‚‹ã€‚
        # 2. ANN-SNNå¤‰æ› + é•·æ™‚é–“ã®å¾®èª¿æ•´: é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼ã—ãŸå¾Œã€ä»£ç†å‹¾é…æ³•ã‚’
        #    ç”¨ã„ã¦SNNãƒ¢ãƒ‡ãƒ«ã‚’å†åº¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã€‚
        #
        # ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã§ã¯ã€ä¸»ã«äº’æ›æ€§ã®ã‚ã‚‹ç·šå½¢å±¤ã®å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚
        logging.warning("LLMã®å®Œå…¨ãªSNNåŒ–ã¯å®Ÿé¨“çš„ã§ã™ã€‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨å¥¨ã—ã¾ã™ã€‚")

        # 2. é‡ã¿ã‚³ãƒ”ãƒ¼
        ann_state_dict = ann_model.state_dict()
        safe_copy_weights(self.snn_model, ann_state_dict)

        # 3. é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if calibration_loader:
            logging.info("LLMã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã«åŸºã¥ãé–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™...")
            thresholds = calibrate_thresholds_by_percentile(ann_model, calibration_loader, device=self.device)
            # ã“ã“ã§ã€è¨ˆç®—ã•ã‚ŒãŸé–¾å€¤ã‚’SNNãƒ¢ãƒ‡ãƒ«ã®å„LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«è¨­å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
            # ä¾‹: for name, module in self.snn_model.named_modules(): ...
            logging.info(f"è¨ˆç®—ã•ã‚ŒãŸé–¾å€¤: {thresholds}")
        else:
            logging.warning("ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€é–¾å€¤èª¿æ•´ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ç²¾åº¦ãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        # 4. å¤‰æ›æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        logging.info(f"âœ… LLMå¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def convert_cnn_weights(
        self,
        ann_model: nn.Module,
        output_path: str,
        calibration_loader: Any
    ):
        """CNNãƒ¢ãƒ‡ãƒ«ã®é«˜å¿ å®Ÿåº¦å¤‰æ›ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
        logging.info("--- ğŸš€ é«˜å¿ å®Ÿåº¦CNNå¤‰æ›é–‹å§‹ ---")
        ann_model.to(self.device)
        ann_model.eval()

        # 1. BatchNorm Folding
        logging.info("BatchNorm Foldingã‚’å®Ÿè¡Œä¸­...")
        folded_model = fold_all_batchnorms(ann_model)
        
        # 2. é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        logging.info("ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œä¸­...")
        thresholds = calibrate_thresholds_by_percentile(folded_model, calibration_loader, device=self.device)
        
        # SNNãƒ¢ãƒ‡ãƒ«ã®å¯¾å¿œã™ã‚‹LIFå±¤ã«é–¾å€¤ã‚’è¨­å®š
        lif_layers = [m for m in self.snn_model.modules() if isinstance(m, AdaptiveLIFNeuron)]
        if len(lif_layers) == len(thresholds):
            for lif, (name, thr) in zip(lif_layers, thresholds.items()):
                lif.base_threshold.data.fill_(thr)
                logging.info(f"SNNãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é–¾å€¤ã‚’ {thr:.4f} ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        else:
            logging.warning("ANNã¨SNNã®ReLU/LIFå±¤ã®æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ã€‚é–¾å€¤è¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            
        # 3. å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼
        logging.info("å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œä¸­...")
        safe_copy_weights(self.snn_model, folded_model.state_dict())
        
        # 4. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        logging.info(f"âœ… CNNå¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")