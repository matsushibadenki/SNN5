# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/learning_rules/probabilistic_hebbian.py
# (æ–°è¦ä½œæˆ)
# Title: ç¢ºç‡çš„ãƒ˜ãƒ–å­¦ç¿’å‰‡
# Description: è«–æ–‡ arXiv:2509.26507v1 ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã«åŸºã¥ãã€
#              ç¢ºç‡çš„ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å‘ã‘ã®ãƒ˜ãƒ–å­¦ç¿’å‰‡ã‚’å®Ÿè£…ã™ã‚‹ã€‚

import torch
from typing import Dict, Any, Optional, Tuple
from .base_rule import BioLearningRule

class ProbabilisticHebbian(BioLearningRule):
    """
    ç¢ºç‡çš„ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ˜ãƒ–å­¦ç¿’å‰‡ã€‚
    ã‚·ãƒŠãƒ—ã‚¹å‰å¾Œã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒåŒæ™‚ã«(ç¢ºç‡çš„ã«)æ´»å‹•ã—ãŸå ´åˆã«çµåˆã‚’å¼·åŒ–ã™ã‚‹ã€‚
    """
    def __init__(self, learning_rate: float = 0.005, weight_decay: float = 0.0001):
        """
        Args:
            learning_rate (float): å­¦ç¿’ç‡ã€‚
            weight_decay (float): é‡ã¿ã®æ¸›è¡°ç‡ï¼ˆå®‰å®šåŒ–ã®ãŸã‚ï¼‰ã€‚
        """
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay
        print("ğŸ’¡ Probabilistic Hebbian learning rule initialized.")

    def update(
        self,
        pre_spikes: torch.Tensor,
        post_spikes: torch.Tensor,
        weights: torch.Tensor,
        optional_params: Optional[Dict[str, Any]] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        ãƒ˜ãƒ–å‰‡ã«åŸºã¥ã„ã¦é‡ã¿å¤‰åŒ–é‡ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        dw = lr * (post_spikes * pre_spikes^T - decay * weights)

        Args:
            pre_spikes (torch.Tensor): ã‚·ãƒŠãƒ—ã‚¹å‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¹ãƒ‘ã‚¤ã‚¯ (ç¢ºç‡çš„ãª0 or 1)ã€‚ (N_pre,)
            post_spikes (torch.Tensor): ã‚·ãƒŠãƒ—ã‚¹å¾Œãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¹ãƒ‘ã‚¤ã‚¯ (ç¢ºç‡çš„ãª0 or 1)ã€‚(N_post,)
            weights (torch.Tensor): ç¾åœ¨ã®é‡ã¿è¡Œåˆ—ã€‚(N_post, N_pre)
            optional_params (Optional[Dict[str, Any]]): è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ã“ã“ã§ã¯æœªä½¿ç”¨)ã€‚

        Returns:
            Tuple[torch.Tensor, Optional[torch.Tensor]]:
                (è¨ˆç®—ã•ã‚ŒãŸé‡ã¿å¤‰åŒ–é‡ (dw), é€†æ–¹å‘ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å· (None))
        """
        # ãƒ˜ãƒ–å‰‡: åŒæ™‚æ´»å‹•ã«ã‚ˆã‚‹çµåˆå¼·åŒ–é …
        # torch.outer(post_spikes, pre_spikes) ã¯ (N_post, N_pre) ã®è¡Œåˆ—ã‚’ç”Ÿæˆ
        hebbian_term = torch.outer(post_spikes, pre_spikes)

        # é‡ã¿æ¸›è¡°é … (éå‰°ãªå¼·åŒ–ã‚’é˜²ãã€å®‰å®šã•ã›ã‚‹)
        decay_term = self.weight_decay * weights

        # é‡ã¿å¤‰åŒ–é‡
        dw = self.learning_rate * (hebbian_term - decay_term)

        # ã“ã®å­¦ç¿’å‰‡ã¯å±€æ‰€çš„ãªã®ã§ã€é€†æ–¹å‘ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã¯ç”Ÿæˆã—ãªã„
        backward_credit = None

        return dw, backward_credit