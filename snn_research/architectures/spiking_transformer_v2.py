# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/architectures/spiking_transformer_v2.py
# Title: Spiking Transformer v2 (SDSAçµ±åˆç‰ˆ)
# Description: Spike-Driven Self-Attention (SDSA) ã‚’çµ„ã¿è¾¼ã‚“ã Spiking Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚
#
# ã€ä¿®æ­£ v_fix_bias_key_mappingã€‘:
# - neuron_config å†…ã®ã‚­ãƒ¼ 'NEURON_BIAS' ã‚’ AdaptiveLIFNeuron ãŒæœŸå¾…ã™ã‚‹ 'bias_init' ã«
#   ç¢ºå®Ÿã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«ãƒ­ã‚¸ãƒƒã‚¯ã‚’å¼·åŒ–ã€‚
#
# ã€ä¿®æ­£ v_fix_attribute_errorã€‘:
# - 'linear2' ãŒ __init__ ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšã€
#   forward ã§ 'AttributeError' ãŒç™ºç”Ÿã™ã‚‹å•é¡Œã«å¯¾å‡¦ã€‚
#   (ã‚¯ãƒ©ã‚¹å±æ€§ã¨ã—ã¦ã®å®šç¾©ã¨ __init__ ã§ã®åˆæœŸåŒ–ã‚’ç¢ºå®Ÿã«è¡Œã†)
#
# ã€ä¿®æ­£ v_fix_spike_rate_zeroã€‘:
# - `run_distill_hpo.py` ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ `neuron_config` å†…ã® `bias` ã‚­ãƒ¼ã‚’
#   `bias_init` ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã€‚
# - ã€v_init ä¿®æ­£ã€‘: `v_init` (åˆæœŸè†œé›»ä½) ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«æ¸¡ã™ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
from typing import List, Tuple, Dict, Any, Optional, Union, cast 
import math
import logging 

# å¿…è¦ãªã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.core.base import BaseModel, SNNLayerNorm
# from snn_research.core.neurons.lif_neuron import LIFNeuron # å¤ã„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„)
from snn_research.core.neurons.adaptive_lif_neuron import AdaptiveLIFNeuron # ã“ã‚ŒãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹
from snn_research.core.attention import SpikingSelfAttention, SpikeDrivenSelfAttention

# (Pdb)
# import pdb

# (Pdb)
# from ..core.layers.abstract_snn_layer import LayerOutput
LayerOutput = Dict[str, torch.Tensor]


# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š (v_fix_bias_key_mapping)
logger: logging.Logger = logging.getLogger(__name__)


class SpikingTransformerV2(BaseModel):
    """
    Spike-Driven Self-Attention (SDSA) ã‚’çµ„ã¿è¾¼ã‚“ã Spiking Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚
    ViTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ï¼‰ã¨äº’æ›æ€§ãŒã‚ã‚‹ã€‚
    """
    def __init__(
        self,
        d_model: int,
        n_head: int,
        num_layers: int,
        dim_feedforward: int,
        dropout: float,
        time_steps: int,
        neuron_config: Dict[str, Any],
        sdsa_config: Dict[str, Any],
        # ViTäº’æ›ã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        img_size: int = 32,
        patch_size: int = 4,
        in_channels: int = 3,
        num_classes: int = 10,
        # (v_hpo_fix_bias_key_mapping): bias_init ã‚’ç›´æ¥å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
        bias_init: float = 0.0,
        **kwargs: Any
    ) -> None:
        
        # (v_hpo_fix_bias_key_mapping):
        # HPO (run_distill_hpo.py) ã‹ã‚‰ 'bias' ã‚­ãƒ¼ã§æ¸¡ã•ã‚Œã‚‹å ´åˆã«å¯¾å¿œ
        # kwargs ã‹ã‚‰ 'bias' ã‚’å–å¾—ã—ã€'bias_init' ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        hpo_bias = kwargs.get('bias', 0.0)
        
        # (v_fix_spike_rate_zero):
        # HPOã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ 'neuron_bias' (å°æ–‡å­—) ã«ã‚‚å¯¾å¿œ
        if hpo_bias == 0.0:
            hpo_bias = kwargs.get('neuron_bias', 0.0)

        # ãƒ­ã‚°ã§æ¸¡ã•ã‚ŒãŸãƒã‚¤ã‚¢ã‚¹ã‚’ç¢ºèª
        if hpo_bias != 0.0:
            logger.info(f"[SpikingTransformerV2] ğŸ§  Overriding bias_init with HPO value: {hpo_bias}")
            # 'bias_init' ã‚’ä¸Šæ›¸ã
            bias_init = hpo_bias
        
        # neuron_config ã« 'bias_init' ã‚’è¨­å®š
        # (v_fix_spike_rate_zero): æ—¢å­˜ã®ã‚­ãƒ¼ã‚’ä¸Šæ›¸ãã—ãªã„ã‚ˆã†ã«ä¿®æ­£
        if 'bias_init' not in neuron_config:
            neuron_config['bias_init'] = bias_init
        
        # (v_hpo_fix_bias_key_mapping): 
        # 'NEURON_BIAS' ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€'bias_init' ã‚ˆã‚Šå„ªå…ˆã™ã‚‹
        if 'NEURON_BIAS' in neuron_config:
            neuron_config['bias_init'] = neuron_config['NEURON_BIAS']
        
        # (v_fix_spike_rate_zero):
        # 'bias' ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€'bias_init' ã‚ˆã‚Šå„ªå…ˆã™ã‚‹
        if 'bias' in neuron_config:
            neuron_config['bias_init'] = neuron_config['bias']

        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã§æœ€çµ‚çš„ãªãƒã‚¤ã‚¢ã‚¹ã‚’ç¢ºèª
        logger.info(f"[SpikingTransformerV2] ğŸ§  Final bias_init for layers: {neuron_config['bias_init']}")
        
        
        super().__init__(time_steps=time_steps, **kwargs)

        self.d_model = d_model
        self.n_head = n_head
        self.time_steps = time_steps

        # --- ViT ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ ---
        self.patch_size = patch_size
        num_patches = (img_size // patch_size) ** 2
        patch_dim = in_channels * (patch_size ** 2)
        
        self.patch_embed = nn.Conv2d(
            in_channels, d_model, 
            kernel_size=patch_size, stride=patch_size
        )
        
        # ä½ç½®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°
        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, d_model))
        # -------------------------

        self.layers = nn.ModuleList([
            SDSAEncoderLayer(
                d_model=d_model,
                n_head=n_head,
                dim_feedforward=dim_feedforward,
                dropout=dropout,
                time_steps=time_steps,
                neuron_config=neuron_config,
                sdsa_config=sdsa_config,
                name=f"SDSAEncoderLayer_{i}",
                # (v_hpo_fix_bias_key_mapping): ä¿®æ­£æ¸ˆã¿ã® bias_init ã‚’æ¸¡ã™
                bias_init=neuron_config['bias_init'] 
            ) for i in range(num_layers)
        ])

        self.norm = SNNLayerNorm(d_model, time_steps=time_steps)
        
        # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ (åˆ†é¡ãƒ˜ãƒƒãƒ‰)
        self.output_projection = nn.Linear(d_model, num_classes)

        self.built = True


    def forward(
        self,
        x: torch.Tensor,
        input_images: Optional[torch.Tensor] = None,
        output_hidden_states: bool = False
    ) -> Union[torch.Tensor, LayerOutput]:
        
        if not self.built:
            raise RuntimeError(f"Layer {self.name} has not been built.")

        # ViTäº’æ›ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        # (B, C, H, W) -> (B, N, D)
        x_patched = self.patch_embed(x).flatten(2).transpose(1, 2)
        x = x_patched + self.pos_embed

        outputs_over_time: List[torch.Tensor] = []
        
        # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ (æ¨è«–/å­¦ç¿’æ™‚ã«å¿…è¦)
        for layer_module in self.layers:
             layer = cast(SDSAEncoderLayer, layer_module)
             layer.set_stateful(True)

        current_x = x 
        for t in range(self.time_steps):
            x_step = current_x 
            
            # (v_hpo_fix_oom): å…ƒã®åŸ‹ã‚è¾¼ã¿ `x` ã‚’æ¯ã‚¹ãƒ†ãƒƒãƒ—å…¥åŠ›
            if t == 0:
                 x_step = x
            else:
                 x_step = current_x # t>0 ã¯ã‚¹ãƒ‘ã‚¤ã‚¯å…¥åŠ›ã‚’æƒ³å®š

            for layer_module in self.layers:
                layer = cast(SDSAEncoderLayer, layer_module)
                x_step = layer(x_step) 

            outputs_over_time.append(x_step)
            current_x = x_step # ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã¨ã™ã‚‹

        x_final = torch.stack(outputs_over_time).mean(dim=0)

        # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ (ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã«æˆ»ã™)
        for layer_module in self.layers:
             layer = cast(SDSAEncoderLayer, layer_module)
             layer.set_stateful(False)

        x_final = self.norm(x_final)

        if output_hidden_states:
             # (Pdb)
             # output = x_final
             output: LayerOutput = {
                'last_hidden_state': x_final,
                'all_hidden_states': torch.stack(outputs_over_time)
             }
        else:
            # åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å ´åˆ (input_images ãŒ None ã§ãªã„)
            if input_images is not None:
                # (B, N, C) -> (B, C) ãƒ—ãƒ¼ãƒªãƒ³ã‚°
                pooled_output = x_final.mean(dim=1) 
                output = self.output_projection(pooled_output) # (B, NumClasses)
            else:
                # Transformerã®æ¨™æº–çš„ãªå‡ºåŠ› (B, N, C) -> (B, N, VocabSize)
                output = self.output_projection(x_final) 

        # (Pdb)
        # return output
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ãŸã‚ã«è¾æ›¸å½¢å¼ã§è¿”ã™
        
        # ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã‚’åé›†
        total_spikes = self.get_total_spikes()
        avg_spike_rate = total_spikes / (self.get_total_neurons() * self.time_steps)

        # (Pdb)
        # ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’è¨ˆç®— (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        # sparsity_loss = self.calculate_sparsity_loss()

        return {
            'output': output, # 'output' ã‚­ãƒ¼ã«ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ ¼ç´
            'activity': avg_spike_rate, # 'activity' ã‚­ãƒ¼ã«ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã‚’æ ¼ç´
            'total_spikes': total_spikes,
            # 'sparsity_loss': sparsity_loss
        }


class SDSAEncoderLayer(nn.Module):
    """
    Spike-Driven Self-Attention (SDSA) ã‚’çµ„ã¿è¾¼ã‚“ã Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€‚
    """
    def __init__(
        self,
        d_model: int,
        n_head: int,
        dim_feedforward: int,
        dropout: float,
        time_steps: int,
        neuron_config: Dict[str, Any],
        sdsa_config: Dict[str, Any],
        name: str = "SDSAEncoderLayer",
        # (v_hpo_fix_bias_key_mapping): bias_init ã‚’ç›´æ¥å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
        bias_init: float = 0.0 
    ) -> None:
        super().__init__()
        self.name = name
        self.d_model = d_model
        self.n_head = n_head
        self.time_steps = time_steps
        self._is_stateful = True # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ« (æ¨è«–/å­¦ç¿’æ™‚)

        # --- v_init ä¿®æ­£: ã“ã“ã§ v_init ã‚’å–å¾— ---
        # ãƒ­ã‚° (log5.txt) ã® V_INIT (Forced): 0.499 ã‚’åæ˜ ã•ã›ã‚‹
        v_init = neuron_config.get('v_init', 0.0)
        # ----------------------------------------
        
        # (v_hpo_fix_bias_key_mapping):
        # neuron_config ã‹ã‚‰ 'NEURON_BIAS' ã¾ãŸã¯ 'bias_init' ã‚’å–å¾—
        # run_distill_hpo.py ã‹ã‚‰ 'bias' ã‚­ãƒ¼ã§æ¸¡ã•ã‚Œã‚‹å ´åˆã«ã‚‚å¯¾å¿œ
        bias = neuron_config.get('NEURON_BIAS', 
               neuron_config.get('bias_init', 
               neuron_config.get('bias', 0.0)))
        
        # (v_fix_spike_rate_zero):
        # HPOã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ 'neuron_bias' (å°æ–‡å­—) ã«ã‚‚å¯¾å¿œ
        if bias == 0.0:
            bias = neuron_config.get('neuron_bias', 0.0)

        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã§æ¸¡ã•ã‚ŒãŸãƒã‚¤ã‚¢ã‚¹ã¨v_initã‚’ç¢ºèª
        if bias != 0.0 or v_init != 0.0:
            logger.info(f"[{self.name}] ğŸ§  Overriding neuron params: bias_init={bias}, v_init={v_init}")
        
        # v_threshold ã¯ spiking_transformer.yaml ã‹ã‚‰æ­£ã—ãæ¸¡ã•ã‚Œã¦ã„ã‚‹ (0.5)
        v_threshold_s = neuron_config.get('v_threshold', 1.0)
        decay_s = neuron_config.get('decay', 0.95)
        
        # (v_hpo_fix_bias_key_mapping): 'bias' ã‚’ 'bias_init' ã¨ã—ã¦æ¸¡ã™
        neuron_params = {
            'threshold': v_threshold_s,
            'decay': decay_s,
            'bias_init': bias, # (v_fix_spike_rate_zero) ä¿®æ­£æ¸ˆã¿ã® bias ã‚’æ¸¡ã™
            'v_init': v_init,  # --- v_init ä¿®æ­£: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦æ¸¡ã™ ---
            **neuron_config
        }

        self.self_attn = SpikeDrivenSelfAttention(
            d_model, n_head, dropout=dropout, **sdsa_config
        )

        # (v_fix_attribute_error): linear2ã‚’å…ˆã«å®šç¾©
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = SNNLayerNorm(d_model, time_steps=time_steps)
        self.norm2 = SNNLayerNorm(d_model, time_steps=time_steps)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.linear1 = nn.Linear(d_model, dim_feedforward)
        
        # --- v_init ä¿®æ­£: 3ç®‡æ‰€ã® AdaptiveLIFNeuron ã« neuron_params (v_init å«ã‚€) ã‚’æ¸¡ã™ ---
        self.neuron = AdaptiveLIFNeuron(
            features=d_model,
            **neuron_params
        )
        self.ffn_neuron1 = AdaptiveLIFNeuron(
            features=dim_feedforward,
            **neuron_params
        )
        self.ffn_neuron2 = AdaptiveLIFNeuron(
            features=d_model,
            **neuron_params
        )
        
        self.built = True

    def set_stateful(self, stateful: bool) -> None:
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çŠ¶æ…‹ç®¡ç† (ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«/ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹) ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚
        """
        self._is_stateful = stateful
        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        if not stateful:
            self.neuron.reset_state()
            self.ffn_neuron1.reset_state()
            self.ffn_neuron2.reset_state()
        
        # SNNLayerNorm ã®çŠ¶æ…‹ã‚‚åˆ‡ã‚Šæ›¿ãˆ
        if isinstance(self.norm1, SNNLayerNorm):
            self.norm1.set_stateful(stateful)
        if isinstance(self.norm2, SNNLayerNorm):
            self.norm2.set_stateful(stateful)

    def forward(self, src: torch.Tensor) -> torch.Tensor:
        """
        LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¨SDSAã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã€‚
        å…¥åŠ› `src` ã¯ (B, N, C) ã®ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        """
        if not self.built:
            raise RuntimeError(f"Layer {self.name} has not been built.")

        # 1. SDSA (Spike-Driven Self-Attention)
        # (B, N, C) -> (B, N, C)
        # SDSAã¯å†…éƒ¨ã§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ (LIF) ã‚’æŒã¡ã€ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å‡ºåŠ›ã™ã‚‹
        x_step, _ = self.self_attn(src) 
        
        # 2. Add & Norm (æ®‹å·®æ¥ç¶š 1)
        # x_step ã¯ã‚¹ãƒ‘ã‚¤ã‚¯ (0 or 1)ã€src ã¯å‰ã®å±¤ã®ã‚¹ãƒ‘ã‚¤ã‚¯ (ã¾ãŸã¯åŸ‹ã‚è¾¼ã¿)
        src = src + self.dropout1(x_step)
        
        # 3. ç™ºç« (LIF)
        # (v_hpo_fix_residual): ã“ã“ã§éã‚¹ãƒ‘ã‚¤ã‚¯ã®æ®‹å·®æ¥ç¶š `src` ã‚’
        # ã‚¹ãƒ‘ã‚¤ã‚¯ã«å¤‰æ› (ã¾ãŸã¯è†œé›»ä½ã‚’æ›´æ–°) ã™ã‚‹
        # AdaptiveLIFNeuron ã¯ (B, N, C) ã®é›»æµã‚’å—ã‘å–ã‚Šã€(B, N, C) ã®ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’è¿”ã™
        src = self.neuron(src) 
        
        # 4. Norm 1
        src = self.norm1(src)

        # 5. Feedforward (FFN)
        # (B, N, C) -> (B, N, C*4)
        x_step = self.linear1(src)
        x_step = self.dropout2(x_step)
        # 6. ç™ºç« (LIF)
        x_step = self.ffn_neuron1(x_step) 

        # (B, N, C*4) -> (B, N, C)
        x_step = self.linear2(x_step)
        x_step = self.dropout3(x_step)
        # 7. ç™ºç« (LIF)
        x_step = self.ffn_neuron2(x_step)

        # 8. Add & Norm (æ®‹å·®æ¥ç¶š 2)
        src = src + x_step
        
        # 9. Norm 2
        src = self.norm2(src)

        return src
