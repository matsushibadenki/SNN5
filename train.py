# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: train.py
# matsushibadenki/snn3/train.py
# (æ›´æ–°)
# æ–°ã—ã„çµ±åˆå­¦ç¿’å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ (å®Œå…¨ç‰ˆ)
#
# (v1-v11 ä¿®æ­£å±¥æ­´ã¯çœç•¥)
#
# ä¿®æ­£ (v12):
# - å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ (health-check) ã§ã® `AttributeError: 'dict' object has no attribute 'training'` ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã€‚
# - DIã‚³ãƒ³ãƒ†ãƒŠ (@inject) ãŒè¿”ã™ config ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ DictConfig ã§ã¯ãªãæ¨™æº–ã® dict ã§ã‚ã‚‹ãŸã‚ã€
#   @inject ã‚’å‰Šé™¤ã—ã€main() é–¢æ•°å†…ã§ container.config() ã‹ã‚‰ dict ã‚’å–å¾—å¾Œã€
#   OmegaConf.create() ã§æ˜ç¤ºçš„ã« DictConfig ã«å¤‰æ›ã—ã¦ã‹ã‚‰ train() é–¢æ•°ã«æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´ã€‚

import argparse
import os
import torch
import torch.distributed as dist
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel as DDP
# --- â–¼ ä¿®æ­£: [annotation-unchecked] noteã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã€‚ â–¼ ---
from torch.utils.data import DataLoader, random_split, DistributedSampler, Dataset, Sampler
from dependency_injector.wiring import inject, Provide
from typing import Optional, Tuple, List, Dict, Any, Callable, cast, Union, TYPE_CHECKING
from transformers import PreTrainedTokenizerBase
from omegaconf import DictConfig, OmegaConf # DictConfig, OmegaConf ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from torch.optim import Optimizer # Optimizerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from torch.optim.lr_scheduler import LRScheduler # LRSchedulerã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.cognitive_architecture.astrocyte_network import AstrocyteNetwork # AstrocyteNetworkã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# --- â–² ä¿®æ­£ â–² ---

from app.containers import TrainingContainer
from snn_research.data.datasets import get_dataset_class, DistillationDataset, DataFormat, SNNBaseDataset
from snn_research.training.trainers import BreakthroughTrainer, ParticleFilterTrainer
from snn_research.training.bio_trainer import BioRLTrainer
# --- â–¼ ä¿®æ­£ (SpQuanté‡å­åŒ–ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ) â–¼ ---
from snn_research.training.quantization import apply_qat, convert_to_quantized_model, apply_spquant_quantization
# --- â–² ä¿®æ­£ â–² ---
# --- â–¼ ä¿®æ­£ (SBCã¨æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ) â–¼ ---
from snn_research.training.pruning import apply_sbc_pruning, apply_spatio_temporal_pruning
# --- â–² ä¿®æ­£ â–² ---
from scripts.data_preparation import prepare_wikitext_data
from snn_research.core.snn_core import SNNCore
from app.utils import get_auto_device
# â—¾ï¸â—¾ï¸â—¾ï¸ è¿½åŠ : logging â—¾ï¸â—¾ï¸â—¾ï¸
import logging
logger = logging.getLogger(__name__)

# DIã‚³ãƒ³ãƒ†ãƒŠã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
container = TrainingContainer()

# --- â–¼ ä¿®æ­£: collate_fn ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ä¿®æ­£ â–¼ ---
def collate_fn(tokenizer: PreTrainedTokenizerBase, is_distillation: bool) -> Callable[[List[Any]], Any]:
# ... existing code ...
    def collate(batch: List[Any]) -> Any:
        padding_val = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
        inputs: List[torch.Tensor] = []
# ... existing code ...
    return collate
# --- â–² ä¿®æ­£ â–² ---


# --- â–¼ ä¿®æ­£ (v12): @inject ã‚’å‰Šé™¤ã—ã€config: DictConfig ã‚’æ˜ç¤ºçš„ã«å—ã‘å–ã‚‹ â–¼ ---
def train(
    args: argparse.Namespace,
    config: DictConfig, # type: ignore[has-type]
    tokenizer: PreTrainedTokenizerBase, # type: ignore[has-type]
) -> None:
# --- â–² ä¿®æ­£ (v12) â–² ---
    """å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    is_distributed = args.distributed
# ... existing code ...
    rank = int(os.environ.get("LOCAL_RANK", -1))
    device = f'cuda:{rank}' if is_distributed and torch.cuda.is_available() else get_auto_device()

    # configãŒDictConfigã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
    paradigm = config.training.paradigm

    print(f"ğŸš€ å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ  '{paradigm}' ã§å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
# ... existing code ...
    trainer: Union[BreakthroughTrainer, BioRLTrainer, ParticleFilterTrainer]

    if paradigm.startswith("bio-"):
# ... existing code ...
            raise ValueError(f"ä¸æ˜ãªç”Ÿç‰©å­¦çš„å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ : {paradigm}")

    elif paradigm in ["gradient_based", "self_supervised", "physics_informed", "probabilistic_ensemble"]:
        # --- å‹¾é…ãƒ™ãƒ¼ã‚¹å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã®å®Ÿè¡Œ ---
# ... existing code ...
        if is_distributed and paradigm != "gradient_based":
            raise NotImplementedError(f"{paradigm} learning does not support DDP yet.")

        is_distillation = paradigm == "gradient_based" and config.training.gradient_based.type == "distillation"
# ... existing code ...
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        wikitext_path = "data/wikitext-103_train.jsonl"
        data_path: str
# ... existing code ...
        if os.path.exists(wikitext_path):
            data_path = wikitext_path
        else:
# ... existing code ...
             data_path = args.data_path or "data/default_data.jsonl"
             # ä¿®æ­£(v12): config ã¯ DictConfig ãªã®ã§ .get() ã‚„ OmegaConf.select ã‚’ä½¿ç”¨
             data_path_config = OmegaConf.select(config, "data.path", default=None)
             if not isinstance(data_path_config, str):
                 data_path = args.data_path or "data/default_data.jsonl"
# ... existing code ...
                 print(f"Warning: config.data.path was not a string, using fallback: {data_path}")
             else:
                 data_path = args.data_path or data_path_config

        DatasetClass = get_dataset_class(DataFormat(config.data.format))
# ... existing code ...
        max_seq_len = OmegaConf.select(config, "model.time_steps", default=128) # Use OmegaConf.select

        if is_distillation:
# ... existing code ...
            dataset = DistillationDataset(file_path=distill_jsonl_path, data_dir=data_dir, tokenizer=tokenizer, max_seq_len=max_seq_len)
        else:
            if not os.path.exists(data_path):
# ... existing code ...
                      raise FileNotFoundError(f"Data file not found: {data_path}")
            dataset = DatasetClass(file_path=data_path, tokenizer=tokenizer, max_seq_len=max_seq_len)

        # Ensure split ratio is valid before splitting
        split_ratio = OmegaConf.select(config, "data.split_ratio", default=0.1)
# ... existing code ...
             if train_size <= 0: raise ValueError("Dataset too small to split.")


        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
# ... existing code ...
        # --- â–¼ ä¿®æ­£ â–¼ ---
        train_sampler: Optional[Sampler[int]] = DistributedSampler(train_dataset) if is_distributed else None # Sampler[int] ã«ä¿®æ­£
        # --- â–² ä¿®æ­£ â–² ---
# ... existing code ...
        val_loader = DataLoader(val_dataset, batch_size=config.training.batch_size, shuffle=False, collate_fn=collate_fn(tokenizer, is_distillation), num_workers=0)

        snn_model: nn.Module = container.snn_model(backend=args.backend)

        # --- â–¼ ä¿®æ­£: SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.2 (SpQuant) ã‚’ QAT ã‚ˆã‚Šå…ˆã«é©ç”¨ â–¼ ---
# ... existing code ...
        # PyTorchæ¨™æº–ã®QAT (SpQuantã¨ä½µç”¨ã¯é€šå¸¸ã—ãªã„ãŒã€è¨­å®šä¸Šã¯å¯èƒ½)
        elif config.training.quantization.enabled:
            logger.info("Applying PyTorch QAT preparation...")
# ... existing code ...
        # --- â–² ä¿®æ­£ â–² ---
            
        snn_model.to(device)

        if is_distributed:
# ... existing code ...
            snn_model = DDP(snn_model, device_ids=[rank], find_unused_parameters=True)

        # --- â–¼ ä¿®æ­£: astrocyte ã®å‹ã‚’ Optional[AstrocyteNetwork] ã« â–¼ ---
        astrocyte: Optional[AstrocyteNetwork] = container.astrocyte_network(snn_model=snn_model) if args.use_astrocyte else None
# ... existing code ...
        # --- â–² ä¿®æ­£ â–² ---

        trainer_provider: Callable[..., BreakthroughTrainer]
        optimizer: Optimizer # Use imported Optimizer
# ... existing code ...
        scheduler: Optional[LRScheduler] # Use imported LRScheduler

        if paradigm == "gradient_based":
            optimizer = container.optimizer(params=snn_model.parameters())
# ... existing code ...
            trainer_provider = container.distillation_trainer if is_distillation else container.standard_trainer
        elif paradigm == "self_supervised":
            optimizer = container.optimizer(params=snn_model.parameters()) # Assuming same optimizer provider
# ... existing code ...
            trainer_provider = container.self_supervised_trainer
        elif paradigm == "physics_informed":
            optimizer = container.pi_optimizer(params=snn_model.parameters())
# ... existing code ...
            trainer_provider = container.physics_informed_trainer
        else: # probabilistic_ensemble
            optimizer = container.optimizer(params=snn_model.parameters()) # Assuming same optimizer provider
# ... existing code ...
            trainer_provider = container.probabilistic_ensemble_trainer

        # --- â–¼ ä¿®æ­£: trainer_kwargs ã®å‹ã‚’æ˜ç¤ºã—ã€astrocyteã®å‹ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆ â–¼ ---
        trainer_kwargs: Dict[str, Any] = {
# ... existing code ...
            "model": snn_model,
            "optimizer": optimizer,
            "scheduler": scheduler,
# ... existing code ...
            "device": device,
            "rank": rank
            # "astrocyte_network" will be added conditionally below
# ... existing code ...
        }
        if args.use_astrocyte and astrocyte is not None and paradigm in ["gradient_based", "self_supervised", "physics_informed", "probabilistic_ensemble"]:
             trainer_kwargs["astrocyte_network"] = astrocyte # Type matches Optional[AstrocyteNetwork]
        # --- â–² ä¿®æ­£ â–² ---
# ... existing code ...


        trainer = trainer_provider(**trainer_kwargs)

        if args.load_ewc_data:
# ... existing code ...
            trainer.load_ewc_data(args.load_ewc_data)

        start_epoch = trainer.load_checkpoint(args.resume_path) if args.resume_path else 0
        for epoch in range(start_epoch, config.training.epochs):
# ... existing code ...
            if train_sampler and isinstance(train_sampler, DistributedSampler): train_sampler.set_epoch(epoch) # isinstanceã§å‹ã‚¬ãƒ¼ãƒ‰
            trainer.train_epoch(train_loader, epoch)
            if rank in [-1, 0] and (epoch % config.training.eval_interval == 0 or epoch == config.training.epochs - 1):
# ... existing code ...
                val_metrics = trainer.evaluate(val_loader, epoch)
                if epoch % config.training.log_interval == 0:
                    checkpoint_path = os.path.join(config.training.log_dir, f"checkpoint_epoch_{epoch}.pth")
                    # --- â–¼ ä¿®æ­£: config.modelã‚’è¾æ›¸ã«å¤‰æ› â–¼ ---
                    model_config_dict = OmegaConf.to_container(config.model, resolve=True) if isinstance(config.model, DictConfig) else config.model
# ... existing code ...
                    if not isinstance(model_config_dict, dict): model_config_dict = {} # Fallback
                    trainer.save_checkpoint(path=checkpoint_path, epoch=epoch, metric_value=val_metrics.get('total', float('inf')), tokenizer_name=config.data.tokenizer_name, config=model_config_dict)
                    # --- â–² ä¿®æ­£ â–² ---
# ... existing code ...

        if rank in [-1, 0] and args.task_name and config.training.gradient_based.loss.ewc_weight > 0:
            trainer._compute_ewc_fisher_matrix(train_loader, args.task_name)

        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã®å‡¦ç† (é‡å­åŒ–ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°)
# ... existing code ...
        # --- â–¼ ä¿®æ­£ (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1, 4.2, 4.3 å¯¾å¿œ): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡å­åŒ–ã®é †åºå¤‰æ›´ãƒ»æ–°æ©Ÿèƒ½è¿½åŠ  â–¼ ---
        # --- â–¼ ä¿®æ­£ (mypy [assignment]): `type: ignore` ã‚’è¿½åŠ  â–¼ ---
        if rank in [-1, 0]:
# ... existing code ...
            final_model_wrapped = trainer.model.module if is_distributed else trainer.model
            
            # SNNCoreãƒ©ãƒƒãƒ‘ãƒ¼ã‹ã‚‰å†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
# ... existing code ...
            final_model: nn.Module
            if isinstance(final_model_wrapped, SNNCore):
                final_model = final_model_wrapped.model # type: ignore[assignment]
# ... existing code ...
            else:
                # DDP ã‚„ä»–ã®ãƒ©ãƒƒãƒ‘ãƒ¼ãŒ SNNCore ã‚’ãƒ©ãƒƒãƒ—ã—ã¦ã„ãªã„å ´åˆ
                final_model = final_model_wrapped # type: ignore[assignment]
            
            if isinstance(final_model, nn.Module):
# ... existing code ...
                model_to_process = final_model # å‡¦ç†å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«
                
                # --- 1a. æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1) ---
                if OmegaConf.select(config, "training.pruning.spatio_temporal.enabled", default=False):
# ... existing code ...
                    
                    st_amount: float = OmegaConf.select(config, "training.pruning.spatio_temporal.spatial_amount", default=0.2)
                    st_kl_thresh: float = OmegaConf.select(config, "training.pruning.spatio_temporal.kl_threshold", default=0.01)
# ... existing code ...
                    
                    # (BaseModelã‹ã‚‰time_stepsã‚’å–å¾—)
                    snn_time_steps: int = cast(int, getattr(model_to_process, 'time_steps', 16))

                    st_pruned_model = apply_spatio_temporal_pruning(
# ... existing code ...
                        model_to_process,
                        dataloader=val_loader, # ã‚¹ã‚¿ãƒ–ã¨ã—ã¦æ¤œè¨¼ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æ¸¡ã™
                        time_steps=snn_time_steps,
# ... existing code ...
                        spatial_amount=st_amount,
                        kl_threshold=st_kl_thresh
                    )
# ... existing code ...
                    torch.save(st_pruned_model.state_dict(), st_pruned_path)
                    logger.info(f"âœ… Spatio-Temporal Pruned model saved to {st_pruned_path}")
                    model_to_process = st_pruned_model # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãŸã‚ã€å‡¦ç†æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°

                # --- 1b. SBC ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.3 é †åº) ---
# ... existing code ...
                if OmegaConf.select(config, "training.pruning.sbc.enabled", default=False): # 'enabled' -> 'sbc.enabled'
                    pruning_amount: float = OmegaConf.select(config, "training.pruning.sbc.amount", default=0.2)
                    logger.info("Applying SBC Pruning to the final model (post ST-pruning if enabled)...")
# ... existing code ...
                    
                    pruned_model = apply_sbc_pruning(
                        model_to_process, 
# ... existing code ...
                        amount=pruning_amount,
                        dataloader_stub=val_loader, # ã‚¹ã‚¿ãƒ–ã¨ã—ã¦æ¤œè¨¼ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æ¸¡ã™
                        loss_fn_stub=trainer.criterion # ã‚¹ã‚¿ãƒ–ã¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®æå¤±é–¢æ•°ã‚’æ¸¡ã™
# ... existing code ...
                    )
                    pruned_path = os.path.join(config.training.log_dir, 'pruned_sbc_best_model.pth')
                    torch.save(pruned_model.state_dict(), pruned_path)
# ... existing code ...
                    logger.info(f"âœ… SBC Pruned model saved to {pruned_path}")
                    model_to_process = pruned_model # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãŸã‚ã€å‡¦ç†æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°
                
                # --- 2a. SNNå›ºæœ‰é‡å­åŒ– (SpQuant) (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.2) ---
# ... existing code ...
                if OmegaConf.select(config, "training.quantization.spquant.enabled", default=False):
                    logger.info("Applying SpQuant-SNN (Membrane Quantization) to the final model (post-pruning if enabled)...")
                    # (SpQuantã¯è¨“ç·´å‰ã«è¡Œã†ã®ãŒQATã ãŒã€ã“ã“ã§ã¯è¨“ç·´å¾Œã®ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã™ã‚‹ã‚¹ã‚¿ãƒ–)
# ... existing code ...
                    spquant_model = apply_spquant_quantization(model_to_process.to('cpu'))
                    spquant_path = os.path.join(config.training.log_dir, 'quantized_spquant_best_model.pth')
                    torch.save(spquant_model.state_dict(), spquant_path)
# ... existing code ...
                    logger.info(f"âœ… SpQuant (Stub) model saved to {spquant_path}")
                
                # --- 2b. æ¨™æº–QAT (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.3 é †åº) ---
                elif config.training.quantization.enabled:
# ... existing code ...
                    logger.info("Applying PyTorch QAT conversion to the final model (post-pruning if enabled)...")
                    quantized_model = convert_to_quantized_model(model_to_process.to('cpu'))
                    quantized_path = os.path.join(config.training.log_dir, 'quantized_qat_best_model.pth')
# ... existing code ...
                    torch.save(quantized_model.state_dict(), quantized_path)
                    logger.info(f"âœ… QAT Quantized model saved to {quantized_path}")
        # --- â–² ä¿®æ­£ â–² ---
# ... existing code ...
        # --- â–² ä¿®æ­£ â–² ---
            
    else:
        raise ValueError(f"Unknown training paradigm: '{paradigm}'.")
# ... existing code ...

    print("âœ… å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")


def main() -> None:
# ... existing code ...
    parser = argparse.ArgumentParser(description="SNN çµ±åˆå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="åŸºæœ¬è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
    parser.add_argument("--model_config", type=str, help="ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«")
# ... existing code ...
    parser.add_argument("--paradigm", type=str, help="å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ä¸Šæ›¸ã (ä¾‹: gradient_based, bio-causal-sparse, bio-particle-filter)")
    parser.add_argument("--backend", type=str, default="spikingjelly", choices=["spikingjelly", "snntorch"], help="SNNã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ©ãƒª")
    args = parser.parse_args()
# ... existing code ...

    # Load base config first
    container.config.from_yaml(args.config)

    # Load model config if provided
# ... existing code ...
    if args.model_config:
         try:
             container.config.from_yaml(args.model_config)
# ... existing code ...
         except FileNotFoundError:
             print(f"Warning: Model config file not found: {args.model_config}. Using base config model settings.")
         except Exception as e:
# ... existing code ...
              print(f"Error loading model config '{args.model_config}': {e}. Using base config model settings.")


    # Explicit overrides from command line
# ... existing code ...
    if args.data_path: container.config.data.path.from_value(args.data_path)
    if args.paradigm: container.config.training.paradigm.from_value(args.paradigm)

    # Apply dotted overrides
# ... existing code ...
    if args.override_config:
        for override in args.override_config:
            try:
# ... existing code ...
                keys, value_str = override.split('=', 1)
                # Try to infer type
                try: value: Any = int(value_str)
# ... existing code ...
                        elif value_str.lower() == 'false': value = False
                        else: value = value_str # Keep as string

                # Use OmegaConf's update method for dotted keys
                OmegaConf.update(container.config(), keys, value, merge=True)
# ... existing code ...
            except Exception as e:
                print(f"Error applying override '{override}': {e}")


    if args.distributed:
# ... existing code ...
        if "MASTER_ADDR" not in os.environ: os.environ["MASTER_ADDR"] = "localhost"
        if "MASTER_PORT" not in os.environ: os.environ["MASTER_PORT"] = "29500" # Default port

        dist.init_process_group(backend="nccl")
# ... existing code ...

    # Wire the container AFTER all configurations are loaded
    container.wire(modules=[__name__])

    # --- â–¼ ä¿®æ­£ (v12): container.config() (dict) ã‚’ OmegaConf.create() ã§ãƒ©ãƒƒãƒ— â–¼ ---
    # Get injected config and tokenizer AFTER wiring
    injected_config_dict: dict = container.config() # DIã‚³ãƒ³ãƒ†ãƒŠã¯ dict ã‚’è¿”ã™
    injected_config: DictConfig = OmegaConf.create(injected_config_dict) # OmegaConfã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    
    injected_tokenizer: PreTrainedTokenizerBase = container.tokenizer() # æ­£ã—ã„å‹ã§å–å¾—
    
    train(args, config=injected_config, tokenizer=injected_tokenizer)
    # --- â–² ä¿®æ­£ (v12) â–² ---

    if args.distributed: dist.destroy_process_group()

if __name__ == "__main__":
# ... existing code ...
    main()

}
