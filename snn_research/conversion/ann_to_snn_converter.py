# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/conversion/ann_to_snn_converter.py
# (æ›´æ–°)
# Title: ANN-SNN å¤‰æ›ã‚³ãƒ³ãƒãƒ¼ã‚¿ (ECLå¯¾å¿œ)
# Description:
# - GGUF/Safetensorså½¢å¼ã®ANNãƒ¢ãƒ‡ãƒ«ã‹ã‚‰SNNã¸ã®å¤‰æ›ãƒ»è’¸ç•™ã‚’è¡Œã†ã‚³ãƒ³ãƒãƒ¼ã‚¿ã€‚
# - [æ”¹å–„ v3] å …ç‰¢ãªå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè£…ã€‚BatchNorm Folding, å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼,
#   ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ­ã‚®ãƒ³ã‚°ã‚’å°å…¥ã€‚
# - [æ”¹å–„ v4] SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1) ã«åŸºã¥ãã€ECLã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®
#   ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¨ã€`convert_cnn_weights` ã§ECLé–¢é€£ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
#   (LearnableClippingLayer, DualThresholdNeuron) ã®ä½¿ç”¨ã‚’
#   è€ƒæ…®ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã‚¹ã‚¿ãƒ–ï¼‰ã‚’è¿½åŠ ã€‚
#
#   (æ”¹å–„ v5):
#   - ECL (ã‚¨ãƒ©ãƒ¼è£œå„Ÿå­¦ç¿’) ã®ã€Œã‚¹ã‚¿ãƒ–ã€ã‚’è§£æ¶ˆã€‚
#   - use_ecl=True ã®å ´åˆã€å®Ÿéš›ã«ANNãƒ¢ãƒ‡ãƒ«ã®ReLUå±¤ã‚’
#     LearnableClippingLayer ã«ç½®ãæ›ãˆã‚‹å‡¦ç†ã‚’å®Ÿè£…ã€‚
#
#   (ä¿®æ­£ v15):
#   - mypy [syntax] (line 20) ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã€‚
#   - mypy [operator] (line 71/107) ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ã€‚
#
#   (ä¿®æ­£ v16):
#   - mypy [syntax] (line 27) ã‚’ `type: ignore[import-untyped]` ã«ä¿®æ­£ã€‚
#   - mypy [operator] (line 70, 91, 107) ã« `type: ignore` ã‚’è¿½åŠ ã€‚
#
# mypy --strict æº–æ‹ ã€‚

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from safetensors.torch import load_file
from tqdm import tqdm
from typing import Dict, Any, Optional, cast, Type, List
import logging
from transformers import AutoModelForCausalLM

from snn_research.core.snn_core import AdaptiveLIFNeuron
from snn_research.core.neurons import DualThresholdNeuron # ECLç”¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
from .conversion_utils import safe_copy_weights, calibrate_thresholds_by_percentile
from .fold_bn import fold_all_batchnorms
from .ecl_components import LearnableClippingLayer # ECLç”¨ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ãƒ¬ã‚¤ãƒ¤ãƒ¼



# GGUFã®ä¾å­˜é–¢ä¿‚ã‚’ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã™ã‚‹
try:
    # --- â–¼ ä¿®æ­£ (v16): [syntax] -> [import-untyped] ã«ä¿®æ­£ â–¼ ---
    from gguf import GGUFReader  # type: ignore[import-untyped]
    # --- â–² ä¿®æ­£ (v16) â–² ---
    GGUF_AVAILABLE = True
except ImportError:
    GGUFReader = Any  # type: ignore[misc, assignment]
    GGUF_AVAILABLE = False

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def _load_gguf(path: str) -> Dict[str, torch.Tensor]:
    """GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€PyTorchã®state_dictã‚’è¿”ã™ã€‚"""
    if not GGUF_AVAILABLE:
        raise ImportError("GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ `gguf` ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™ã€‚`pip install gguf` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    logging.info(f"GGUFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {path}")
    # --- â–¼ ä¿®æ­£ (v16): [operator] èª¤æ¤œçŸ¥ã‚’æŠ‘åˆ¶ â–¼ ---
    reader = GGUFReader(path, 'r') # type: ignore[operator]
    # --- â–² ä¿®æ­£ (v16) â–² ---
    state_dict: Dict[str, torch.Tensor] = {}
    for tensor in reader.tensors:
        state_dict[tensor.name] = torch.from_numpy(tensor.data.copy())
    logging.info(f"âœ… GGUFã‹ã‚‰ {len(state_dict)} å€‹ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
    return state_dict  # type: ignore[return-value]

def _replace_relu_with_ecl(
    module: nn.Module, 
    initial_threshold: float = 1.0,
    inplace: bool = True
) -> nn.Module:
    """
    (æ”¹å–„ v5) ãƒ¢ãƒ‡ãƒ«å†…ã® nn.ReLU ã‚’ LearnableClippingLayer ã«å†å¸°çš„ã«ç½®ãæ›ãˆã‚‹ã€‚
    SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³3.1, å¼•ç”¨[6]) ã®ãŸã‚ã®å®Ÿè£…ã€‚
    
    Note: inplace=True ã®ã¿ã‚µãƒãƒ¼ãƒˆï¼ˆå¸¸ã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç›´æ¥å¤‰æ›´ã—ã¾ã™ï¼‰
    """
    for name, child in list(module.named_children()):
        if isinstance(child, nn.ReLU):
            ecl_layer = LearnableClippingLayer(initial_threshold=initial_threshold, num_features=None)
            setattr(module, name, ecl_layer)
            logging.info(f"  - [ECL] Replaced '{name}' (ReLU) with LearnableClippingLayer.")
        else:
            _replace_relu_with_ecl(child, initial_threshold, inplace=True)
            
    return module

class AnnToSnnConverter:
    """
    æ—¢å­˜ã®ANNãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰SNNãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã€‚
    """
    def __init__(self, snn_model: nn.Module, model_config: Dict[str, Any]):
        self.snn_model = snn_model
        self.model_config = model_config
        self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        self.snn_model.to(self.device)

    def _load_ann_weights(self, ann_model_path: str, is_llm: bool = False) -> Dict[str, torch.Tensor]:
        """ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€ã€‚"""
        logging.info(f"ğŸ’¾ ANNãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {ann_model_path}")
        if ann_model_path.endswith(".safetensors"):
            return load_file(ann_model_path, device=self.device)
        elif ann_model_path.endswith(".gguf"):
            return _load_gguf(ann_model_path)
        elif is_llm:
            try:
                # --- â–¼ ä¿®æ­£ (v16): mypy [operator] èª¤æ¤œçŸ¥ã‚’æŠ‘åˆ¶ â–¼ ---
                model = AutoModelForCausalLM.from_pretrained(ann_model_path).to(self.device) # type: ignore[operator]
                # --- â–² ä¿®æ­£ (v16) â–² ---
                return model.state_dict()
            except Exception as e:
                logging.error(f"Hugging Faceãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                raise
        else:
            try:
                return torch.load(ann_model_path, map_location=self.device)
            except Exception as e:
                logging.error(f"PyTorchãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                raise

    def convert_llm_weights(
        self,
        ann_model_name_or_path: str,
        output_path: str,
        calibration_loader: Optional[Any] = None,
        # --- â–¼ è¿½åŠ  â–¼ ---
        use_ecl: bool = False # ECL (ã‚¨ãƒ©ãƒ¼è£œå„Ÿå­¦ç¿’) ã‚’è©¦ã¿ã‚‹ã‹
        # --- â–² è¿½åŠ  â–¼ ---
    ) -> None:
        """
        Hugging Faceã®LLMã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€æ­£è¦åŒ–ã¨é«˜åº¦ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã£ã¦SNNã«å¤‰æ›ã™ã‚‹ã€‚
        """
        logging.info(f"--- ğŸš€ é«˜å¿ å®Ÿåº¦LLMå¤‰æ›é–‹å§‹: {ann_model_name_or_path} ---")
        
        # 1. ANNãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        # --- â–¼ ä¿®æ­£ (v16): mypy [operator] èª¤æ¤œçŸ¥ã‚’æŠ‘åˆ¶ â–¼ ---
        ann_model = AutoModelForCausalLM.from_pretrained(ann_model_name_or_path).to(self.device) # type: ignore[operator]
        # --- â–² ä¿®æ­£ (v16) â–² ---
        ann_model.eval()

        # (ä¸­ç•¥: LLMå¤‰æ›ã®è­¦å‘Š)
        logging.warning("LLMã®å®Œå…¨ãªSNNåŒ–ã¯å®Ÿé¨“çš„ã§ã™ã€‚ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        # --- â–¼ ä¿®æ­£: ECL (ã‚¹ã‚¿ãƒ–è§£æ¶ˆ v5) â–¼ ---
        if use_ecl:
            logging.info("ECL (ã‚¨ãƒ©ãƒ¼è£œå„Ÿå­¦ç¿’) ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™ã€‚")
            # (ã‚¹ã‚¿ãƒ–: å®Ÿéš›ã«ã¯ã“ã“ã§ANNãƒ¢ãƒ‡ãƒ«ã®ReLUã‚’LearnableClippingLayerã«ç½®ãæ›ãˆã‚‹å‰å‡¦ç†ãŒå¿…è¦)
            # (æ”¹å–„ v5: å®Ÿéš›ã«ç½®ãæ›ãˆå‡¦ç†ã‚’å‘¼ã³å‡ºã™)
            logging.info("  - ANNãƒ¢ãƒ‡ãƒ«ã®ReLUã‚’LearnableClippingLayerã«ç½®ãæ›ãˆä¸­...")
            ann_model = _replace_relu_with_ecl(ann_model, initial_threshold=1.0, inplace=True)
            
            is_dual_threshold = any(isinstance(m, DualThresholdNeuron) for m in self.snn_model.modules())
            if not is_dual_threshold:
                logging.warning("ECLãŒæœ‰åŠ¹ã§ã™ãŒã€SNNãƒ¢ãƒ‡ãƒ«ã«DualThresholdNeuronãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        # --- â–² ä¿®æ­£ â–² ---

        # 2. é‡ã¿ã‚³ãƒ”ãƒ¼
        ann_state_dict = ann_model.state_dict()
        safe_copy_weights(self.snn_model, ann_state_dict)

        # 3. é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        if calibration_loader:
            logging.info("LLMã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã«åŸºã¥ãé–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™...")
            thresholds = calibrate_thresholds_by_percentile(ann_model, calibration_loader, device=self.device)
            # (ä¸­ç•¥: é–¾å€¤è¨­å®šãƒ­ã‚¸ãƒƒã‚¯)
            logging.info(f"è¨ˆç®—ã•ã‚ŒãŸé–¾å€¤: {thresholds}")
        else:
            logging.warning("ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€é–¾å€¤èª¿æ•´ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ç²¾åº¦ãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        # 4. å¤‰æ›æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        logging.info(f"âœ… LLMå¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

    def convert_cnn_weights(
        self,
        ann_model: nn.Module,
        output_path: str,
        calibration_loader: Any,
        # --- â–¼ è¿½åŠ  â–¼ ---
        use_ecl: bool = False
        # --- â–² è¿½åŠ  â–¼ ---
    ):
        """CNNãƒ¢ãƒ‡ãƒ«ã®é«˜å¿ å®Ÿåº¦å¤‰æ›ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
        logging.info("--- ğŸš€ é«˜å¿ å®Ÿåº¦CNNå¤‰æ›é–‹å§‹ ---")
        ann_model.to(self.device)
        ann_model.eval()

        # --- â–¼ ä¿®æ­£: ECL (ã‚¹ã‚¿ãƒ–è§£æ¶ˆ v5) â–¼ ---
        if use_ecl:
            logging.info("ECL (ã‚¨ãƒ©ãƒ¼è£œå„Ÿå­¦ç¿’) ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™ã€‚")
            # (ã‚¹ã‚¿ãƒ–: å®Ÿéš›ã«ã¯ã“ã“ã§ANNãƒ¢ãƒ‡ãƒ«ã®ReLUã‚’LearnableClippingLayerã«ç½®ãæ›ãˆã‚‹)
            # (æ”¹å–„ v5: å®Ÿéš›ã«ç½®ãæ›ãˆå‡¦ç†ã‚’å‘¼ã³å‡ºã™)
            logging.info("  - ANNãƒ¢ãƒ‡ãƒ«ã®ReLUã‚’LearnableClippingLayerã«ç½®ãæ›ãˆä¸­...")
            ann_model = _replace_relu_with_ecl(ann_model, initial_threshold=1.0, inplace=True)
            
            is_dual_threshold = any(isinstance(m, DualThresholdNeuron) for m in self.snn_model.modules())
            if not is_dual_threshold:
                logging.warning("ECLãŒæœ‰åŠ¹ã§ã™ãŒã€SNNãƒ¢ãƒ‡ãƒ«ã«DualThresholdNeuronãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        # --- â–² ä¿®æ­£ â–² ---

        # 1. BatchNorm Folding
        logging.info("BatchNorm Foldingã‚’å®Ÿè¡Œä¸­...")
        folded_model = fold_all_batchnorms(ann_model)
        
        # 2. é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        logging.info("ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®é–¾å€¤ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œä¸­...")
        thresholds = calibrate_thresholds_by_percentile(folded_model, calibration_loader, device=self.device)
        
        # SNNãƒ¢ãƒ‡ãƒ«ã®å¯¾å¿œã™ã‚‹LIFå±¤ã«é–¾å€¤ã‚’è¨­å®š
        # --- â–¼ ä¿®æ­£: ECLãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚‚å¯¾è±¡ã«ã™ã‚‹ â–¼ ---
        snn_neuron_layers: List[nn.Module] = [
            m for m in self.snn_model.modules() 
            if isinstance(m, (AdaptiveLIFNeuron, DualThresholdNeuron))
        ]
        
        if len(snn_neuron_layers) == len(thresholds):
            # (ä¸­ç•¥: é–¾å€¤è¨­å®šãƒ­ã‚¸ãƒƒã‚¯)
            for lif, (name, thr) in zip(snn_neuron_layers, thresholds.items()):
                if isinstance(lif, DualThresholdNeuron):
                    lif.threshold_high.data.fill_(thr)
                    lif.threshold_low.data.fill_(thr * 0.5)
                    logging.info(f"SNN ECL Neuron (T_h, T_l) ã‚’è¨­å®š: ({thr:.4f}, {thr*0.5:.4f})")
                elif isinstance(lif, AdaptiveLIFNeuron):
                    lif.base_threshold.data.fill_(thr)
                    logging.info(f"SNN LIF Neuron (base_threshold) ã‚’ {thr:.4f} ã«è¨­å®šã—ã¾ã—ãŸã€‚")
        # --- â–² ä¿®æ­£ â–² ---
        else:
            logging.warning(f"ANNã¨SNNã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³/ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®æ•°ãŒä¸€è‡´ã—ã¾ã›ã‚“ (ANN: {len(thresholds)}, SNN: {len(snn_neuron_layers)})ã€‚é–¾å€¤è¨­å®šã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            
        # 3. å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼
        logging.info("å®‰å…¨ãªé‡ã¿ã‚³ãƒ”ãƒ¼ã‚’å®Ÿè¡Œä¸­...")
        safe_copy_weights(self.snn_model, folded_model.state_dict())
        
        # 4. ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        torch.save({
            'model_state_dict': self.snn_model.state_dict(),
            'config': self.model_config
        }, output_path)
        logging.info(f"âœ… CNNå¤‰æ›ãŒå®Œäº†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ '{output_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")