# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/hippocampus.py
# (æ›´æ–°)
#
# Title: Hippocampus (æµ·é¦¬) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
#
# Description:
# - äººå·¥è„³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã€Œè¨˜æ†¶å±¤ã€ã«å±ã—ã€çŸ­æœŸè¨˜æ†¶ï¼ˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªï¼‰ã‚’æ‹…ã†ã€‚
# - æ–°ã—ã„æƒ…å ±ã‚„çµŒé¨“ã‚’ã€Œã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€ã¨ã—ã¦æ™‚ç³»åˆ—ã§çŸ­æœŸçš„ã«ä¿æŒã™ã‚‹ã€‚
# - ä¿æŒã§ãã‚‹æƒ…å ±é‡ã«ã¯é™ã‚ŠãŒã‚ã‚Šã€å¤ã„è¨˜æ†¶ã¯å¿˜å´ã•ã‚Œã‚‹ï¼ˆFIFOï¼‰ã€‚
# - å°†æ¥çš„ã«ã¯ã€é•·æœŸè¨˜æ†¶ã¸ã®è»¢é€ï¼ˆè¨˜æ†¶ã®å›ºå®šï¼‰ã‚„ã€
#   æ³¨æ„æ©Ÿæ§‹ã¨é€£æºã—ãŸæƒ…å ±ã®é‡ã¿ä»˜ã‘ãªã©ã®æ©Ÿèƒ½æ‹¡å¼µã‚’æƒ³å®šã€‚
#
# æ”¹å–„ç‚¹(v2):
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º3ã«åŸºã¥ãã€é•·æœŸè¨˜æ†¶ã¸ã®å›ºå®šåŒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚
# æ”¹å–„ç‚¹(v3):
# - ã€Œæ„è­˜çš„èªçŸ¥ã‚µã‚¤ã‚¯ãƒ«ã€å®Ÿè£…ã®ãŸã‚ã€GlobalWorkspaceã¨é€£æºã€‚
# - æ–°ã—ã„æƒ…å ±ã¨çŸ­æœŸè¨˜æ†¶ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã€ãã®é–¢é€£åº¦ã‚’é¡•è‘—æ€§ã‚¹ã‚³ã‚¢ã¨ã—ã¦Workspaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚

from typing import List, Dict, Any
from collections import deque
import torch

from .global_workspace import GlobalWorkspace

class Hippocampus:
    """
    çŸ­æœŸçš„ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‚’ç®¡ç†ã—ã€è¨˜æ†¶ã¨ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã™ã‚‹æµ·é¦¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    """
    def __init__(self, workspace: GlobalWorkspace, capacity: int = 100):
        """
        Args:
            workspace (GlobalWorkspace): æƒ…å ±ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®ä¸­å¤®ãƒãƒ–ã€‚
            capacity (int): ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªãŒä¿æŒã§ãã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®æœ€å¤§æ•°ã€‚
        """
        self.workspace = workspace
        self.capacity = capacity
        self.working_memory: deque = deque(maxlen=capacity)
        print(f"ğŸ§  æµ·é¦¬ï¼ˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸ (å®¹é‡: {capacity} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)ã€‚")

    def evaluate_relevance_and_upload(self, perception_features: torch.Tensor):
        """
        æ–°ã—ã„çŸ¥è¦šæƒ…å ±ã¨çŸ­æœŸè¨˜æ†¶ã¨ã®é–¢é€£æ€§ã‚’è©•ä¾¡ã—ã€çµæœã‚’GlobalWorkspaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚
        """
        if not self.working_memory:
            salience = 0.8  # è¨˜æ†¶ãŒãªã‘ã‚Œã°ã€æ–°ã—ã„æƒ…å ±ã¯å¸¸ã«é¡•è‘—
            relevance_info = {"type": "memory_relevance", "relevance": 0.0, "details": "No existing memories."}
        else:
            # ç°¡æ˜“çš„ãªé–¢é€£æ€§è©•ä¾¡ï¼šç›´è¿‘ã®è¨˜æ†¶ã®ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦
            recent_episode = self.retrieve_recent_episodes(1)[0]
            recent_features = recent_episode.get('content', {}).get('features')
            
            if recent_features is not None and isinstance(recent_features, torch.Tensor):
                # ãƒ™ã‚¯ãƒˆãƒ«ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—
                similarity = torch.nn.functional.cosine_similarity(
                    perception_features.flatten(), 
                    recent_features.flatten(), 
                    dim=0
                ).item()
                # é¡ä¼¼åº¦ãŒä½ã„ï¼ˆæ–°è¦æ€§ãŒé«˜ã„ï¼‰ã»ã©é¡•è‘—æ€§ãŒé«˜ã„
                salience = 1.0 - similarity
                relevance_info = {"type": "memory_relevance", "relevance": similarity}
            else:
                salience = 0.7 # æ¯”è¼ƒå¯¾è±¡ãŒãªã„å ´åˆ
                relevance_info = {"type": "memory_relevance", "relevance": 0.0, "details": "Previous memory has no features."}

        self.workspace.upload_to_workspace(
            source="hippocampus",
            data=relevance_info,
            salience=salience
        )

    def store_episode(self, episode: Dict[str, Any]):
        """
        æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼ˆçµŒé¨“ã‚„è¦³æ¸¬ï¼‰ã‚’ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã™ã‚‹ã€‚
        """
        self.working_memory.append(episode)
        print(f"ğŸ“ æµ·é¦¬: æ–°ã—ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¨˜æ†¶ã—ã¾ã—ãŸã€‚ (ç¾åœ¨ã®è¨˜æ†¶æ•°: {len(self.working_memory)})")

    def retrieve_recent_episodes(self, num_episodes: int = 5) -> List[Dict[str, Any]]:
        """
        ç›´è¿‘ã®ã„ãã¤ã‹ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªã‹ã‚‰æ¤œç´¢ã—ã¦è¿”ã™ã€‚
        """
        if num_episodes <= 0:
            return []
        num_to_retrieve = min(num_episodes, len(self.working_memory))
        return [self.working_memory[-i] for i in range(1, num_to_retrieve + 1)]
    
    def get_and_clear_episodes_for_consolidation(self) -> List[Dict[str, Any]]:
        """
        é•·æœŸè¨˜æ†¶ã¸ã®å›ºå®šåŒ–ã®ãŸã‚ã«ã€ç¾åœ¨ã®å…¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è¿”ã—ã€ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã™ã‚‹ã€‚
        """
        episodes_to_consolidate = list(self.working_memory)
        self.clear_memory()
        print(f"ğŸ“¤ æµ·é¦¬: é•·æœŸè¨˜æ†¶ã¸ã®å›ºå®šåŒ–ã®ãŸã‚ã€{len(episodes_to_consolidate)}ä»¶ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’è»¢é€ã—ã¾ã—ãŸã€‚")
        return episodes_to_consolidate

    def clear_memory(self):
        """
        ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªã®å†…å®¹ã‚’ã™ã¹ã¦æ¶ˆå»ã™ã‚‹ã€‚
        """
        self.working_memory.clear()
        print("ğŸ—‘ï¸ æµ·é¦¬: ãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚")