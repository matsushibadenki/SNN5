# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: run_rl_agent.py
# Title: å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: ç”Ÿç‰©å­¦çš„å­¦ç¿’å‰‡ã«åŸºã¥ãSNNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’èµ·å‹•ã—ã€
#              å¼·åŒ–å­¦ç¿’ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2æ¤œè¨¼ã®ãŸã‚ã€GridWorldEnvã«å¯¾å¿œã€‚
# - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# æ”¹å–„ç‚¹ (v2):
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2å®Œäº†ã®ãŸã‚ã€å­¦ç¿’çµæœã‚’å¯è¦–åŒ–ãƒ»ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - å­¦ç¿’çµ‚äº†å¾Œã«å ±é…¬ã®æ¨ç§»ã‚’ã‚°ãƒ©ãƒ•ã¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã€‚
# - è¨“ç·´æ¸ˆã¿ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
# æ”¹å–„ç‚¹ (v3): DIã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ã€ä¸€è²«æ€§ã‚’å‘ä¸Šã€‚

import torch
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent))

from app.containers import TrainingContainer

def plot_rewards(rewards: list, save_path: Path):
    """å ±é…¬ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ä¿å­˜ã™ã‚‹ã€‚"""
    plt.figure(figsize=(12, 6))
    plt.plot(rewards, label='Reward per Episode')
    # ç§»å‹•å¹³å‡ã‚’è¨ˆç®—ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆ
    if len(rewards) >= 50:
        moving_avg = [sum(rewards[i-50:i]) / 50 for i in range(50, len(rewards))]
        plt.plot(range(50, len(rewards)), moving_avg, label='Moving Average (50 episodes)', color='red')
    plt.title('Reinforcement Learning Curve')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    print(f"ğŸ“Š å­¦ç¿’æ›²ç·šã‚°ãƒ©ãƒ•ã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def main():
    parser = argparse.ArgumentParser(description="Biologically Plausible Reinforcement Learning Framework")
    parser.add_argument("--episodes", type=int, default=1000, help="Number of learning episodes.")
    parser.add_argument("--output_dir", type=str, default="runs/rl_results", help="Directory to save results.")
    args = parser.parse_args()
    
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # --- DIã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’æ§‹ç¯‰ ---
    container = TrainingContainer()
    container.config.from_yaml("configs/base_config.yaml")
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ã‚³ãƒ³ãƒ•ã‚£ã‚°ã«åæ˜ 
    container.config.training.epochs.from_value(args.episodes)

    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å®Œæˆå“ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å–å¾—
    trainer = container.bio_rl_trainer()
    
    print("\n" + "="*20 + "ğŸ¤– ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’é–‹å§‹ (Grid World) ğŸ¤–" + "="*20)
    print(f"Device: {trainer.agent.device}")
    
    # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ ---
    results = trainer.train(num_episodes=args.episodes)
    
    print("\n" + "="*20 + "âœ… å­¦ç¿’å®Œäº†" + "="*20)
    print(f"æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {results.get('final_average_reward', 0.0):.4f}")

    # --- çµæœã®ä¿å­˜ ---
    # å­¦ç¿’æ›²ç·šã¯BioRLTrainerå†…ã§ãƒ—ãƒ­ãƒƒãƒˆãƒ»ä¿å­˜ã•ã‚Œã‚‹ã¨ä»®å®š
    # ã“ã“ã§ã¯ã€æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã®ã¿è¡Œã†
    model_save_path = output_path / "trained_rl_agent.pth"
    torch.save(trainer.agent.model.state_dict(), model_save_path)
    print(f"ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()