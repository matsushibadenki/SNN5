# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: run_rl_agent.py
# Title: å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: ç”Ÿç‰©å­¦çš„å­¦ç¿’å‰‡ã«åŸºã¥ãSNNã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’èµ·å‹•ã—ã€
#              å¼·åŒ–å­¦ç¿’ã®ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
# æ”¹å–„ç‚¹:
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2æ¤œè¨¼ã®ãŸã‚ã€GridWorldEnvã«å¯¾å¿œã€‚
# - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã—ã€è¤‡æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# æ”¹å–„ç‚¹ (v2):
# - ROADMAPãƒ•ã‚§ãƒ¼ã‚º2å®Œäº†ã®ãŸã‚ã€å­¦ç¿’çµæœã‚’å¯è¦–åŒ–ãƒ»ä¿å­˜ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã€‚
# - å­¦ç¿’çµ‚äº†å¾Œã«å ±é…¬ã®æ¨ç§»ã‚’ã‚°ãƒ©ãƒ•ã¨ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã€ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã€‚
# - è¨“ç·´æ¸ˆã¿ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã€‚
# æ”¹å–„ç‚¹ (v3): DIã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã—ã€ä¸€è²«æ€§ã‚’å‘ä¸Šã€‚
#
# ä¿®æ­£ (v4):
# - å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ (health-check) ã§ã® `IndentationError` ã‚’è§£æ¶ˆã€‚
# - `train.py` ã¨åŒæ§˜ã«ã€DIã‚³ãƒ³ãƒ†ãƒŠ (dependency-injector) ãŒ `dict` ã‚’
#   è¿”ã™ãŸã‚ã€`OmegaConf.create()` ã§ `DictConfig` ã«å¤‰æ›ã—ã¦ã‹ã‚‰ä½¿ç”¨ã™ã‚‹ã€‚

import torch
import argparse
from tqdm import tqdm
import matplotlib.pyplot as plt
from pathlib import Path
import sys
# --- â–¼ ä¿®æ­£ (v4): OmegaConf ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from omegaconf import OmegaConf, DictConfig
from typing import Dict, Any
# --- â–² ä¿®æ­£ (v4) â–² ---


# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent))

from app.containers import TrainingContainer

def plot_rewards(rewards: list, save_path: Path):
# ... existing code ...
    """å ±é…¬ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦ä¿å­˜ã™ã‚‹ã€‚"""
    plt.figure(figsize=(12, 6))
    plt.plot(rewards, label='Reward per Episode')
# ... existing code ...
    plt.legend()
    plt.grid(True)
    plt.savefig(save_path)
    print(f"ğŸ“Š å­¦ç¿’æ›²ç·šã‚°ãƒ©ãƒ•ã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

def main():
    parser = argparse.ArgumentParser(description="Biologically Plausible Reinforcement Learning Framework")
    parser.add_argument("--episodes", type=int, default=1000, help="Number of learning episodes.")
    parser.add_argument("--output_dir", type=str, default="runs/rl_results", help="Directory to save results.")
    args = parser.parse_args()
    
    output_path = Path(args.output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # --- DIã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ç”¨ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’æ§‹ç¯‰ ---
    container = TrainingContainer()
    
    # --- â–¼ ä¿®æ­£ (v4): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ­ãƒ¼ãƒ‰æ–¹æ³•ã‚’å …ç‰¢åŒ– â–¼ ---
    try:
        # OmegaConfã§è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€ãƒãƒ¼ã‚¸
        base_cfg = OmegaConf.load("configs/base_config.yaml")
        # OmegaConfã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ dict ã«å¤‰æ›ã—ã¦ã‚³ãƒ³ãƒ†ãƒŠã«è¨­å®š
        config_dict = OmegaConf.to_container(base_cfg, resolve=True)
        if isinstance(config_dict, dict):
            container.config.from_dict(config_dict)
        else:
            raise TypeError("Loaded base_config is not a dictionary.")
            
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ã‚³ãƒ³ãƒ•ã‚£ã‚°ã«åæ˜  (DIã‚³ãƒ³ãƒ†ãƒŠã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’ä¸Šæ›¸ã)
        container.config.training.epochs.from_value(args.episodes)
        
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        sys.exit(1)
    # --- â–² ä¿®æ­£ (v4) â–² ---

    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å®Œæˆå“ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’å–å¾—
    trainer = container.bio_rl_trainer()
    
    print("\n" + "="*20 + "ğŸ¤– ç”Ÿç‰©å­¦çš„å¼·åŒ–å­¦ç¿’é–‹å§‹ (Grid World) ğŸ¤–" + "="*20)
    print(f"Device: {trainer.agent.device}")
    
    # --- å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã®å®Ÿè¡Œ ---
    results = trainer.train(num_episodes=args.episodes)
    
    print("\n" + "="*20 + "âœ… å­¦ç¿’å®Œäº†" + "="*20)
    print(f"æœ€çµ‚çš„ãªå¹³å‡å ±é…¬: {results.get('final_average_reward', 0.0):.4f}")

    # --- çµæœã®ä¿å­˜ ---
    # å­¦ç¿’æ›²ç·šã¯BioRLTrainerå†…ã§ãƒ—ãƒ­ãƒƒãƒˆãƒ»ä¿å­˜ã•ã‚Œã‚‹ã¨ä»®å®š
    # (æ³¨: `run_rl_agent.py` ã® `plot_rewards` ã¯ `BioRLTrainer` ã‹ã‚‰ã¯å‘¼ã°ã‚Œã¦ã„ãªã„)
    # å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ã®ãŸã‚ã€ã“ã“ã§ `plot_rewards` ã‚’å‘¼ã³å‡ºã™ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ 
    if results.get('rewards_history'):
        plot_save_path = output_path / "learning_curve.png"
        plot_rewards(results['rewards_history'], plot_save_path)
    
    # æœ€çµ‚çš„ãªãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    model_save_path = output_path / "trained_rl_agent.pth"
    torch.save(trainer.agent.model.state_dict(), model_save_path)
    print(f"ğŸ’¾ è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")


if __name__ == "__main__":
    main()
