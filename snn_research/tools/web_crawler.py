# snn_research/tools/web_crawler.py
# Title: Web Crawler Tool
# Description: æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰Webãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚
# æ”¹å–„ç‚¹: URLãŒMarkdownãƒªãƒ³ã‚¯å½¢å¼ã§æ¸¡ã•ã‚ŒãŸå ´åˆã‚„ã€æœ«å°¾ã«ä¸è¦ãªæ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã§ã‚‚å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå‡¦ç†ã‚’å¼·åŒ–ã€‚

import requests
from bs4 import BeautifulSoup, Tag
from urllib.parse import urljoin, urlparse
from typing import Set, List, Optional
import time
import os
import json
import re

class WebCrawler:
    """
    Webã‚’å·¡å›ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åé›†ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã€‚
    """
    def __init__(self, output_dir: str = "workspace/web_data"):
        self.visited_urls: Set[str] = set()
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)

    def _sanitize_url(self, url: str) -> str:
        """Markdownãƒªãƒ³ã‚¯å½¢å¼ã‚„ä¸è¦ãªæ–‡å­—ãŒå«ã¾ã‚Œã‚‹URLã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã™ã‚‹ã€‚"""
        # [https://example.com](...) or (https://example.com) ã®ã‚ˆã†ãªå½¢å¼ã‹ã‚‰URLã‚’æŠ½å‡º
        match = re.search(r'https?://[^\]\)]+', url)
        if match:
            # æŠ½å‡ºã—ãŸURLã®æœ«å°¾ã«ã‚ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ä¸è¦ãªæ–‡å­—ï¼ˆãƒãƒƒã‚¯ã‚¹ãƒ©ãƒƒã‚·ãƒ¥ãªã©ï¼‰ã‚’å‰Šé™¤
            return match.group(0).rstrip('\\/')
        return url.rstrip('\\/')

    def _is_valid_url(self, url: str, base_domain: str) -> bool:
        """å·¡å›å¯¾è±¡ã¨ã—ã¦é©åˆ‡ãªURLã‹åˆ¤æ–­ã™ã‚‹ã€‚"""
        parsed_url = urlparse(url)
        return (
            parsed_url.scheme in ["http", "https"] and
            parsed_url.netloc == base_domain and
            url not in self.visited_urls
        )

    def _extract_text_from_html(self, html_content: str) -> str:
        """BeautifulSoupã‚’ä½¿ã£ã¦HTMLã‹ã‚‰ä¸»è¦ãªãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        soup = BeautifulSoup(html_content, 'html.parser')
        # ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã®ä¸è¦ãªéƒ¨åˆ†ã‚’å‰Šé™¤
        for element in soup(['script', 'style', 'header', 'footer', 'nav', 'aside']):
            element.decompose()
        
        text = soup.get_text(separator='\n', strip=True)
        return text

    def crawl(self, start_url: str, max_pages: int = 5) -> str:
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’é–‹å§‹ã—ã€åé›†ã—ãŸãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚

        Returns:
            str: ä¿å­˜ã•ã‚ŒãŸjsonlãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚
        """
        sanitized_start_url = self._sanitize_url(start_url)
        urls_to_visit: List[str] = [sanitized_start_url]
        base_domain = urlparse(sanitized_start_url).netloc
        
        output_filename = f"crawled_data_{int(time.time())}.jsonl"
        output_filepath = os.path.join(self.output_dir, output_filename)

        page_count = 0
        with open(output_filepath, 'w', encoding='utf-8') as f:
            while urls_to_visit and page_count < max_pages:
                current_url = urls_to_visit.pop(0)
                if not self._is_valid_url(current_url, base_domain):
                    continue

                try:
                    print(f"ğŸ“„ ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­: {current_url}")
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®šã—ã¦ã€ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ãƒªã‚¹ã‚¯ã‚’ä½æ¸›
                    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'}
                    response = requests.get(current_url, timeout=10, headers=headers)
                    response.raise_for_status()
                    self.visited_urls.add(current_url)
                    page_count += 1

                    text_content = self._extract_text_from_html(response.text)
                    
                    if text_content:
                        record = {"text": text_content, "source_url": current_url}
                        f.write(json.dumps(record, ensure_ascii=False) + "\n")

                    soup = BeautifulSoup(response.text, 'html.parser')
                    for link in soup.find_all('a', href=True):
                        if isinstance(link, Tag):
                            href = link.get('href')
                            if isinstance(href, str):
                                absolute_link = urljoin(current_url, href)
                                if self._is_valid_url(absolute_link, base_domain):
                                    urls_to_visit.append(absolute_link)
                    
                    time.sleep(1)

                except requests.RequestException as e:
                    print(f"âŒ ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {current_url} ({e})")

        print(f"âœ… ã‚¯ãƒ­ãƒ¼ãƒ«å®Œäº†ã€‚{page_count}ãƒšãƒ¼ã‚¸ã®ãƒ‡ãƒ¼ã‚¿ã‚’ '{output_filepath}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
        return output_filepath