# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/pruning.py
# (æ”¹ä¿®: SBC ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè£…)
# Title: æ§‹é€ çš„ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (SBC - Spiking Brain Compression)
# Description:
# doc/SNNé–‹ç™ºï¼šåŸºæœ¬è¨­è¨ˆæ€æƒ³.md (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.4, å¼•ç”¨[15]) ã«åŸºã¥ãã€
# é«˜ã‚³ã‚¹ãƒˆãªåå¾©ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ãªãã€ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆæå¤±ã®äºŒæ¬¡å¾®åˆ†ï¼‰ã‚’åˆ©ç”¨ã—ãŸ
# ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSBCï¼‰ã‚’å®Ÿè£…ã™ã‚‹ã€‚
#
# å®Ÿè£…æ¦‚è¦ (ã‚¹ã‚¿ãƒ–):
# 1. (apply_sbc_pruning): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®å±¤ã‚’ç‰¹å®šã™ã‚‹ã€‚
# 2. (_compute_hessian_diag): æå¤±ã®äºŒæ¬¡å¾®åˆ†ï¼ˆãƒ˜ãƒƒã‚»è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚(ãƒ€ãƒŸãƒ¼å®Ÿè£…)
# 3. (_compute_saliency): ãƒ˜ãƒƒã‚»è¡Œåˆ—ã«åŸºã¥ãã€å„é‡ã¿ã®é‡è¦åº¦ï¼ˆSaliencyï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
# 4. (prune_and_update_weights): é‡è¦åº¦ãŒä½ã„é‡ã¿ã‚’å‰Šé™¤ï¼ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ã—ã€
#    æ®‹ã£ãŸé‡ã¿ã‚’è£œæ­£ï¼ˆUpdateï¼‰ã—ã¦æå¤±ã®å¢—åŠ ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã€‚
#
# è¿½åŠ  (v2):
# - SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
#   æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã®ã‚¹ã‚¿ãƒ–ã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
from typing import List, Tuple, Dict, Any, cast # å¿…è¦ãªå‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import logging # ãƒ­ã‚®ãƒ³ã‚°ã‚’è¿½åŠ 

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def _compute_hessian_diag(model: nn.Module, loss_fn: nn.Module, dataloader: Any) -> Dict[str, torch.Tensor]:
    """
    ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®å¯¾è§’æˆåˆ†ã‚’è¨ˆç®—ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚
    å®Ÿéš›ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ã„ã€
    ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’2å›è¡Œã†ãªã©ã®æ‰‹æ³•ï¼ˆä¾‹: L-BFGSï¼‰ãŒå¿…è¦ã€‚
    """
    logger.info("Computing Hessian matrix diagonal (Stub)...")
    hessian_diag: Dict[str, torch.Tensor] = {}
    
    # --- ãƒ€ãƒŸãƒ¼å®Ÿè£… ---
    # å®Ÿéš›ã«ã¯ã“ã“ã§ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’æ•°ãƒãƒƒãƒå›ã—ã€
    # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® (d^2 L / d w^2) ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    for name, param in model.named_parameters():
        if "weight" in name and param.requires_grad and param.dim() > 1:
            # å¯¾è§’æˆåˆ†ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨åŒã˜å½¢çŠ¶ã‚’æŒã¤
            # ãƒ€ãƒŸãƒ¼ã¨ã—ã¦ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¤§ãã•ã«å¿œã˜ãŸãƒ©ãƒ³ãƒ€ãƒ ãªæ­£ã®å€¤ã‚’è¨­å®š
            hessian_diag[name] = torch.rand_like(param) * 0.1 + (param.data.abs() * 0.5) + 1e-6
    # --- ãƒ€ãƒŸãƒ¼å®Ÿè£…çµ‚äº† ---
    
    logger.info(f"Hessian diagonal computed (dummy) for {len(hessian_diag)} layers.")
    return hessian_diag

def _compute_saliency(param: torch.Tensor, hessian_diag: torch.Tensor) -> torch.Tensor:
    """
    SBC (Optimal Brain Compression) ã«åŸºã¥ãé‡ã¿ã®é‡è¦åº¦ï¼ˆSaliencyï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
    Saliency = (1/2) * (w^2) * (H_ii)
    """
    return 0.5 * (param.data ** 2) * hessian_diag

@torch.no_grad()
def _prune_and_update_weights(
    module: nn.Module,
    param_name: str,
    saliency: torch.Tensor,
    amount: float
) -> Tuple[int, int]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€é‡ã¿ã‚’è£œæ­£ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚
    """
    param: torch.Tensor = getattr(module, param_name)
    
    # 1. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã‚’æ±ºå®š
    num_to_prune = int(param.numel() * amount)
    if num_to_prune == 0:
        return 0, param.numel()
        
    threshold = torch.kthvalue(saliency.view(-1), k=num_to_prune).values
    mask = saliency > threshold
    
    # 2. é‡ã¿è£œæ­£ (SBCã®æ ¸å¿ƒéƒ¨ - ãƒ€ãƒŸãƒ¼å®Ÿè£…)
    # å®Ÿéš›ã«ã¯ã€SBCã¯å‰Šé™¤ã™ã‚‹é‡ã¿ (w_j) ãŒæ®‹ã‚Šã®é‡ã¿ (w_i) ã«
    # ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ (H_ij) ã‚’è€ƒæ…®ã—ã¦ w_i ã‚’æ›´æ–°ã™ã‚‹ã€‚
    # delta_w_i = - (H_ii)^-1 * H_ij * w_j
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€æ®‹ã£ãŸé‡ã¿ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
    
    # ç°¡æ˜“è£œæ­£: å‰Šé™¤ã•ã‚Œã‚‹é‡ã¿ã®ç·å’Œã‚’æ®‹ã‚Šã®é‡ã¿ã§å‰²ã£ãŸå€¤ã‚’ã€
    # å­¦ç¿’ç‡ã§ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦åŠ ç®—ã™ã‚‹ï¼ˆç”Ÿç‰©å­¦çš„å¯å¡‘æ€§ã«è¿‘ã„ãƒ€ãƒŸãƒ¼è£œæ­£ï¼‰
    # update_factor = (param.data * ~mask).sum() / (param.data * mask).sum().clamp(min=1e-6)
    # param.data[mask] += param.data[mask] * update_factor * 0.01 # 1%è£œæ­£
    
    # 3. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒã‚¹ã‚¯ã‚’é©ç”¨)
    param.data *= mask.float()
    
    original_count = param.numel()
    pruned_count = original_count - mask.sum().item()
    return int(pruned_count), original_count

def apply_sbc_pruning(
    model: nn.Module,
    amount: float,
    dataloader_stub: Any, # ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)
    loss_fn_stub: nn.Module # æå¤±é–¢æ•° (ã‚¹ã‚¿ãƒ–)
) -> nn.Module:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã€SBC (Spiking Brain Compression) ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã€‚

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚
        amount (float): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã®å‰²åˆ (0.0ã‹ã‚‰1.0ã®é–“)ã€‚
        dataloader_stub (Any): ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ç¾åœ¨ã¯æœªä½¿ç”¨)ã€‚
        loss_fn_stub (nn.Module): æå¤±é–¢æ•° (ç¾åœ¨ã¯æœªä½¿ç”¨)ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    if not (0.0 < amount < 1.0):
        logger.warning(f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é‡ãŒç„¡åŠ¹ã§ã™ ({amount})ã€‚0.0ã‹ã‚‰1.0ã®é–“ã®å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return model

    logger.info(f"--- ğŸ§  Spiking Brain Compression (SBC) é–‹å§‹ (Amount: {amount:.1%}) ---")

    # 1. ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆå¯¾è§’æˆåˆ†ï¼‰ã‚’è¨ˆç®— (ã‚¹ã‚¿ãƒ–)
    hessian_diagonals = _compute_hessian_diag(model, loss_fn_stub, dataloader_stub)
    
    total_pruned = 0
    total_params = 0

    # 2. å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡ã¿è£œæ­£ã‚’å®Ÿè¡Œ
    # (ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯ãªãã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«æŒ‡å®šã•ã‚ŒãŸå‰²åˆã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°)
    target_modules: List[Tuple[nn.Module, str]] = []
    for module in model.modules():
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
            target_modules.append((module, 'weight'))

    if not target_modules:
        logger.warning("ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return model

    logger.info(f"SBCå¯¾è±¡ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {len(target_modules)}")
    
    for module, param_name in target_modules:
        full_param_name = [name for name, mod in model.named_modules() if mod is module][0] + f".{param_name}"
        
        if full_param_name in hessian_diagonals:
            param: torch.Tensor = getattr(module, param_name)
            hessian_diag = hessian_diagonals[full_param_name]
            
            # 3. é‡è¦åº¦ã‚’è¨ˆç®—
            saliency = _compute_saliency(param, hessian_diag)
            
            # 4. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡ã¿è£œæ­£ (ã‚¹ã‚¿ãƒ–)
            pruned, total = _prune_and_update_weights(module, param_name, saliency, amount)
            total_pruned += pruned
            total_params += total
            logger.info(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': {pruned}/{total} ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (è£œæ­£å®Ÿè¡Œæ¸ˆã‚¹ã‚¿ãƒ–)ã€‚")
        else:
            logger.warning(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': ãƒ˜ãƒƒã‚»è¡Œåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"--- âœ… SBC å®Œäº† ---")
        logger.info(f"  - åˆè¨ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ç‡: {actual_sparsity:.2%} ({total_pruned} / {total_params})")
    else:
        logger.error("--- âŒ SBC å¤±æ•—: å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ0ã§ã—ãŸ ---")

    return model

# --- â–¼â–¼â–¼ SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ãè¿½åŠ å®Ÿè£… â–¼â–¼â–¼ ---

@torch.no_grad()
def _calculate_temporal_redundancy(
    model: nn.Module, 
    dataloader: Any, 
    time_steps: int
) -> Dict[str, int]:
    """
    (ã‚¹ã‚¿ãƒ–) SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€‚
    KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ç›£è¦–ã—ã€æƒ…å ±ãŒé£½å’Œã—ãŸå†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹ã€‚
    
    Returns:
        Dict[str, int]: ãƒ¬ã‚¤ãƒ¤ãƒ¼åã¨ã€ãã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‰Šæ¸›å¯èƒ½ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¾æ›¸ (ã‚¹ã‚¿ãƒ–)ã€‚
    """
    logger.info("Calculating temporal redundancy (Stub)...")
    # (ãƒ€ãƒŸãƒ¼å®Ÿè£…: å®Ÿéš›ã«ã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§å®Ÿè¡Œã—ã€
    #  å„å±¤ã®ã‚¹ãƒ‘ã‚¤ã‚¯å‡ºåŠ›ã®æ™‚ç³»åˆ—åˆ†å¸ƒ(KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹)ã‚’è¨ˆç®—ã™ã‚‹)
    
    # ãƒ€ãƒŸãƒ¼ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®20%ã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’å†—é•·ã¨ã¿ãªã™
    redundant_steps = int(time_steps * 0.2)
    
    redundancy_report: Dict[str, int] = {}
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)): # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡å±¤
            redundancy_report[name] = redundant_steps

    logger.info(f"Temporal redundancy calculated (dummy). Proposing {redundant_steps} steps reduction.")
    return redundancy_report

@torch.no_grad()
def apply_spatio_temporal_pruning(
    model: nn.Module,
    dataloader: Any,
    time_steps: int,
    spatial_amount: float # ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰²åˆ
) -> nn.Module:
    """
    SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
    æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã‚’é©ç”¨ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®SNNãƒ¢ãƒ‡ãƒ«ã€‚
        dataloader (Any): KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)ã€‚
        time_steps (int): å…ƒã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€‚
        spatial_amount (float): ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé‡ã¿å‰Šé™¤ï¼‰ã®å‰²åˆã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    logger.info(f"--- âš¡ï¸ Spatio-Temporal Pruning é–‹å§‹ (Spatial Amount: {spatial_amount:.1%}) ---")
    
    # --- 1. æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Temporal Pruning) ---
    # å¼•ç”¨[19]ã«åŸºã¥ãã€å†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹
    redundancy_report = _calculate_temporal_redundancy(model, dataloader, time_steps)
    
    # (ã‚¹ã‚¿ãƒ–: å®Ÿéš›ã«ã¯ã€ç‰¹å®šã•ã‚ŒãŸã‚¹ãƒ†ãƒƒãƒ—æ•°ã ã‘ã€ãƒ¢ãƒ‡ãƒ«ã® time_steps ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’
    #  å¤‰æ›´ã—ã€ä»¥é™ã®æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã€‚ã“ã“ã§ã¯ãƒ­ã‚°å‡ºåŠ›ã®ã¿)
    avg_redundant_steps = int(sum(redundancy_report.values()) / len(redundancy_report)) if redundancy_report else 0
    new_time_steps = time_steps - avg_redundant_steps
    
    logger.info(f"  [Temporal Pruning (Stub)]: æ¨å®šå‰Šæ¸›å¯èƒ½ã‚¹ãƒ†ãƒƒãƒ—æ•°: {avg_redundant_steps}. (T={time_steps} -> T={new_time_steps})")
    
    # --- 2. ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatial Pruning) ---
    # å¼•ç”¨[19]ã®LAMPSãƒ™ãƒ¼ã‚¹ã€ã¾ãŸã¯å˜ç´”ãªMagnitudeãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
    logger.info("  [Spatial Pruning (Magnitude Stub)]: é‡ã¿ã®ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    
    total_pruned = 0
    total_params = 0
    
    for module in model.modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            param: torch.Tensor = module.weight
            
            num_to_prune = int(param.numel() * spatial_amount)
            if num_to_prune == 0:
                continue
            
            # å˜ç´”ãª Magnitude Pruning (ã‚¹ã‚¿ãƒ–)
            threshold = torch.kthvalue(param.data.abs().view(-1), k=num_to_prune).values
            mask = param.data.abs() > threshold
            param.data *= mask.float()
            
            pruned_count = param.numel() - mask.sum().item()
            total_pruned += int(pruned_count)
            total_params += param.numel()

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"  [Spatial Pruning]: {actual_sparsity:.2%} ({total_pruned} / {total_params}) ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
    
    logger.info("--- âœ… Spatio-Temporal Pruning å®Œäº† (Stub) ---")
    return model
