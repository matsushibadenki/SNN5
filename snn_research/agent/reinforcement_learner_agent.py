# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/agent/reinforcement_learner_agent.py
# (æ›´æ–°)
# æ”¹å–„ç‚¹:
# - `learn`ãƒ¡ã‚½ãƒƒãƒ‰ã«`causal_credit`å¼•æ•°ã‚’è¿½åŠ ã€‚
# - ã“ã®å¼•æ•°ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã€é€šå¸¸ã®å ±é…¬ã‚ˆã‚Šã‚‚å„ªå…ˆã—ã€
#   ã‚ˆã‚Šå¤§ããªå­¦ç¿’ç‡ã§é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…ã€‚

import torch
from typing import Dict, Any, List

from snn_research.bio_models.simple_network import BioSNN
from snn_research.learning_rules.reward_modulated_stdp import RewardModulatedSTDP
from snn_research.communication import SpikeEncoderDecoder

class ReinforcementLearnerAgent:
    """
    BioSNNã¨å ±é…¬å¤‰èª¿å‹STDPã‚’ç”¨ã„ã€ãƒˆãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã®å› æœã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã§å­¦ç¿’ãŒå¤‰èª¿ã•ã‚Œã‚‹å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    """
    def __init__(self, input_size: int, output_size: int, device: str):
        self.device = device
        
        learning_rule = RewardModulatedSTDP(
            learning_rate=0.005, a_plus=1.0, a_minus=1.0,
            tau_trace=20.0, tau_eligibility=50.0
        )
        
        hidden_size = (input_size + output_size) * 2
        layer_sizes = [input_size, hidden_size, output_size]
        
        self.model = BioSNN(
            layer_sizes=layer_sizes,
            neuron_params={'tau_mem': 10.0, 'v_threshold': 1.0, 'v_reset': 0.0, 'v_rest': 0.0},
            learning_rule=learning_rule
        ).to(device)

        self.encoder = SpikeEncoderDecoder(num_neurons=input_size, time_steps=1)
        self.experience_buffer: List[List[torch.Tensor]] = []

    def get_action(self, state: torch.Tensor) -> int:
        """
        ç¾åœ¨ã®çŠ¶æ…‹ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã«ã‚ˆã£ã¦å˜ä¸€ã®è¡Œå‹•ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®šã™ã‚‹ã€‚
        """
        self.model.eval()
        with torch.no_grad():
            input_spikes = (torch.rand_like(state) < (state * 0.5 + 0.5)).float()
            output_spikes, hidden_spikes_history = self.model(input_spikes)
            self.experience_buffer.append([input_spikes] + hidden_spikes_history)
            action = torch.argmax(output_spikes).item()
            return int(action)

    # --- â–¼ ä¿®æ­£ â–¼ ---
    def learn(self, reward: float, causal_credit: float = 0.0):
        """
        å—ã‘å–ã£ãŸå ±é…¬ä¿¡å·ã¾ãŸã¯å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã€‚
        """
        if not self.experience_buffer:
            return

        self.model.train()
        
        # å› æœçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãã‚Œã‚’å„ªå…ˆã—ã€å­¦ç¿’ã‚’å¢—å¹…ã•ã›ã‚‹
        if causal_credit > 0:
            # ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã¯é€šå¸¸ã®å ±é…¬ã‚ˆã‚Šã‚‚å¼·åŠ›ãªå­¦ç¿’ãƒˆãƒªã‚¬ãƒ¼ã¨ã™ã‚‹
            final_reward_signal = reward + causal_credit * 10.0 
            print(f"ğŸ§  ã‚·ãƒŠãƒ—ã‚¹å­¦ç¿’å¢—å¼·ï¼ (Causal Credit: {causal_credit})")
        else:
            final_reward_signal = reward
            
        optional_params = {"reward": final_reward_signal}
        
        for step_spikes in self.experience_buffer:
            self.model.update_weights(
                all_layer_spikes=step_spikes,
                optional_params=optional_params
            )
        
        # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãŒçµ‚äº†ã€ã¾ãŸã¯å¼·åŠ›ãªå­¦ç¿’ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸã‚‰ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
        if reward != -0.05 or causal_credit > 0:
            self.experience_buffer = []
    # --- â–² ä¿®æ­£ â–² ---