# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: app/main.py
# (å‹•çš„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰UI ä¿®æ­£ v16 - Gradio 4.20.0 æœ€çµ‚äº’æ›æ€§ä¿®æ­£)
# DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆ©ç”¨ã—ãŸã€Gradioãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±UIã®èµ·å‹•ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
#
# æ©Ÿèƒ½:
# - model_registry.json ã‚’èª­ã¿è¾¼ã¿ã€åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’Gradioãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã«è¡¨ç¤ºã€‚
# - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•° (argparse) ã‚’å—ã‘ä»˜ã‘ã€ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä¸Šæ›¸ã/è¿½åŠ ã™ã‚‹ã€‚
# - ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã¨ã€æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’å‹•çš„ã«åˆæœŸåŒ–ã™ã‚‹ã€‚
# - ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¤ãƒ—ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ/ç”»åƒï¼‰ã‚’configã‹ã‚‰åˆ¤æ–­ã—ã€é©åˆ‡ãªã‚¿ãƒ–ã«UIã‚’è¡¨ç¤ºã™ã‚‹ã€‚

import gradio as gr  # type: ignore[import-untyped]
import argparse
import sys
import time
from pathlib import Path
from typing import Iterator, Tuple, List, Dict, Optional, Any
from omegaconf import OmegaConf, DictConfig, Container # Container ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from dependency_injector import providers
import numpy as np
from PIL import Image
import asyncio
import logging
import os 

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.containers import AppContainer
# --- â–¼ ä¿®æ­£: get_avatar_svgs ã® import ã‚’å‰Šé™¤ â–¼ ---
# from app.utils import get_avatar_svgs 
# --- â–² ä¿®æ­£ â–² ---
from app.services.chat_service import ChatService
from app.services.image_classification_service import ImageClassificationService
from snn_research.deployment import SNNInferenceEngine
from snn_research.distillation.model_registry import ModelRegistry, SimpleModelRegistry 

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•° ---
container = AppContainer()
available_models_dict: Dict[str, Dict[str, Any]] = {} 

# --- ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–¢æ•° ---
def load_model_registry(registry_provider: providers.Provider[ModelRegistry]) -> Dict[str, Dict[str, Any]]:
    """model_registry.json ã‚’èª­ã¿è¾¼ã¿ã€UIç”¨ã®ãƒ¢ãƒ‡ãƒ«è¾æ›¸ã‚’ä½œæˆã™ã‚‹"""
    print("Loading model registry...")
    try:
        registry = registry_provider()
        if not hasattr(registry, 'registry_path') or not registry.registry_path or not Path(registry.registry_path).exists():
             logger.error(f"model_registry.json not found at: {getattr(registry, 'registry_path', 'N/A')}")
             raise FileNotFoundError
        
        models_list = asyncio.run(registry.list_models()) # åŒæœŸçš„ã«å®Ÿè¡Œ
        
        # UIãƒ•ãƒ¬ãƒ³ãƒ‰ãƒªãƒ¼ãªå½¢å¼ã«å¤‰æ› { "model_id (display)": { "path": "...", "config": {...} } }
        ui_models_dict: Dict[str, Dict[str, Any]] = {}
        for model_info in models_list:
            model_id = model_info.get("model_id")
            model_path = model_info.get("model_path") or model_info.get("path")
            config = model_info.get("config") # configã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ(è¾æ›¸)
            
            if model_id and model_path and config:
                # config ãŒ OmegaConf ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã€è¾æ›¸ã«å¤‰æ›
                if isinstance(config, Container): # OmegaConfã®åŸºåº•ã‚¯ãƒ©ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                    config_dict = OmegaConf.to_container(config, resolve=True)
                elif isinstance(config, dict):
                    config_dict = config
                else:
                    logger.warning(f"Skipping invalid model in registry for {model_id}: {type(config)}. Skipping.")
                    continue

                if not isinstance(config_dict, dict):
                     logger.warning(f"Converted config for {model_id} is not a dict: {type(config_dict)}. Skipping.")
                     continue
                     
                task_type = "image" if "spiking_cnn" in (config_dict.get("architecture_type") or "") else "text"
                ui_models_dict[model_id] = {
                    "path": model_path,
                    "config": config_dict, # <-- ãƒ—ãƒ¬ãƒ¼ãƒ³ãªè¾æ›¸ã‚’ä¿å­˜
                    "task_type": task_type
                }
            else:
                logger.warning(f"Skipping invalid model in registry: {model_id} (Path: {model_path}, Config: {'Exists' if config else 'Missing'})")
                
        print(f"âœ… Found {len(ui_models_dict)} valid models in registry.")
        return ui_models_dict
    except FileNotFoundError:
        registry_path = "config 'model_registry.file.path'"
        try:
             cfg_obj = container.config()
             if cfg_obj and hasattr(cfg_obj, 'model_registry') and hasattr(cfg_obj.model_registry, 'file') and hasattr(cfg_obj.model_registry.file, 'path'):
                 registry_path = container.config.model_registry.file.path() or "runs/model_registry.json"
        except Exception:
             pass
        logger.error(f"model_registry.json not found at path specified in config ({registry_path}). No models loaded.")
        return {}
    except Exception as e:
        logger.error(f"Error loading model registry: {e}")
        import traceback
        traceback.print_exc()
        return {}

def load_inference_services(model_id: str) -> Tuple[Optional[ChatService], Optional[ImageClassificationService], str, Dict, Dict, Dict]:
    """é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«IDã«åŸºã¥ã„ã¦æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹"""
    global available_models_dict # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’å‚ç…§

    if not model_id or model_id == "Select Model":
        return None, None, "Please select a model from the dropdown.", gr.update(visible=True), gr.update(visible=False), gr.update(visible=False) # Textã‚¿ãƒ–ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤º

    try:
        model_info = available_models_dict.get(model_id) 
        if not model_info:
            raise KeyError(f"Model ID '{model_id}' not found in the loaded models dictionary.")

        relative_path = model_info.get("path")
        model_config_dict = model_info.get("config") # .get() ã‚’ä½¿ç”¨
        task_type = model_info.get("task_type")
        
        model_path: Optional[str] = None
        if relative_path:
            resolved_path = Path(relative_path).resolve()
            if resolved_path.exists():
                model_path = str(resolved_path)
            else:
                logger.warning(f"Path for '{model_id}' exists in dict ('{relative_path}') but not found on disk at '{resolved_path}'.")
        
        print(f"[DEBUG] Loading model '{model_id}':")
        print(f"  [DEBUG] - Relative Path in Dict: {relative_path}")
        print(f"  [DEBUG] - Resolved Path: {model_path}")
        print(f"  [DEBUG] - Config retrieved (type {type(model_config_dict)}): {str(model_config_dict)[:200]}...") 
        print(f"  [DEBUG] - Task type retrieved: {task_type}")

        if not model_path or model_config_dict is None or not task_type:
            missing = []
            if not model_path: missing.append(f"path (Resolved path from '{relative_path}' failed)")
            if model_config_dict is None: missing.append("config")
            if not task_type: missing.append("task_type")
            error_msg = f"Model info for '{model_id}' is incomplete. Missing: {', '.join(missing)}."
            logger.error(error_msg)
            raise ValueError(error_msg)

        # ãƒ™ãƒ¼ã‚¹è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ãƒãƒ¼ã‚¸
        config = container.config()
        full_config_dict = OmegaConf.merge(config, {"model": model_config_dict})
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’ä¸Šæ›¸ã
        OmegaConf.update(full_config_dict, "model.path", model_path, merge=True)
        
        # DIã‚³ãƒ³ãƒ†ãƒŠã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä¸Šæ›¸ãã—ã¦ã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
        engine_provider = container.snn_inference_engine
        
        chat_service: Optional[ChatService] = None
        image_service: Optional[ImageClassificationService] = None
        # service_instance can hold either a ChatService or ImageClassificationService depending on task
        service_instance: Optional[ChatService | ImageClassificationService] = None
        status_message = ""
        
        if task_type == "text":
            with engine_provider.override(providers.Factory(SNNInferenceEngine, config=full_config_dict)):
                service_instance = container.chat_service()
            chat_service = service_instance
            status_message = f"âœ… Text Model '{model_id}' loaded."
            print(status_message)
            # ãƒ†ã‚­ã‚¹ãƒˆã‚¿ãƒ–ã‚’è¡¨ç¤ºã—ã€ç”»åƒã‚¿ãƒ–ã‚’éš ã™
            return chat_service, None, status_message, gr.update(selected="text_tab"), gr.update(visible=True), gr.update(visible=False)

        elif task_type == "image":
            with engine_provider.override(providers.Factory(SNNInferenceEngine, config=full_config_dict)):
                service_instance = container.image_classification_service()
            
            # --- â–¼ ä¿®æ­£: mypy [assignment] ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ (v24) â–¼ ---
            # 196è¡Œç›®ä»˜è¿‘ã®ã‚¨ãƒ©ãƒ¼:
            chat_service = None
            
            # 204è¡Œç›®ä»˜è¿‘ã®ã‚¨ãƒ©ãƒ¼ (returnæ–‡ã§ã®å‹ä¸ä¸€è‡´) ã‚’é˜²ããŸã‚ã€
            # image_service ã«æ­£ã—ã„å‹ (ImageClassificationService) ã‚’ä»£å…¥ã™ã‚‹
            image_service = service_instance
            # --- â–² ä¿®æ­£ â–² ---

            status_message = f"âœ… Image Model '{model_id}' loaded."
            print(status_message)
            # ç”»åƒã‚¿ãƒ–ã‚’è¡¨ç¤ºã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¿ãƒ–ã‚’éš ã™
            return chat_service, image_service, status_message, gr.update(selected="image_tab"), gr.update(visible=False), gr.update(visible=True)

        else:
            status_message = f"âš ï¸ Unknown task type '{task_type}' for model '{model_id}'."
            print(status_message)
            # ä¸æ˜ãªå ´åˆã¯ãƒ†ã‚­ã‚¹ãƒˆã‚¿ãƒ–ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤º
            return None, None, status_message, gr.update(selected="text_tab"), gr.update(visible=True), gr.update(visible=False)

    except Exception as e:
        status_message = f"âŒ Error loading model '{model_id}': {e}"
        print(status_message)
        import traceback
        traceback.print_exc()
         # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ†ã‚­ã‚¹ãƒˆã‚¿ãƒ–ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¡¨ç¤º
        return None, None, status_message, gr.update(selected="text_tab"), gr.update(visible=True), gr.update(visible=False)


# --- Gradio UI æ§‹ç¯‰ ---
def main():
    global available_models_dict # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’é–¢æ•°å†…ã§ä½¿ç”¨å®£è¨€
    
    # 1. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’å®šç¾©
    parser = argparse.ArgumentParser(description="SNN Multi-Task Interface")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="Base config file path")
    # --- å‹•çš„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã®ãŸã‚ã®å¼•æ•°ã‚’è¿½åŠ  ---
    parser.add_argument("--chat_model_config", type=str, help="Path to the chat model config file (e.g., small.yaml)")
    parser.add_argument("--chat_model_path", type=str, help="Path to the chat model weights (.pth)")
    parser.add_argument("--cifar_model_config", type=str, help="Path to the CIFAR model config file")
    parser.add_argument("--cifar_model_path", type=str, help="Path to the CIFAR model weights (.pth)")
    parser.add_argument("--ai_tech_model_config", type=str, help="Path to the AI tech model config file")
    parser.add_argument("--ai_tech_model_path", type=str, help="Path to the AI tech model weights (.pth)")
    parser.add_argument("--summarization_model_config", type=str, help="Path to the summarization model config file")
    parser.add_argument("--summarization_model_path", type=str, help="Path to the summarization model weights (.pth)")
    
    args = parser.parse_args()

    # 2. DIã‚³ãƒ³ãƒ†ãƒŠã¨è¨­å®šã®ãƒ­ãƒ¼ãƒ‰
    container.config.from_yaml(args.config)
    container.wire(modules=[__name__])

    # 3. ãƒ¢ãƒ‡ãƒ«ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ (ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã«ä»£å…¥)
    available_models_dict = load_model_registry(container.model_registry)

    # 4. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ *ä¸Šæ›¸ã* ã¾ãŸã¯ *è¿½åŠ * (ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’æ›´æ–°)
    
    # (ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°)
    def add_model_from_args(model_id, config_path, model_path):
        global available_models_dict # ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã‚’å¤‰æ›´ã™ã‚‹å®£è¨€
        if config_path and model_path:
            # --- â–¼ ä¿®æ­£: ã“ã“ã§ã¯ .exists() ãƒã‚§ãƒƒã‚¯ã‚’ã—ãªã„ (ç›¸å¯¾ãƒ‘ã‚¹ã®ãŸã‚) â–¼ ---
            # if not Path(model_path).exists():
            #     logger.warning(f"File not found for model '{model_id}' from command line: {model_path}. Skipping.")
            #     return
            if not Path(config_path).exists():
                logger.warning(f"Config file not found for model '{model_id}' from command line: {config_path}. Skipping.")
                return
            # --- â–² ä¿®æ­£ â–² ---
                
            try:
                config_obj = OmegaConf.load(config_path)
                model_config_block = config_obj.get('model', config_obj) 
                
                model_config_dict = OmegaConf.to_container(model_config_block, resolve=True)
                if not isinstance(model_config_dict, dict):
                    raise TypeError(f"Loaded config for {model_id} is not a dictionary.")
                    
                task_type = "image" if "spiking_cnn" in (model_config_dict.get("architecture_type") or "") else "text"
                
                print(f"[DEBUG] Preparing to add/update model '{model_id}' from args:")
                print(f"  [DEBUG] - Path: {model_path}")
                print(f"  [DEBUG] - Config dict (type {type(model_config_dict)}): {str(model_config_dict)[:200]}...")
                print(f"  [DEBUG] - Task type: {task_type}")

                available_models_dict[model_id] = {
                    "path": model_path, # <-- ç›¸å¯¾ãƒ‘ã‚¹ã®ã¾ã¾ä¿å­˜
                    "config": model_config_dict, 
                    "task_type": task_type
                }
                print(f"âœ… Loaded/Updated model '{model_id}' from command line arguments.")
            except Exception as e:
                logger.error(f"Error loading model '{model_id}' from command line args ({config_path}): {e}")

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§æŒ‡å®šã•ã‚ŒãŸå„ãƒ¢ãƒ‡ãƒ«ã‚’å‡¦ç†
    add_model_from_args("chat_model_default", args.chat_model_config, args.chat_model_path)
    add_model_from_args("cifar10_distilled_from_resnet18", args.cifar_model_config, args.cifar_model_path)
    add_model_from_args("æœ€æ–°ã®aiæŠ€è¡“", args.ai_tech_model_config, args.ai_tech_model_path)
    add_model_from_args("æ–‡ç« è¦ç´„", args.summarization_model_config, args.summarization_model_path)

    model_choices = ["Select Model"] + list(available_models_dict.keys())

    # 5. Gradio UI æ§‹ç¯‰
    initial_stats_md = "**Inference Time:** `N/A`\n**Tokens/Second:** `N/A`\n---\n**Total Spikes:** `N/A`\n**Spikes/Second:** `N/A`"

    with gr.Blocks(theme=gr.themes.Soft(primary_hue="teal", secondary_hue="green")) as demo:
        
        chat_service_state = gr.State(None)
        image_service_state = gr.State(None)

        gr.Markdown("# ğŸ§  SNN Multi-Task Interface (Dynamic Loading)")
        gr.Markdown("`runs/model_registry.json` ãŠã‚ˆã³ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚")

        with gr.Row():
            model_dropdown = gr.Dropdown(
                label="Select Model",
                choices=model_choices,
                value=model_choices[0]
            )
            status_textbox = gr.Textbox(label="Status", interactive=False)
        
        with gr.Tabs() as tabs_container:
            
            with gr.TabItem("ğŸ’¬ Text / Chat", id="text_tab") as text_tab:
                gr.Markdown("ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒãƒ£ãƒƒãƒˆã€QAã€è¦ç´„ãªã©ï¼‰ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚")
                with gr.Row():
                    with gr.Column(scale=2):
                        # --- â–¼ ä¿®æ­£: type="messages" ã¨ avatar_images ã‚’å‰Šé™¤ â–¼ ---
                        chat_chatbot = gr.Chatbot(
                            label="SNN Chat", 
                            height=500
                        )
                        # --- â–² ä¿®æ­£ â–² ---
                    with gr.Column(scale=1):
                        chat_stats_display = gr.Markdown(value=initial_stats_md, label="ğŸ“Š Inference Stats")
                with gr.Row():
                    chat_msg_textbox = gr.Textbox(show_label=False, placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›...", container=False, scale=6)
                    chat_submit_btn = gr.Button("Send", variant="primary", scale=1)
                    chat_clear_btn = gr.Button("Clear", scale=1)
                
                with gr.Accordion("Summarization", open=False):
                    gr.Markdown("ãƒãƒ£ãƒƒãƒˆã§ã¯ãªãã€ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã§è¦ç´„ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚")
                    with gr.Row():
                        sum_input_textbox = gr.Textbox(label="Input Text", lines=10, placeholder="è¦ç´„ã—ãŸã„æ–‡ç« ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...")
                        sum_output_textbox = gr.Textbox(label="Summary", lines=10, interactive=False)
                    sum_summarize_btn = gr.Button("Summarize", variant="primary")
                    sum_stats_display = gr.Markdown(value=initial_stats_md, label="ğŸ“Š Inference Stats")

            with gr.TabItem("ğŸ–¼ï¸ Image Classification", id="image_tab", visible=False) as image_tab:
                gr.Markdown("ç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ï¼ˆSpikingCNNãªã©ï¼‰ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚")
                with gr.Row():
                    img_input = gr.Image(type="pil", label="Upload Image")
                    img_output_label = gr.Label(num_top_classes=3, label="Classification Result")
                img_classify_btn = gr.Button("Classify Image", variant="primary")

        # --- ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ© ---
        
        model_dropdown.change(
            fn=load_inference_services,
            inputs=[model_dropdown], 
            outputs=[
                chat_service_state, 
                image_service_state, 
                status_textbox, 
                tabs_container,
                text_tab,
                image_tab
            ],
            queue=False # queue=False ã‚’æŒ‡å®š
        )

        def chat_clear_all(): return [], "", initial_stats_md
        
        # --- â–¼ ä¿®æ­£: stream_chat_wrapper ã‚’ "List[List]" (ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ) å½¢å¼ã«å¯¾å¿œã•ã›ã‚‹ â–¼ ---
        def stream_chat_wrapper(message: str, history: List[List[Optional[str]]], service: Optional[ChatService]):
            """
            Gradioã® "List[List]" å½¢å¼ (ã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆ) ã‚’å—ã‘å–ã‚Šã€
            ChatServiceã® List[List] å½¢å¼ã«æ¸¡ã—ã€
            çµæœã‚’å†ã³ "List[List]" å½¢å¼ã§è¿”ã™ãƒ©ãƒƒãƒ‘ãƒ¼ã€‚
            """
            if not service:
                history.append([message, "Error: Chat service is not loaded."])
                yield history, initial_stats_md
                return
            if not message:
                 yield history, initial_stats_md
                 return

            try:
                # ChatService.stream_response ã¯ List[List[Optional[str]]] ã‚’æœŸå¾…
                yield from service.stream_response(message, history) 
            except Exception as e:
                 logger.error(f"Error during chat stream: {e}")
                 history.append([message, f"Error: {e}"])
                 yield history, initial_stats_md
        # --- â–² ä¿®æ­£ â–² ---

        chat_submit_event = msg_textbox.submit(fn=stream_chat_wrapper, inputs=[chat_msg_textbox, chat_chatbot, chat_service_state], outputs=[chat_chatbot, chat_stats_display], queue=False) # queue=False
        chat_submit_event.then(fn=lambda: "", inputs=None, outputs=chat_msg_textbox)
        chat_button_submit_event = chat_submit_btn.click(fn=stream_chat_wrapper, inputs=[chat_msg_textbox, chat_chatbot, chat_service_state], outputs=[chat_chatbot, chat_stats_display], queue=False) # queue=False
        chat_button_submit_event.then(fn=lambda: "", inputs=None, outputs=chat_msg_textbox)
        
        chat_clear_btn.click(fn=chat_clear_all, inputs=None, outputs=[chat_chatbot, chat_msg_textbox, chat_stats_display], queue=False)
        
        def summarize_text(text: str, service: Optional[ChatService]) -> Tuple[str, str]:
            if not service:
                return "Error: Summarization service is not loaded.", initial_stats_md
            if not text:
                return "", initial_stats_md
            full_response = ""
            stats_md_output = initial_stats_md
            try:
                iterator = service.stream_response(text, [])
                final_history: List[List[Optional[str]]] = []
                while True:
                    try:
                        current_history, stats_md_output = next(iterator)
                        final_history = current_history
                    except StopIteration:
                        if final_history and final_history[-1] and len(final_history[-1]) > 1:
                            response_content = final_history[-1][1]
                            full_response = response_content if response_content is not None else ""
                        break
            except Exception as e:
                logger.error(f"Error during summarization: {e}")
                return f"Error: {e}", initial_stats_md
            return full_response, stats_md_output

        sum_summarize_btn.click(
            fn=summarize_text,
            inputs=[sum_input_textbox, chat_service_state],
            outputs=[sum_output_textbox, sum_stats_display],
            queue=False # queue=False ã‚’æŒ‡å®š
        )

        def classify_image(image: Any, service: Optional[ImageClassificationService]) -> Dict[str, float]:
             if not service:
                 return {"Error": 1.0, "Service not loaded": 0.0}
             if image is None:
                 return {"Error": 1.0, "No image provided": 0.0}
             try:
                 return service.predict(image)
             except Exception as e:
                  logger.error(f"Error during image classification: {e}")
                  return {"Error": 1.0, str(e): 0.0}

        img_classify_btn.click(
            fn=classify_image, 
            inputs=[img_input, image_service_state], 
            outputs=[img_output_label],
            queue=False # queue=False ã‚’æŒ‡å®š
        )

    # 6. Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•
    config_obj = container.config()
    server_port_val = config_obj.get('app', {}).get('server_port', 7860)
    server_name_val = config_obj.get('app', {}).get('server_name', '127.0.0.1')
    
    server_port = int(server_port_val) if server_port_val is not None else 7860
    server_name = str(server_name_val) if server_name_val is not None else "127.0.0.1"

    print("\nStarting Gradio web server for Multi-Task app...")
    print(f"Please open http://{server_name}:{server_port} in your browser.")
    
    # --- â–¼ ä¿®æ­£: .queue() ã‚’å‰Šé™¤ â–¼ ---
    demo.launch(server_name=server_name, server_port=server_port)
    # --- â–² ä¿®æ­£ â–² ---

if __name__ == "__main__":
    main()