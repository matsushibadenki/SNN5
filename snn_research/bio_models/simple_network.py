# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/bio_models/simple_network.py
# (ä¿®æ­£)
# ä¿®æ­£: learning_rule.update ãŒã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã‚ˆã†ã«ãªã£ãŸãŸã‚ã€
#       æˆ»ã‚Šå€¤ã‚’æ­£ã—ãã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
# ä¿®æ­£: CausalTraceCreditAssignmentEnhancedV2 ã«å¯¾å¿œ
#
# æ”¹å–„ (v2):
# - doc/The-flow-of-brain-behavior.md ãŠã‚ˆã³ doc/ãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆå¼·åŒ–æ¡ˆã®èª¿æŸ».md (ã‚»ã‚¯ã‚·ãƒ§ãƒ³2.3) ã«åŸºã¥ãã€
#   å˜ä¸€ã®å­¦ç¿’å‰‡ã—ã‹æŒã¦ãªã‹ã£ãŸåˆ¶ç´„ã‚’è§£æ¶ˆã€‚
# - ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ãƒ«ãƒ¼ãƒ« (synaptic_rule) ã¨ æ’å¸¸æ€§ç¶­æŒãƒ«ãƒ¼ãƒ« (homeostatic_rule) ã‚’
#   åˆ¥ã€…ã«å—ã‘å–ã‚Šã€ä¸¡æ–¹ã‚’é©ç”¨ã§ãã‚‹ã‚ˆã†ã« __init__ ã¨ update_weights ã‚’å¤‰æ›´ã€‚
#
# æ”¹å–„ (v3):
# - doc/ROADMAP.md (P8.2) ãŠã‚ˆã³ doc/The-flow-of-brain-behavior.md ã«åŸºã¥ãã€
#   E/Iåˆ†é›¢ï¼ˆèˆˆå¥®æ€§/æŠ‘åˆ¶æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®åˆ†é›¢ï¼‰ã¨ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ã‚’å®Ÿè£…ã™ã‚‹ã€‚
# - __init__:
#   - `layer_sizes: List[int]` ã‚’ `layer_configs: List[Dict[str, int]]` ã«å¤‰æ›´ã€‚
#     å„å±¤ã®Eãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (n_e) ã¨ Iãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (n_i) ã‚’æŒ‡å®šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
#   - `weights` ã‚’ `weights_ee`, `weights_ei`, `weights_ie`, `weights_ii` ã®
#     4ã¤ã® nn.ParameterList ã«åˆ†å‰²ã€‚
# - forward:
#   - ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ã‚’é©ç”¨ã€‚Eãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰ã®é‡ã¿ã¯ `F.relu()` ã§æ­£ã«ã€
#     Iãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰ã®é‡ã¿ã¯ `-F.relu()` ã§è² ã«åˆ¶ç´„ã™ã‚‹ã€‚
#   - E/Ié›†å›£é–“ã®ç›¸äº’ä½œç”¨ã‚’è¨ˆç®—ã™ã‚‹ã‚ˆã†ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã€‚
# - update_weights:
#   - å­¦ç¿’å‰‡ãŒ4ã¤ã®é‡ã¿è¡Œåˆ—ã™ã¹ã¦ã«é©ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
#   - ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ã®åˆ¶ç´„ã‚’å­¦ç¿’å¾Œã«ã‚‚é©ç”¨ï¼ˆclamp_weightsãƒ¡ã‚½ãƒƒãƒ‰ï¼‰ã€‚
#
# ä¿®æ­£ (v4):
# - mypy [name-defined] ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€cast ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚

import torch
import torch.nn as nn
# --- â–¼ ä¿®æ­£ (v4): cast ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import Dict, Any, Optional, Tuple, List, cast
import torch.nn.functional as F
# --- â–² ä¿®æ­£ (v4) â–² ---

from .lif_neuron import BioLIFNeuron
from snn_research.learning_rules.base_rule import BioLearningRule
# --- â–¼ ä¿®æ­£ â–¼ ---
# V2 ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.learning_rules.causal_trace import CausalTraceCreditAssignmentEnhancedV2
# --- â–² ä¿®æ­£ â–² ---


class BioSNN(nn.Module):
    """
    (æ”¹å–„ v3) E/Iåˆ†é›¢ã¨ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ã‚’å®Ÿè£…ã—ãŸç”Ÿç‰©å­¦çš„SNN (P8.2)ã€‚
    """
    def __init__(
        self, 
        # --- â–¼ ä¿®æ­£ (v3): layer_sizes ã‚’ layer_configs ã«å¤‰æ›´ â–¼ ---
        layer_configs: List[Dict[str, int]], # ä¾‹: [{"n_e": 80, "n_i": 20}, {"n_e": 50, "n_i": 10}]
        input_size: int, # å…¥åŠ›å±¤ã®ã‚µã‚¤ã‚ºã¯åˆ¥é€”æŒ‡å®š
        # --- â–² ä¿®æ­£ (v3) â–² ---
        neuron_params: dict, 
        synaptic_rule: BioLearningRule,
        homeostatic_rule: Optional[BioLearningRule] = None,
        sparsification_config: Optional[Dict[str, Any]] = None
    ):
        super().__init__()
        self.layer_configs = layer_configs
        self.input_size = input_size
        self.synaptic_rule = synaptic_rule
        self.homeostatic_rule = homeostatic_rule
        self.sparsification_enabled = sparsification_config.get("enabled", False) if sparsification_config else False
        self.contribution_threshold = sparsification_config.get("contribution_threshold", 0.0) if sparsification_config else 0.0
        if self.sparsification_enabled:
            print(f"ğŸ§¬ é©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ãŒæœ‰åŠ¹ã§ã™ (è²¢çŒ®åº¦é–¾å€¤: {self.contribution_threshold})")
        if self.homeostatic_rule:
            print(f"âš–ï¸ æ’å¸¸æ€§ç¶­æŒãƒ«ãƒ¼ãƒ« ({type(self.homeostatic_rule).__name__}) ãŒæœ‰åŠ¹ã§ã™ã€‚")
        if not self.layer_configs:
             raise ValueError("layer_configs must not be empty.")

        self.layers_e = nn.ModuleList()
        self.layers_i = nn.ModuleList()
        
        # --- â–¼ ä¿®æ­£ (v3): E/Iåˆ†é›¢ã—ãŸé‡ã¿è¡Œåˆ— â–¼ ---
        self.weights_ee = nn.ParameterList()
        self.weights_ei = nn.ParameterList() # I -> E (æŠ‘åˆ¶æ€§)
        self.weights_ie = nn.ParameterList() # E -> I (èˆˆå¥®æ€§)
        self.weights_ii = nn.ParameterList() # I -> I (æŠ‘åˆ¶æ€§)
        # --- â–² ä¿®æ­£ (v3) â–² ---

        current_input_dim_e = input_size
        current_input_dim_i = 0 # å…¥åŠ›å±¤ã¯æŠ‘åˆ¶æ€§ã‚’æŒãŸãªã„ã¨ä»®å®š

        for config in layer_configs:
            n_e = config["n_e"]
            n_i = config["n_i"]
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®ä½œæˆ
            self.layers_e.append(BioLIFNeuron(n_e, neuron_params))
            if n_i > 0:
                self.layers_i.append(BioLIFNeuron(n_i, neuron_params))
            
            # --- é‡ã¿è¡Œåˆ—ã®ä½œæˆ (ãƒ‡ãƒ¼ãƒ«å‰‡ã®ãŸã‚4åˆ†å‰²) ---
            # E -> E
            self.weights_ee.append(nn.Parameter(torch.rand(n_e, current_input_dim_e) * 0.5))
            # E -> I
            if n_i > 0:
                self.weights_ie.append(nn.Parameter(torch.rand(n_i, current_input_dim_e) * 0.5))
            
            if current_input_dim_i > 0:
                # I -> E
                self.weights_ei.append(nn.Parameter(torch.rand(n_e, current_input_dim_i) * 0.5))
                # I -> I
                if n_i > 0:
                    self.weights_ii.append(nn.Parameter(torch.rand(n_i, current_input_dim_i) * 0.5))
            
            # æ¬¡ã®å±¤ã®å…¥åŠ›æ¬¡å…ƒã‚’æ›´æ–°
            current_input_dim_e = n_e
            current_input_dim_i = n_i
            
        print(f"âœ… E/Iåˆ†é›¢å‹ BioSNN (P8.2) ãŒ {len(self.layers_e)} å±¤ã§æ§‹ç¯‰ã•ã‚Œã¾ã—ãŸã€‚")


    def _apply_dale_law(self) -> None:
        """ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ï¼ˆé‡ã¿ã®ç¬¦å·åˆ¶ç´„ï¼‰ã‚’ã‚¤ãƒ³ãƒ—ãƒ¬ãƒ¼ã‚¹ã§é©ç”¨ã™ã‚‹ã€‚"""
        with torch.no_grad():
            for w in self.weights_ee: w.data = F.relu(w.data)
            for w in self.weights_ie: w.data = F.relu(w.data)
            # æŠ‘åˆ¶æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰ã®é‡ã¿ã¯è² ã®å€¤ï¼ˆã®çµ¶å¯¾å€¤ï¼‰ã¨ã—ã¦æ‰±ã†
            for w in self.weights_ei: w.data = F.relu(w.data)
            for w in self.weights_ii: w.data = F.relu(w.data)

    def forward(self, input_spikes: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        """
        E/Iåˆ†é›¢ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã€‚
        Args:
            input_spikes (torch.Tensor): å…¥åŠ›ã‚¹ãƒ‘ã‚¤ã‚¯ (N_input,)
        Returns:
            Tuple[torch.Tensor, List[torch.Tensor]]: 
                (æœ€çµ‚å±¤ã®Eãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¯, å…¨å±¤ã®E/Iã‚¹ãƒ‘ã‚¤ã‚¯å±¥æ­´)
        """
        all_spikes_history: List[torch.Tensor] = [input_spikes]
        
        spikes_e_prev = input_spikes
        spikes_i_prev = None # æœ€åˆã®å±¤ã«ã¯æŠ‘åˆ¶æ€§å…¥åŠ›ã¯ãªã„

        # ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ï¼ˆé‡ã¿ã®ç¬¦å·åˆ¶ç´„ï¼‰ã‚’é©ç”¨
        self._apply_dale_law()

        for i in range(len(self.layers_e)):
            layer_e = cast(BioLIFNeuron, self.layers_e[i])
            layer_i: Optional[BioLIFNeuron] = None
            if i < len(self.layers_i):
                layer_i = cast(BioLIFNeuron, self.layers_i[i])
            
            # 1. èˆˆå¥®æ€§ (E) ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¸ã®é›»æµã‚’è¨ˆç®—
            current_e = torch.matmul(self.weights_ee[i], spikes_e_prev) # E -> E (èˆˆå¥®æ€§)
            
            if i > 0 and spikes_i_prev is not None and i-1 < len(self.weights_ei):
                # I -> E (æŠ‘åˆ¶æ€§)
                current_e -= torch.matmul(self.weights_ei[i-1], spikes_i_prev)
            
            spikes_e_t = layer_e(current_e) # (N_e,)
            spikes_i_t: Optional[torch.Tensor] = None
            
            # 2. æŠ‘åˆ¶æ€§ (I) ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¸ã®é›»æµã‚’è¨ˆç®—
            if layer_i is not None:
                current_i = torch.matmul(self.weights_ie[i], spikes_e_prev) # E -> I (èˆˆå¥®æ€§)
                
                if i > 0 and spikes_i_prev is not None and i-1 < len(self.weights_ii):
                    # I -> I (æŠ‘åˆ¶æ€§)
                    current_i -= torch.matmul(self.weights_ii[i-1], spikes_i_prev)
                
                spikes_i_t = layer_i(current_i) # (N_i,)
                
                # å±¥æ­´ã«ä¿å­˜ (Eã¨Iã‚’çµåˆ)
                all_spikes_history.append(torch.cat([spikes_e_t, spikes_i_t]))
                spikes_i_prev = spikes_i_t
            else:
                # æŠ‘åˆ¶æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒãªã„å±¤
                all_spikes_history.append(spikes_e_t)
                spikes_i_prev = None

            # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›
            spikes_e_prev = spikes_e_t

        return spikes_e_prev, all_spikes_history # æœ€çµ‚å±¤ã®èˆˆå¥®æ€§ã‚¹ãƒ‘ã‚¤ã‚¯ã¨å…¨å±¥æ­´ã‚’è¿”ã™


    def update_weights(
        self,
        all_layer_spikes: List[torch.Tensor], # E/Içµåˆæ¸ˆã¿ã‚¹ãƒ‘ã‚¤ã‚¯ã®ãƒªã‚¹ãƒˆ
        optional_params: Optional[Dict[str, Any]] = None
    ):
        """(æ”¹å–„ v3) E/Iåˆ†é›¢ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å­¦ç¿’å‰‡ã«åŸºã¥ãæ›´æ–°ã™ã‚‹ã€‚"""
        if not self.training:
            return

        backward_credit_e: Optional[torch.Tensor] = None
        backward_credit_i: Optional[torch.Tensor] = None
        current_params = optional_params.copy() if optional_params else {}

        for i in reversed(range(len(self.layers_e))):
            # --- 1. ã‚¹ãƒ‘ã‚¤ã‚¯å±¥æ­´ã®åˆ†é›¢ ---
            pre_spikes_all = all_layer_spikes[i]
            post_spikes_all = all_layer_spikes[i+1]
            
            n_e_pre: int
            n_i_pre: int = 0
            spikes_i_pre: Optional[torch.Tensor] = None
            
            if i == 0:
                n_e_pre = self.input_size
                spikes_e_pre = pre_spikes_all
            else:
                n_e_pre = self.layer_configs[i-1]["n_e"]
                n_i_pre = self.layer_configs[i-1]["n_i"]
                spikes_e_pre = pre_spikes_all[:n_e_pre]
                if n_i_pre > 0:
                    spikes_i_pre = pre_spikes_all[n_e_pre:]

            n_e_post = self.layer_configs[i]["n_e"]
            n_i_post = self.layer_configs[i]["n_i"]
            spikes_e_post = post_spikes_all[:n_e_post]
            spikes_i_post: Optional[torch.Tensor] = None
            if n_i_post > 0:
                spikes_i_post = post_spikes_all[n_e_post:]
            
            # --- 2. ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã®æº–å‚™ ---
            current_credit_e = current_params.get("reward", 0.0)
            current_credit_i = current_params.get("reward", 0.0)
            if backward_credit_e is not None:
                current_credit_e += backward_credit_e.mean().item() * 0.1
            if backward_credit_i is not None:
                current_credit_i += backward_credit_i.mean().item() * 0.1
                
            params_e = current_params.copy(); params_e["reward"] = current_credit_e
            params_i = current_params.copy(); params_i["reward"] = current_credit_i

            # --- 3. å­¦ç¿’å‰‡ã®é©ç”¨ (4ã¤ã®è¡Œåˆ—ã™ã¹ã¦) ---
            
            # --- E -> E ---
            dw_ee, bwd_e_from_ee = self._apply_rules(self.weights_ee[i], spikes_e_pre, spikes_e_post, params_e)
            
            bwd_e_from_ie = torch.zeros_like(spikes_e_pre)
            if spikes_i_post is not None and i < len(self.weights_ie):
                # --- E -> I ---
                dw_ie, bwd_e_from_ie_new = self._apply_rules(self.weights_ie[i], spikes_e_pre, spikes_i_post, params_i)
                self.weights_ie[i].data += dw_ie
                bwd_e_from_ie = bwd_e_from_ie_new

            backward_credit_e_t = bwd_e_from_ee + bwd_e_from_ie # E_pre ã¸ã®ç·ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ
            backward_credit_i_t = torch.zeros_like(spikes_e_pre) # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (I_pre ãŒãªã„å ´åˆ)
            
            if i > 0 and spikes_i_pre is not None:
                bwd_i_from_ei = torch.zeros_like(spikes_i_pre)
                bwd_i_from_ii = torch.zeros_like(spikes_i_pre)
                
                if i-1 < len(self.weights_ei):
                    # --- I -> E ---
                    dw_ei, bwd_i_from_ei_new = self._apply_rules(self.weights_ei[i-1], spikes_i_pre, spikes_e_post, params_e)
                    self.weights_ei[i-1].data += dw_ei
                    bwd_i_from_ei = bwd_i_from_ei_new
                
                if spikes_i_post is not None and i-1 < len(self.weights_ii):
                    # --- I -> I ---
                    dw_ii, bwd_i_from_ii_new = self._apply_rules(self.weights_ii[i-1], spikes_i_pre, spikes_i_post, params_i)
                    self.weights_ii[i-1].data += dw_ii
                    bwd_i_from_ii = bwd_i_from_ii_new
                
                backward_credit_i_t = bwd_i_from_ei + bwd_i_from_ii # I_pre ã¸ã®ç·ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ

            # é‡ã¿æ›´æ–° (E -> E/I)
            self.weights_ee[i].data += dw_ee
            
            # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’æ›´æ–°
            backward_credit_e = backward_credit_e_t
            backward_credit_i = backward_credit_i_t

        # æœ€çµ‚çš„ãªé‡ã¿ã®åˆ¶ç´„ï¼ˆãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ï¼‰ã‚’é©ç”¨
        self.clamp_weights()

    def _apply_rules(
        self, 
        weights: nn.Parameter, 
        pre_spikes: torch.Tensor, 
        post_spikes: torch.Tensor, 
        params: Dict[str, Any]
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """(v3) ã‚·ãƒŠãƒ—ã‚¹å‰‡ã¨æ’å¸¸æ€§å‰‡ã®ä¸¡æ–¹ã‚’é©ç”¨ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        
        dw_synaptic, backward_credit = self.synaptic_rule.update(
            pre_spikes=pre_spikes,
            post_spikes=post_spikes,
            weights=weights,
            optional_params=params
        )
        
        dw_homeostasis = torch.zeros_like(weights.data)
        if self.homeostatic_rule:
            dw_homeo, _ = self.homeostatic_rule.update(
                pre_spikes=pre_spikes,
                post_spikes=post_spikes,
                weights=weights,
                optional_params=params
            )
            dw_homeostasis = dw_homeo

        dw = dw_synaptic + dw_homeostasis
        
        if backward_credit is None:
            backward_credit = torch.zeros(pre_spikes.shape[0], device=pre_spikes.device) # å½¢çŠ¶ã‚’ pre_spikes ã«åˆã‚ã›ã‚‹

        # ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ– (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
        if self.sparsification_enabled and isinstance(self.synaptic_rule, CausalTraceCreditAssignmentEnhancedV2):
            causal_contribution = self.synaptic_rule.get_causal_contribution()
            if causal_contribution is not None:
                contribution_mask = causal_contribution > self.contribution_threshold
                dw = dw * contribution_mask

        return dw, backward_credit

    def clamp_weights(self) -> None:
        """ãƒ‡ãƒ¼ãƒ«ã®æ³•å‰‡ï¼ˆé‡ã¿ã®ç¬¦å·åˆ¶ç´„ï¼‰ã‚’å­¦ç¿’å¾Œã«å¼·åˆ¶çš„ã«é©ç”¨ã™ã‚‹ã€‚"""
        with torch.no_grad():
            for w_list in [self.weights_ee, self.weights_ie, self.weights_ei, self.weights_ii]:
                for w in w_list:
                    w.data.clamp_(min=0) # ã™ã¹ã¦ã®é‡ã¿ã‚’éè² ã«ä¿ã¤