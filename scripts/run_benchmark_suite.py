# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/run_benchmark_suite.py
# Title: çµ±åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆ
# Description: è¤‡æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿé¨“ã‚’ä½“ç³»çš„ã«å®Ÿè¡Œã—ã€çµæœã‚’ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰å½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆã«è¿½è¨˜ã™ã‚‹ã€‚
# æ”¹å–„ç‚¹(v2): MRPCã‚¿ã‚¹ã‚¯ã®æ¯”è¼ƒå®Ÿé¨“ã‚’è¿½åŠ ã€‚
# æ”¹å–„ç‚¹(v3): ç¶™ç¶šå­¦ç¿’è©•ä¾¡ã®ãŸã‚ã€--model_path ã¨ --eval_only ã‚’è¿½åŠ ã€‚
#             è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã€è©•ä¾¡ã®ã¿ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
#
# ä¿®æ­£ (v4):
# - å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ (health-check) ã§ã® `omegaconf.errors.ConfigAttributeError: Missing key model` ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã€‚
# - `train_and_evaluate_model` ãŒ `cifar10_spikingcnn_config.yaml` ã®ã‚ˆã†ãª
#   `model:` ã‚­ãƒ¼ã‚’æŒãŸãªã„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã€
#   `run_distillation.py` ã¨åŒæ§˜ã« `{'model': ...}` ã§ãƒ©ãƒƒãƒ—ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
#
# ä¿®æ­£ (v5):
# - å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯ (health-check) ã§ã® `TypeError: train_and_evaluate_model() got multiple values for argument 'eval_only'` ã‚’ä¿®æ­£ã€‚
# - `run_experiment_by_name` å†…ã§ã® `train_and_evaluate_model` å‘¼ã³å‡ºã—ï¼ˆ`args.model_type` ãŒãªã„å ´åˆï¼‰ã«ãŠã„ã¦ã€
#   `eval_only` ãŒä½ç½®å¼•æ•°ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã®ä¸¡æ–¹ã§æ¸¡ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãƒã‚°ã‚’ä¿®æ­£ã€‚
#   `vocab_size` ã¾ã§ã®å¼•æ•°ã‚’ä½ç½®å¼•æ•°ã¨ã—ã€ãã‚Œä»¥é™ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã¨ã—ã¦æ˜ç¤ºçš„ã«æ¸¡ã™ã‚ˆã†ã«å¤‰æ›´ã€‚

import argparse
import time
import pandas as pd  # type: ignore
import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.utils.data import DataLoader
from tqdm import tqdm
from pathlib import Path
import sys
# --- â–¼ ä¿®æ­£: å¿…è¦ãªå‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ  â–¼ ---
from typing import Dict, List, Any, Optional, Callable, Sized, cast
# --- â–² ä¿®æ­£ â–² ---

sys.path.append(str(Path(__file__).resolve().parent.parent))

from snn_research.benchmark import TASK_REGISTRY, BenchmarkTask
from app.utils import get_auto_device
from transformers import AutoTokenizer
# --- â–¼ ä¿®æ­£: SNNCoreã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ç”¨ï¼‰ â–¼ ---
from snn_research.core.snn_core import SNNCore
from omegaconf import OmegaConf, DictConfig # DictConfig ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# --- â–² ä¿®æ­£ â–² ---

def train_and_evaluate_model(
    model_type: str,
    task: BenchmarkTask,
    train_loader: DataLoader,
    val_loader: DataLoader,
    device: str,
    epochs: int,
    learning_rate: float,
    vocab_size: int,
    # --- â–¼ ä¿®æ­£: è©•ä¾¡å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã®å¼•æ•°ã‚’è¿½åŠ  â–¼ ---
    eval_only: bool = False,
    model_path: Optional[str] = None,
    model_config_path: Optional[str] = None
    # --- â–² ä¿®æ­£ â–² ---
) -> Dict[str, Any]:
    """æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®è¨“ç·´ã¨è©•ä¾¡ã‚’è¡Œã†ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã€‚"""
    
    model: nn.Module
    
    # --- â–¼ ä¿®æ­£: è©•ä¾¡å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã®ãƒ­ã‚¸ãƒƒã‚¯ â–¼ ---
    if eval_only:
        if not model_path or not model_config_path:
            raise ValueError("--eval_only ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€--model_path ã¨ --model_config_path ã®ä¸¡æ–¹ãŒå¿…è¦ã§ã™ã€‚")
        print("\n" + "="*20 + f" ğŸš€ Starting EVALUATION for: {model_path} on {task.__class__.__name__} " + "="*20)
        
        # SNNCoreã‚³ãƒ³ãƒ†ãƒŠçµŒç”±ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
        try:
            # --- â–¼ ä¿®æ­£ (v4): 'model:' ã‚­ãƒ¼ãŒãªã„ã‚³ãƒ³ãƒ•ã‚£ã‚°ã«å¯¾å¿œ â–¼ ---
            cfg_raw: DictConfig = OmegaConf.load(model_config_path)
            cfg_model: DictConfig
            if "model" in cfg_raw:
                cfg_model = cfg_raw.model
            else:
                # cifar10_spikingcnn_config.yaml ã®ã‚ˆã†ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã€
                # cfg_rawè‡ªä½“ãŒãƒ¢ãƒ‡ãƒ«è¨­å®šã ã¨è¦‹ãªã™
                cfg_model = cfg_raw
            # --- â–² ä¿®æ­£ (v4) â–² ---

            # ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆSNNã‹ANNã‹ï¼‰ã«åŸºã¥ã„ã¦ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’å¤‰æ›´
            if model_type == 'SNN':
                # vocab_sizeã¯ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦è¨­å®š
                num_classes = 10 if task.__class__.__name__ == "CIFAR10Task" else vocab_size
                model_container = SNNCore(config=cfg_model, vocab_size=num_classes)
                model = model_container.model # SNNCoreå†…éƒ¨ã®å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
            else: # ANN
                # ANNBaselineModelã¾ãŸã¯SimpleCNNã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
                if task.__class__.__name__ == "CIFAR10Task":
                    model = task.build_model('ANN', vocab_size=10) # SimpleCNN
                else:
                    model = task.build_model('ANN', vocab_size=vocab_size) # ANNBaselineModel
            
            # state_dictã®ãƒ­ãƒ¼ãƒ‰
            checkpoint = torch.load(model_path, map_location=device)
            state_dict = checkpoint.get('model_state_dict', checkpoint)
            if list(state_dict.keys())[0].startswith('module.'):
                 state_dict = {k[7:]: v for k, v in state_dict.items()}
            
            model.load_state_dict(state_dict, strict=False)
            model.to(device)
            print(f"âœ… è©•ä¾¡ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ '{model_path}' ã‹ã‚‰æ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚")
        except Exception as e:
            print(f"âŒ è©•ä¾¡ç”¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise
    else:
        # å¾“æ¥ã®è¨“ç·´ãƒ¢ãƒ¼ãƒ‰
        print("\n" + "="*20 + f" ğŸš€ Starting Experiment for: {model_type} on {task.__class__.__name__} " + "="*20)
        model = task.build_model(model_type, vocab_size=vocab_size).to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = AdamW(model.parameters(), lr=learning_rate)
        
        for epoch in range(epochs):
            model.train()
            train_progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [{model_type} Training]")
            for batch in train_progress:
                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}
                labels = batch['labels'].to(device)

                optimizer.zero_grad()
                outputs = model(**inputs)
                logits = outputs[0] if isinstance(outputs, tuple) else outputs
                loss = criterion(logits, labels)
                loss.backward()
                optimizer.step()
                train_progress.set_postfix({"loss": f"{loss.item():.4f}"})
    # --- â–² ä¿®æ­£ â–² ---
            
    print(f"\n--- Evaluating {model_type} model ---")
    start_time = time.time()
    metrics = task.evaluate(model, val_loader)
    duration = time.time() - start_time
    
    metrics["model"] = model_type
    metrics["eval_time_sec"] = duration
    
    print(f"  - Results: {metrics}")
    return metrics

# --- â–¼ ä¿®æ­£: å®Ÿè¡Œé–¢æ•°ãŒargså…¨ä½“ã‚’å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´ â–¼ ---
def run_experiment_by_name(experiment_name: str, args: argparse.Namespace) -> pd.DataFrame:
    """å®Ÿé¨“åã«åŸºã¥ã„ã¦é©åˆ‡ãªæ¯”è¼ƒå®Ÿé¨“ã‚’å®Ÿè¡Œã™ã‚‹ã€‚"""
    device = get_auto_device()
    TaskClass = TASK_REGISTRY.get(experiment_name.split('_')[0]) #ä¾‹: "cifar10"
    if not TaskClass:
        raise ValueError(f"Task for experiment '{experiment_name}' not found.")

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token
    if tokenizer.sep_token is None: tokenizer.sep_token = tokenizer.eos_token
    
    task = TaskClass(tokenizer=tokenizer, device=device, hardware_profile={})
    
    train_dataset, val_dataset = task.prepare_data(data_dir="data")
    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=task.get_collate_fn(), shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, collate_fn=task.get_collate_fn())

    vocab_size = tokenizer.vocab_size
    if experiment_name == "cifar10_comparison":
        vocab_size = 10 # CIFAR10ã®ã‚¯ãƒ©ã‚¹æ•°

    results = []
    
    # --- â–¼ ä¿®æ­£: eval_onlyãƒ­ã‚¸ãƒƒã‚¯ã‚’åæ˜  â–¼ ---
    # SNNãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã¨ã‚³ãƒ³ãƒ•ã‚£ã‚°
    snn_model_path = args.model_path if args.model_type == 'SNN' else None
    snn_model_config = args.model_config if args.model_type == 'SNN' else None
    
    # ANNãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã¨ã‚³ãƒ³ãƒ•ã‚£ã‚°
    ann_model_path = args.model_path if args.model_type == 'ANN' else None
    # ANNã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ã¯SNNã¨åŒã˜ã‚‚ã®ã‚’ä½¿ã†ã‹ã€åˆ¥é€”æŒ‡å®šãŒå¿…è¦ (ã“ã“ã§ã¯SNNç”¨ã‚’æµç”¨)
    ann_model_config = args.model_config if args.model_type == 'ANN' else None 
    
    # --model_type ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãã®ã‚¿ã‚¤ãƒ—ã®ã¿ã‚’è©•ä¾¡
    if args.model_type:
        model_path = args.model_path
        model_config = args.model_config
        
        metrics = train_and_evaluate_model(
            args.model_type, task, train_loader, val_loader, device, args.epochs, args.learning_rate, vocab_size,
            eval_only=args.eval_only, model_path=model_path, model_config_path=model_config
        )
        results.append(metrics)
    else:
        # é€šå¸¸ã®æ¯”è¼ƒå®Ÿè¡Œ
        # --- â–¼ ä¿®æ­£ (v5): TypeErrorè§£æ¶ˆã®ãŸã‚ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã‚’æ˜ç¤º â–¼ ---
        ann_metrics = train_and_evaluate_model(
            'ANN', task, train_loader, val_loader, device, args.epochs, args.learning_rate, vocab_size,
            eval_only=args.eval_only, 
            model_path=ann_model_path, 
            model_config_path=ann_model_config
        )
        results.append(ann_metrics)
        snn_metrics = train_and_evaluate_model(
            'SNN', task, train_loader, val_loader, device, args.epochs, args.learning_rate, vocab_size,
            eval_only=args.eval_only, 
            model_path=snn_model_path, 
            model_config_path=snn_model_config
        )
        # --- â–² ä¿®æ­£ (v5) â–² ---
        results.append(snn_metrics)
    # --- â–² ä¿®æ­£ â–² ---
    
    return pd.DataFrame(results)


def save_report(df: pd.DataFrame, output_dir: str, experiment_name: str, args: argparse.Namespace):
    """å®Ÿé¨“çµæœã‚’Markdownå½¢å¼ã§ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã«è¿½è¨˜ã™ã‚‹ã€‚"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    report_path = output_path / f"{experiment_name}_leaderboard.md"

    if 'estimated_energy_j' in df.columns and 'model' in df.columns:
        snn_row = df[df['model'] == 'SNN']
        ann_row = df[df['model'] == 'ANN']
        if not snn_row.empty and not ann_row.empty:
            snn_energy = snn_row['estimated_energy_j'].iloc[0]
            ann_energy = ann_row['estimated_energy_j'].iloc[0]
            if ann_energy > 0 and snn_energy is not None:
                efficiency_gain = (1 - (snn_energy / ann_energy)) * 100
                df['efficiency_gain_%'] = [f"{efficiency_gain:.2f}%" if m == 'SNN' else '-' for m in df['model']]

    # æ–°ã—ã„å®Ÿè¡Œçµæœã«ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    df['run_date'] = time.strftime('%Y-%m-%d %H:%M:%S')
    df['tag'] = args.tag or 'default'
    # --- â–¼ ä¿®æ­£: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’è¿½è¨˜ â–¼ ---
    if args.eval_only:
         df['mode'] = f"EvalOnly ({Path(args.model_path).name if args.model_path else 'N/A'})" # model_pathãŒNoneã®å ´åˆã‚’å‡¦ç†
    else:
         df['mode'] = "Train+Eval"
    # --- â–² ä¿®æ­£ â–² ---

    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
    with open(report_path, 'a', encoding='utf-8') as f:
        if f.tell() == 0: # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆã€ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›¸ãè¾¼ã‚€
            f.write(f"# Benchmark Leaderboard: {experiment_name.replace('_', ' ').title()}\n\n")
        f.write(f"## ğŸ“Š Run at: {df['run_date'].iloc[0]} (Tag: {df['tag'].iloc[0]})\n\n")
        
        # --- â–¼ ä¿®æ­£: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã®æƒ…å ±ã‚’è¿½è¨˜ â–¼ ---
        if args.eval_only:
            f.write(f"**Configuration:** Mode: EvalOnly, Model: {args.model_path}, Config: {args.model_config}\n\n")
        else:
            f.write(f"**Configuration:** Mode: Train+Eval, Epochs: {args.epochs}, Batch Size: {args.batch_size}, LR: {args.learning_rate}\n\n")
        # --- â–² ä¿®æ­£ â–² ---
            
        f.write(df.to_markdown(index=False))
        f.write("\n\n---\n\n")

    print(f"\nâœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ '{report_path}' ã«è¿½è¨˜ã—ã¾ã—ãŸã€‚")

def main(args: argparse.Namespace):
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã®ãƒ¡ã‚¤ãƒ³é–¢æ•°ã€‚"""
    if args.experiment == "all":
        # 'all' ã®å ´åˆã€eval_only ã¯ã‚µãƒãƒ¼ãƒˆã—ãªã„ï¼ˆè¤‡é›‘ã«ãªã‚Šã™ãã‚‹ãŸã‚ï¼‰
        if args.eval_only:
            print("Error: --eval_only ã¯ 'all' å®Ÿé¨“ã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
            return
            
        cifar10_results_df = run_experiment_by_name("cifar10_comparison", args)
        save_report(cifar10_results_df, args.output_dir, "cifar10_ann_vs_snn", args)
        
        sst2_results_df = run_experiment_by_name("sst2_comparison", args)
        save_report(sst2_results_df, args.output_dir, "sst2_ann_vs_snn", args)
        
        mrpc_results_df = run_experiment_by_name("mrpc_comparison", args)
        save_report(mrpc_results_df, args.output_dir, "mrpc_ann_vs_snn", args)
        
    elif args.experiment in ["cifar10_comparison", "sst2_comparison", "mrpc_comparison"]:
        results_df = run_experiment_by_name(args.experiment, args)
        report_name = args.experiment.replace('_comparison', '_ann_vs_snn')
        save_report(results_df, args.output_dir, report_name, args)
    else:
        print(f"Unknown experiment: {args.experiment}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="SNN vs ANN Benchmark Suite")
    parser.add_argument("--experiment", type=str, default="all", choices=["all", "cifar10_comparison", "sst2_comparison", "mrpc_comparison"], help="å®Ÿè¡Œã™ã‚‹å®Ÿé¨“ã‚’é¸æŠã—ã¾ã™ã€‚")
    parser.add_argument("--tag", type=str, help="å®Ÿé¨“ã«ã‚«ã‚¹ã‚¿ãƒ ã‚¿ã‚°ã‚’ä»˜ã‘ã¾ã™ã€‚")
    parser.add_argument("--output_dir", type=str, default="benchmarks", help="çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚")
    
    # è¨“ç·´ãƒ¢ãƒ¼ãƒ‰ç”¨
    parser.add_argument("--epochs", type=int, default=3, help="[è¨“ç·´ãƒ¢ãƒ¼ãƒ‰] è¨“ç·´ã®ã‚¨ãƒãƒƒã‚¯æ•°ã€‚")
    parser.add_argument("--batch_size", type=int, default=32, help="[è¨“ç·´ãƒ¢ãƒ¼ãƒ‰] è¨“ç·´ã¨è©•ä¾¡ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã€‚")
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="[è¨“ç·´ãƒ¢ãƒ¼ãƒ‰] ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®å­¦ç¿’ç‡ã€‚")

    # --- â–¼ ä¿®æ­£: è©•ä¾¡å°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã®å¼•æ•°ã‚’è¿½åŠ  â–¼ ---
    parser.add_argument("--eval_only", action="store_true", help="[è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰] è¨“ç·´ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§è©•ä¾¡ã®ã¿ã‚’è¡Œã„ã¾ã™ã€‚")
    parser.add_argument("--model_path", type=str, help="[è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰] è©•ä¾¡ã™ã‚‹å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ (.pth)ã€‚")
    parser.add_argument("--model_config", type=str, help="[è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰] è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (.yaml)ã€‚")
    parser.add_argument("--model_type", type=str, choices=['SNN', 'ANN'], help="[è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰] è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ã‚¤ãƒ— (SNNã¾ãŸã¯ANN)ã€‚")
    # --- â–² ä¿®æ­£ â–² ---
    
    args = parser.parse_args()
    
    # --- â–¼ ä¿®æ­£: è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã®å¼•æ•°ãƒã‚§ãƒƒã‚¯ â–¼ ---
    if args.eval_only and (not args.model_path or not args.model_config):
        print("Error: --eval_only ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€--model_path ã¨ --model_config ã®ä¸¡æ–¹ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
        sys.exit(1)
    if args.eval_only and not args.model_type:
         # model_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€ä¸¡æ–¹ã‚’è©•ä¾¡ã—ã‚ˆã†ã¨ã™ã‚‹ãŸã‚ã€
         # --model_pathã¨--model_configãŒSNNã¨ANNã®ä¸¡æ–¹ã§å¿…è¦ã«ãªã‚‹ã€‚
         # ã“ã“ã§ã¯ã€--eval_onlyæ™‚ã¯--model_typeã‚‚å¿…é ˆã¨ã™ã‚‹ã€‚
         print("Error: --eval_only ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€--model_type ('SNN' ã¾ãŸã¯ 'ANN') ã‚‚æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚")
         sys.exit(1)
    if args.eval_only and args.experiment == "all":
         print("Error: --eval_only ã¯ 'all' å®Ÿé¨“ã§ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
         sys.exit(1)
    # --- â–² ä¿®æ­£ â–² ---

    main(args)
