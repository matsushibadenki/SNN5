# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/architectures/spiking_transformer_v2.py
# Title: Spiking Transformer v2 (SDSAçµ±åˆç‰ˆ)
# Description: Spike-Driven Self-Attention (SDSA) ã‚’çµ„ã¿è¾¼ã‚“ã Spiking Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚
#
# (ä¸­ç•¥)
#
# ã€ä¿®æ­£ v_fix_spike_rate_zeroã€‘:
# - `run_distill_hpo.py` ã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ `neuron_config` å†…ã® `bias` ã‚­ãƒ¼ã‚’
#   `bias_init` ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã€‚
# - ã€v_init ä¿®æ­£ã€‘: `v_init` (åˆæœŸè†œé›»ä½) ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«æ¸¡ã™ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ ã€‚
#
# ã€ä¿®æ­£ v_fix_import_errorã€‘:
# - å­˜åœ¨ã—ãªã„ 'SpikingSelfAttention' ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’å‰Šé™¤ (log6.txt)
#
# ã€ä¿®æ­£ v_fix_type_error (log9.txt)ã€‘:
# - HPO (dependency_injector) çµŒç”±ã§ int å‹å¼•æ•°ãŒ float (ä¾‹: 256.0) ã¨ã—ã¦
#   æ¸¡ã•ã‚Œã‚‹ã“ã¨ãŒåŸå› ã§ TypeError ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€
#   __init__ ã®å†’é ­ã§å…¨ã¦ã®æ•´æ•°å¼•æ•°ã‚’ int() ã§æ˜ç¤ºçš„ã«ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹ã€‚

import torch
import torch.nn as nn
from typing import List, Tuple, Dict, Any, Optional, Union, cast 
import math
import logging 

# å¿…è¦ãªã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.core.base import BaseModel, SNNLayerNorm
# from snn_research.core.neurons.lif_neuron import LIFNeuron # å¤ã„ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„)
from snn_research.core.neurons.adaptive_lif_neuron import AdaptiveLIFNeuron # ã“ã‚ŒãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹

# --- ä¿®æ­£ v_fix_import_error ---
from snn_research.core.attention import SpikeDrivenSelfAttention 
# ---------------------------------

LayerOutput = Dict[str, torch.Tensor]

# ãƒ­ã‚¬ãƒ¼ã®è¨­å®š (v_fix_bias_key_mapping)
logger: logging.Logger = logging.getLogger(__name__)


class SpikingTransformerV2(BaseModel):
    """
    Spike-Driven Self-Attention (SDSA) ã‚’çµ„ã¿è¾¼ã‚“ã Spiking Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚
    ViTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ï¼‰ã¨äº’æ›æ€§ãŒã‚ã‚‹ã€‚
    """
    def __init__(
        self,
        d_model: int,
        n_head: int,
        num_layers: int,
        dim_feedforward: int,
        dropout: float,
        time_steps: int,
        neuron_config: Dict[str, Any],
        sdsa_config: Dict[str, Any],
        # ViTäº’æ›ã®ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        img_size: int = 32,
        patch_size: int = 4,
        in_channels: int = 3,
        num_classes: int = 10,
        # (v_hpo_fix_bias_key_mapping): bias_init ã‚’ç›´æ¥å—ã‘å–ã‚‹ã‚ˆã†ã«å¤‰æ›´
        bias_init: float = 0.0,
        **kwargs: Any
    ) -> None:
        
        # --- ä¿®æ­£ v_fix_type_error (log9.txt) ---
        # HPO (dependency_injector) ãŒ float ã‚’æ¸¡ã™ãŸã‚ã€å…¨ã¦ int ã«ã‚­ãƒ£ã‚¹ãƒˆ
        _d_model = int(d_model)
        _n_head = int(n_head)
        _num_layers = int(num_layers)
        _dim_feedforward = int(dim_feedforward)
        _time_steps = int(time_steps)
        _img_size = int(img_size)
        _patch_size = int(patch_size)
        _in_channels = int(in_channels)
        _num_classes = int(num_classes)
        # ----------------------------------------
        
        # (v_hpo_fix_bias_key_mapping):
        # (ä¸­ç•¥: bias_init ã®ãƒ­ã‚¸ãƒƒã‚¯ã¯å¤‰æ›´ãªã—)
        hpo_bias = kwargs.get('bias', 0.0)
        if hpo_bias == 0.0:
            hpo_bias = kwargs.get('neuron_bias', 0.0)
        if hpo_bias != 0.0:
            logger.info(f"[SpikingTransformerV2] ğŸ§  Overriding bias_init with HPO value: {hpo_bias}")
            bias_init = hpo_bias
        if 'bias_init' not in neuron_config:
            neuron_config['bias_init'] = bias_init
        if 'NEURON_BIAS' in neuron_config:
            neuron_config['bias_init'] = neuron_config['NEURON_BIAS']
        if 'bias' in neuron_config:
            neuron_config['bias_init'] = neuron_config['bias']
        logger.info(f"[SpikingTransformerV2] ğŸ§  Final bias_init for layers: {neuron_config['bias_init']}")
        
        
        super().__init__(time_steps=_time_steps, **kwargs) # _time_steps ã‚’ä½¿ç”¨

        self.d_model = _d_model # _d_model ã‚’ä½¿ç”¨
        self.n_head = _n_head # _n_head ã‚’ä½¿ç”¨
        self.time_steps = _time_steps # _time_steps ã‚’ä½¿ç”¨

        # --- ViT ãƒ‘ãƒƒãƒåŸ‹ã‚è¾¼ã¿ ---
        self.patch_size = _patch_size # _patch_size ã‚’ä½¿ç”¨

        # --- ä¿®æ­£ v_fix_type_error (log9.txt) ---
        # ã‚­ãƒ£ã‚¹ãƒˆæ¸ˆã¿ã®ãƒ­ãƒ¼ã‚«ãƒ«å¤‰æ•°ã‚’ä½¿ç”¨
        num_patches = (_img_size // _patch_size) ** 2
        patch_dim = _in_channels * (_patch_size ** 2)
        
        self.patch_embed = nn.Conv2d(
            _in_channels, _d_model, 
            kernel_size=_patch_size, stride=_patch_size
        )
        
        # ä½ç½®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚° (ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿç®‡æ‰€)
        # num_patches ã‚‚ int() ã§ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ä¸‡å…¨ã‚’æœŸã™
        self.pos_embed = nn.Parameter(torch.randn(1, int(num_patches), _d_model))
        # -------------------------

        self.layers = nn.ModuleList([
            SDSAEncoderLayer(
                d_model=_d_model,
                n_head=_n_head,
                dim_feedforward=_dim_feedforward,
                dropout=dropout,
                time_steps=_time_steps,
                neuron_config=neuron_config,
                sdsa_config=sdsa_config,
                name=f"SDSAEncoderLayer_{i}",
                bias_init=neuron_config['bias_init'] 
            ) for i in range(_num_layers) # _num_layers ã‚’ä½¿ç”¨
        ])

        self.norm = SNNLayerNorm(_d_model, time_steps=_time_steps)
        
        # å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ (åˆ†é¡ãƒ˜ãƒƒãƒ‰)
        self.output_projection = nn.Linear(_d_model, _num_classes) # _num_classes ã‚’ä½¿ç”¨

        self.built = True


    def forward(
        self,
        x: torch.Tensor,
        input_images: Optional[torch.Tensor] = None,
        output_hidden_states: bool = False
    ) -> Union[torch.Tensor, LayerOutput]:
        
        if not self.built:
            raise RuntimeError(f"Layer {self.name} has not been built.")

        # ViTäº’æ›ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰
        # (B, C, H, W) -> (B, N, D)
        x_patched = self.patch_embed(x).flatten(2).transpose(1, 2)
        x = x_patched + self.pos_embed

        outputs_over_time: List[torch.Tensor] = []
        
        # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ (æ¨è«–/å­¦ç¿’æ™‚ã«å¿…è¦)
        for layer_module in self.layers:
             layer = cast(SDSAEncoderLayer, layer_module)
             layer.set_stateful(True)

        current_x = x 
        for t in range(self.time_steps):
            x_step = current_x 
            
            # (v_hpo_fix_oom): å…ƒã®åŸ‹ã‚è¾¼ã¿ `x` ã‚’æ¯ã‚¹ãƒ†ãƒƒãƒ—å…¥åŠ›
            if t == 0:
                 x_step = x
            else:
                 x_step = current_x # t>0 ã¯ã‚¹ãƒ‘ã‚¤ã‚¯å…¥åŠ›ã‚’æƒ³å®š

            for layer_module in self.layers:
                layer = cast(SDSAEncoderLayer, layer_module)
                x_step = layer(x_step) 

            outputs_over_time.append(x_step)
            current_x = x_step # ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã¨ã™ã‚‹

        x_final = torch.stack(outputs_over_time).mean(dim=0)

        # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ (ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã«æˆ»ã™)
        for layer_module in self.layers:
             layer = cast(SDSAEncoderLayer, layer_module)
             layer.set_stateful(False)

        x_final = self.norm(x_final)

        if output_hidden_states:
             output: LayerOutput = {
                'last_hidden_state': x_final,
                'all_hidden_states': torch.stack(outputs_over_time)
             }
        else:
            # åˆ†é¡ã‚¿ã‚¹ã‚¯ã®å ´åˆ (input_images ãŒ None ã§ãªã„)
            if input_images is not None:
                # (B, N, C) -> (B, C) ãƒ—ãƒ¼ãƒªãƒ³ã‚°
                pooled_output = x_final.mean(dim=1) 
                output = self.output_projection(pooled_output) # (B, NumClasses)
            else:
                # Transformerã®æ¨™æº–çš„ãªå‡ºåŠ› (B, N, C) -> (B, N, VocabSize)
                output = self.output_projection(x_final) 

        # ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã‚’åé›†
        total_spikes = self.get_total_spikes()
        avg_spike_rate = total_spikes / (self.get_total_neurons() * self.time_steps)

        return {
            'output': output, # 'output' ã‚­ãƒ¼ã«ãƒ†ãƒ³ã‚½ãƒ«ã‚’æ ¼ç´
            'activity': avg_spike_rate, # 'activity' ã‚­ãƒ¼ã«ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã‚’æ ¼ç´
            'total_spikes': total_spikes,
        }


class SDSAEncoderLayer(nn.Module):
    """
    Spike-Driven Self-Attention (SDSA) ã‚’çµ„ã¿è¾¼ã‚“ã Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã€‚
    """
    def __init__(
        self,
        d_model: int,
        n_head: int,
        dim_feedforward: int,
        dropout: float,
        time_steps: int,
        neuron_config: Dict[str, Any],
        sdsa_config: Dict[str, Any],
        name: str = "SDSAEncoderLayer",
        bias_init: float = 0.0 
    ) -> None:
        super().__init__()
        self.name = name
        
        # --- ä¿®æ­£ v_fix_type_error (log9.txt) ---
        # ã“ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«æ¸¡ã•ã‚Œã‚‹å¼•æ•°ã‚‚ã‚­ãƒ£ã‚¹ãƒˆ
        _d_model = int(d_model)
        _n_head = int(n_head)
        _dim_feedforward = int(dim_feedforward)
        _time_steps = int(time_steps)
        # ----------------------------------------
        
        self.d_model = _d_model
        self.n_head = _n_head
        self.time_steps = _time_steps
        self._is_stateful = True # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ« (æ¨è«–/å­¦ç¿’æ™‚)

        # --- v_init ä¿®æ­£: (å¤‰æ›´ãªã—) ---
        v_init = neuron_config.get('v_init', 0.0)
        
        # (v_hpo_fix_bias_key_mapping): (å¤‰æ›´ãªã—)
        bias = neuron_config.get('NEURON_BIAS', 
               neuron_config.get('bias_init', 
               neuron_config.get('bias', 0.0)))
        if bias == 0.0:
            bias = neuron_config.get('neuron_bias', 0.0)
        if bias != 0.0 or v_init != 0.0:
            logger.info(f"[{self.name}] ğŸ§  Overriding neuron params: bias_init={bias}, v_init={v_init}")
        
        v_threshold_s = neuron_config.get('v_threshold', 1.0)
        decay_s = neuron_config.get('decay', 0.95)
        
        neuron_params = {
            'threshold': v_threshold_s,
            'decay': decay_s,
            'bias_init': bias, 
            'v_init': v_init,  
            **neuron_config
        }

        self.self_attn = SpikeDrivenSelfAttention(
            _d_model, _n_head, dropout=dropout, **sdsa_config
        )

        # (v_fix_attribute_error): (å¤‰æ›´ãªã—)
        self.linear2 = nn.Linear(_dim_feedforward, _d_model)

        self.norm1 = SNNLayerNorm(_d_model, time_steps=_time_steps)
        self.norm2 = SNNLayerNorm(_d_model, time_steps=_time_steps)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        self.linear1 = nn.Linear(_d_model, _dim_feedforward)
        
        # --- v_init ä¿®æ­£: (å¤‰æ›´ãªã—) ---
        self.neuron = AdaptiveLIFNeuron(
            features=_d_model,
            **neuron_params
        )
        self.ffn_neuron1 = AdaptiveLIFNeuron(
            features=_dim_feedforward,
            **neuron_params
        )
        self.ffn_neuron2 = AdaptiveLIFNeuron(
            features=_d_model,
            **neuron_params
        )
        
        self.built = True

    def set_stateful(self, stateful: bool) -> None:
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çŠ¶æ…‹ç®¡ç† (ã‚¹ãƒ†ãƒ¼ãƒˆãƒ•ãƒ«/ã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹) ã‚’åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚
        """
        self._is_stateful = stateful
        # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ
        if not stateful:
            self.neuron.reset_state()
            self.ffn_neuron1.reset_state()
            self.ffn_neuron2.reset_state()
        
        # SNNLayerNorm ã®çŠ¶æ…‹ã‚‚åˆ‡ã‚Šæ›¿ãˆ
        if isinstance(self.norm1, SNNLayerNorm):
            self.norm1.set_stateful(stateful)
        if isinstance(self.norm2, SNNLayerNorm):
            self.norm2.set_stateful(stateful)

    def forward(self, src: torch.Tensor) -> torch.Tensor:
        """
        LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¨SDSAã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã€‚
        å…¥åŠ› `src` ã¯ (B, N, C) ã®ãƒ†ãƒ³ã‚½ãƒ«ã€‚
        """
        if not self.built:
            raise RuntimeError(f"Layer {self.name} has not been built.")

        # 1. SDSA (Spike-Driven Self-Attention)
        x_step, _ = self.self_attn(src) 
        
        # 2. Add & Norm (æ®‹å·®æ¥ç¶š 1)
        src = src + self.dropout1(x_step)
        
        # 3. ç™ºç« (LIF)
        src = self.neuron(src) 
        
        # 4. Norm 1
        src = self.norm1(src)

        # 5. Feedforward (FFN)
        x_step = self.linear1(src)
        x_step = self.dropout2(x_step)
        # 6. ç™ºç« (LIF)
        x_step = self.ffn_neuron1(x_step) 

        # (B, N, C*4) -> (B, N, C)
        x_step = self.linear2(x_step)
        x_step = self.dropout3(x_step)
        # 7. ç™ºç« (LIF)
        x_step = self.ffn_neuron2(x_step)

        # 8. Add & Norm (æ®‹å·®æ¥ç¶š 2)
        src = src + x_step
        
        # 9. Norm 2
        src = self.norm2(src)

        return src
