# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/core/adaptive_neuron_selector.py
# (æ–°è¦ä½œæˆãƒ»mypyä¿®æ­£)
# Title: é©å¿œçš„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚»ãƒ¬ã‚¯ã‚¿ (Adaptive Neuron Selector)
# Description:
# doc/Improvement-Plan.md (æ”¹å–„æ¡ˆ1, Phase 2) ã«åŸºã¥ãã€
# SNNã®å­¦ç¿’ä¸­ã®æŒ¯ã‚‹èˆã„ã‚’ç›£è¦–ã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¿ã‚¤ãƒ—ï¼ˆä¾‹: LIF, BIFï¼‰ã‚’
# å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãŸã‚ã®ãƒ¡ã‚¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã‚’å®Ÿè£…ã—ã¾ã™ã€‚
# ã“ã‚Œã«ã‚ˆã‚Šã€å®‰å®šæ€§ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´é¢ã§ã¯LIFã‚’ã€è¡¨ç¾åŠ›ãŒå¿…è¦ãªå ´é¢ã§ã¯BIFã‚’
# è‡ªå‹•çš„ã«é¸æŠã—ã€å­¦ç¿’ã®å®‰å®šæ€§ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚
# mypy --strict æº–æ‹ ã€‚
# ä¿®æ­£: mypy [name-defined] ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€Anyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚

import torch
import torch.nn as nn
# --- â–¼ ä¿®æ­£ â–¼ ---
from typing import List, Deque, Dict, Tuple, Type, cast, Any
# --- â–² ä¿®æ­£ â–² ---
from collections import deque
import logging

# å¿…è¦ãªãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .neurons import AdaptiveLIFNeuron
from .neurons.bif_neuron import BistableIFNeuron

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)

class AdaptiveNeuronSelector(nn.Module):
    """
    å­¦ç¿’ä¸­ã®æŒ¯ã‚‹èˆã„ï¼ˆæå¤±ã€ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ï¼‰ã‚’ç›£è¦–ã—ã€
    LIFã¨BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ¡ã‚¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã€‚

    ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è‡ªä½“ãŒå­¦ç¿’ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«åŸºã¥ã„ã¦
    ä¸‹ä½å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å·®ã—æ›¿ãˆã¾ã™ã€‚
    """

    def __init__(
        self,
        module_to_wrap: nn.Module,
        layer_name_to_monitor: str,
        lif_params: Dict[str, Any],
        bif_params: Dict[str, Any],
        monitor_window: int = 20,
        loss_plateau_threshold: float = 0.001,
        low_spike_rate_threshold: float = 0.05,
        high_spike_rate_threshold: float = 0.95
    ) -> None:
        """
        Args:
            module_to_wrap (nn.Module): å†…éƒ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹å¯¾è±¡ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (ä¾‹: SpikingCNN)ã€‚
            layer_name_to_monitor (str): module_to_wrapå†…ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®åå‰ (ä¾‹: "neuron2")ã€‚
            lif_params (Dict[str, Any]): LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
            bif_params (Dict[str, Any]): BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
            monitor_window (int): æå¤±ã‚„ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã®ç›£è¦–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã€‚
            loss_plateau_threshold (float): æå¤±ã®åœæ»ã¨ã¿ãªã™æ¨™æº–åå·®ã®é–¾å€¤ã€‚
            low_spike_rate_threshold (float): ä½ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã¨ã¿ãªã™é–¾å€¤ã€‚
            high_spike_rate_threshold (float): é«˜ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã¨ã¿ãªã™é–¾å€¤ã€‚
        """
        super().__init__()
        self.module_to_wrap: nn.Module = module_to_wrap
        self.layer_name_to_monitor: str = layer_name_to_monitor
        self.lif_params: Dict[str, Any] = lif_params
        self.bif_params: Dict[str, Any] = bif_params
        self.monitor_window: int = monitor_window
        self.loss_plateau_threshold: float = loss_plateau_threshold
        self.low_spike_rate_threshold: float = low_spike_rate_threshold
        self.high_spike_rate_threshold: float = high_spike_rate_threshold

        # ç›£è¦–ç”¨ã®å±¥æ­´ãƒãƒƒãƒ•ã‚¡
        self.loss_history: Deque[float] = deque(maxlen=monitor_window)
        self.spike_rate_history: Deque[float] = deque(maxlen=monitor_window)
        
        self.current_neuron_type: Type[nn.Module] = AdaptiveLIFNeuron
        
        # ç›£è¦–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã¸ã®å‚ç…§ã‚’å–å¾—
        try:
            self.monitored_neuron: nn.Module = self._find_layer(layer_name_to_monitor)
            self.current_neuron_type = type(self.monitored_neuron)
            logger.info(f"âœ… AdaptiveNeuronSelectorãŒå±¤ '{layer_name_to_monitor}' ({self.current_neuron_type.__name__}) ã®ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
        except AttributeError:
            logger.error(f"âŒ '{layer_name_to_monitor}' ãŒ 'module_to_wrap' ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            # å®Ÿè¡Œã‚’ç¶™ç¶šã™ã‚‹ãŸã‚ã«ãƒ€ãƒŸãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®š
            self.monitored_neuron = nn.Identity() 
            self.current_neuron_type = nn.Identity

    def _find_layer(self, layer_name: str) -> nn.Module:
        """æŒ‡å®šã•ã‚ŒãŸåå‰ã®ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
        # "conv2.neuron" ã®ã‚ˆã†ãªãƒã‚¹ãƒˆã—ãŸåå‰ã‚’è§£æ±º
        current_module: nn.Module = self.module_to_wrap
        for name in layer_name.split('.'):
            if not hasattr(current_module, name):
                raise AttributeError(f"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« '{type(current_module).__name__}' ã«å±æ€§ '{name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            current_module = getattr(current_module, name)
        return current_module

    def _replace_neuron_layer(self, target_class: Type[nn.Module], params: Dict[str, Any]) -> None:
        """ç›£è¦–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ç½®ãæ›ãˆã‚‹"""
        if self.current_neuron_type == target_class:
            logger.debug(f"å±¤ '{self.layer_name_to_monitor}' ã¯æ—¢ã« '{target_class.__name__}' ã§ã™ã€‚")
            return

        try:
            # å…ƒã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ç‰¹å¾´é‡æ•°ã‚’å–å¾—
            original_features: int = 0
            if hasattr(self.monitored_neuron, 'features'):
                original_features = cast(int, getattr(self.monitored_neuron, 'features'))
            elif hasattr(self.monitored_neuron, 'n_neurons'): # BioLIFNeuronã®å ´åˆ
                original_features = cast(int, getattr(self.monitored_neuron, 'n_neurons'))
            
            if original_features == 0:
                logger.warning(f"å±¤ '{self.layer_name_to_monitor}' ã®ç‰¹å¾´é‡æ•°ãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ç½®ãæ›ãˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return

            # æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            # paramsã‹ã‚‰featuresã‚’å‰Šé™¤ (AdaptiveLIFNeuron/BistableIFNeuronã®å¼•æ•°åãŒç•°ãªã‚‹ãŸã‚)
            params_no_features: Dict[str, Any] = params.copy()
            params_no_features.pop('features', None)
            
            # 'features'å¼•æ•°ã¯ä¸¡æ–¹ã®ã‚¯ãƒ©ã‚¹ã§å¿…é ˆã¨ä»®å®š
            new_neuron: nn.Module = target_class(features=original_features, **params_no_features)
            
            # ãƒ‡ãƒã‚¤ã‚¹ã‚’åˆã‚ã›ã‚‹
            original_device: torch.device = next(self.monitored_neuron.parameters(), torch.tensor(0)).device
            new_neuron.to(original_device)

            # è¦ªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—ã—ã¦å±æ€§ã‚’ç½®ãæ›ãˆ
            parent_module: nn.Module = self.module_to_wrap
            layer_name_parts: List[str] = self.layer_name_to_monitor.split('.')
            if len(layer_name_parts) > 1:
                # ãƒã‚¹ãƒˆã—ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å ´åˆã€è¦ªã‚’å–å¾—
                for name in layer_name_parts[:-1]:
                    parent_module = getattr(parent_module, name)
            
            final_layer_name: str = layer_name_parts[-1]
            setattr(parent_module, final_layer_name, new_neuron)
            
            # å‚ç…§ã¨ã‚¿ã‚¤ãƒ—ã‚’æ›´æ–°
            self.monitored_neuron = new_neuron
            self.current_neuron_type = target_class
            logger.info(f"ğŸ§¬ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é€²åŒ–: å±¤ '{self.layer_name_to_monitor}' ãŒ '{target_class.__name__}' ã«åˆ‡ã‚Šæ›¿ã‚ã‚Šã¾ã—ãŸã€‚")

        except Exception as e:
            logger.error(f"ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®ç½®ãæ›ãˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)


    def step(self, current_loss: float) -> Tuple[bool, str]:
        """
        å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å‘¼ã³å‡ºã•ã‚Œã€çµ±è¨ˆã‚’æ›´æ–°ã—ã€åˆ‡ã‚Šæ›¿ãˆã‚’åˆ¤æ–­ã™ã‚‹ã€‚

        Args:
            current_loss (float): ç¾åœ¨ã®ãƒãƒƒãƒã®æå¤±ã€‚

        Returns:
            Tuple[bool, str]: (åˆ‡ã‚Šæ›¿ãˆãŒç™ºç”Ÿã—ãŸã‹, ç†ç”±)
        """
        # 1. çµ±è¨ˆã®åé›†
        self.loss_history.append(current_loss)
        
        spike_rate: float = 0.0
        if hasattr(self.monitored_neuron, 'spikes'): # AdaptiveLIFNeuron / BistableIFNeuron
            spikes_tensor: torch.Tensor = getattr(self.monitored_neuron, 'spikes')
            if spikes_tensor is not None and spikes_tensor.numel() > 0:
                spike_rate = spikes_tensor.mean().item()
        self.spike_rate_history.append(spike_rate)

        # å±¥æ­´ãŒæºœã¾ã‚‹ã¾ã§å¾…æ©Ÿ
        if len(self.loss_history) < self.monitor_window:
            return False, "Initializing history"

        # 2. çŠ¶æ…‹ã®è¨ºæ–­ (Improvement-Plan.mdã®ãƒ­ã‚¸ãƒƒã‚¯)
        avg_spike_rate: float = float(torch.tensor(list(self.spike_rate_history)).mean().item())
        loss_std_dev: float = float(torch.tensor(list(self.loss_history)).std().item())

        # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        if avg_spike_rate < self.low_spike_rate_threshold:
            # ç—‡çŠ¶: Dead Neuron å•é¡Œ
            # å¯¾ç­–: BIFã®åŒå®‰å®šæ€§ã§æ´»æ€§åŒ–ã‚’è©¦ã¿ã‚‹
            if self.current_neuron_type != BistableIFNeuron:
                self._replace_neuron_layer(BistableIFNeuron, self.bif_params)
                return True, "low_spike_rate: Switched to BIF"
            
        elif avg_spike_rate > self.high_spike_rate_threshold:
            # ç—‡çŠ¶: Over-excitation (éå‰°ç™ºç«)
            # å¯¾ç­–: å®‰å®šã—ãŸLIFã«æˆ»ã™
            if self.current_neuron_type != AdaptiveLIFNeuron:
                self._replace_neuron_layer(AdaptiveLIFNeuron, self.lif_params)
                return True, "high_spike_rate: Switched to LIF"

        elif loss_std_dev > self.loss_plateau_threshold * 10: # é–¾å€¤ã®10å€ä»¥ä¸Š
            # ç—‡çŠ¶: å­¦ç¿’ãŒä¸å®‰å®šãƒ»ç™ºæ•£å‚¾å‘
            # å¯¾ç­–: å®‰å®šã—ãŸLIFã«æˆ»ã™
            if self.current_neuron_type != AdaptiveLIFNeuron:
                self._replace_neuron_layer(AdaptiveLIFNeuron, self.lif_params)
                return True, "loss_diverging: Switched to LIF"

        elif loss_std_dev < self.loss_plateau_threshold:
            # ç—‡çŠ¶: åœæ»
            # å¯¾ç­–: BIFã§è¡¨ç¾åŠ›ã‚’é«˜ã‚ã€åœæ»ã‹ã‚‰ã®è„±å‡ºã‚’è©¦ã¿ã‚‹
            if self.current_neuron_type != BistableIFNeuron:
                self._replace_neuron_layer(BistableIFNeuron, self.bif_params)
                return True, "loss_plateau: Switched to BIF"
        
        return False, "Stable"

    def forward(self, *args: Any, **kwargs: Any) -> Any:
        """
        ã‚»ãƒ¬ã‚¯ã‚¿ã¯ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã—ã€å†…éƒ¨ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã™ã€‚
        """
        # ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è‡ªä½“ã¯è¨ˆç®—ã‚°ãƒ©ãƒ•ã®ä¸€éƒ¨ã§ã¯ãªãã€
        # å¤–éƒ¨ã®ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ãŒ step() ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã€‚
        # ã‚‚ã—ãƒ©ãƒƒãƒ‘ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã•ã›ã‚‹å ´åˆã¯ã€ã“ã“ã§ module_to_wrap ã‚’å‘¼ã³å‡ºã™ã€‚
        return self.module_to_wrap(*args, **kwargs)