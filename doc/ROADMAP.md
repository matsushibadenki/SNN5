# **SNN5 高度化ロードマップ (v10.0): SOTA精度とエネルギー効率の実現**

## **I. 基本方針**

doc/プロジェクト強化案の調査.md（SNN5パラダイム強化に関する包括的技術レポート）に基づき、SNN5プロジェクトをSOTA（State-of-the-Art）レベルのAIフレームワークへと強化する。

プロジェクトの設計思想である「非GPU依存」「高効率」を追求しつつ、ANN（人工ニューラルネットワーク）に匹敵する「高精度」を達成するため、以下の戦略的実装計画を推進する。

1. **学習基盤:** 実証済みの「代理勾配（Surrogate Gradient: SG）法」を標準学習基盤として採用し、エンドツーエンドの直接訓練を可能にする (強化案 1.2)。  
2. **アーキテクチャ:** ResNet, TransformerといったSOTAアーキテクチャをSNNネイティブに実装し、精度を追求する (強化案 2.1, 5.1)。  
3. **効率化:** 精度を維持しつつ、SNN固有の利点（低レイテンシ・低消費電力）を最大化するため、スパース化、量子化、プルーニング、動的推論を徹底的に実装する (強化案 4.1-4.4, 6.1)。  
4. **ベンチマーク:** 評価基盤をSpikingJelly 24 に準拠させ、ニューロモーフィック・データセット（CIFAR10-DVS, SHD）での性能を標準指標とする (強化案 7.1)。

## **II. 実装ロードマップ**

### **フェーズ1: 高精度アーキテクチャの実装 (精度向上)**

SNNの深層化と、Transformerアーキテクチャの導入により、ANN SOTAモデルに匹敵する精度基盤を構築する。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P1.1** | **深層残差SNNの実装** | 勾配消失を防ぎ、100層以上の深層SNNを可能にする残差接続（Residual Connection）を実装する。 | **SEW-ResNet** 9 | 2.1 | 計画中 |
| **P1.2** | **イベント駆動型Transformer (HW Track)** | SEW-ResNetが持つ「非スパイク計算」の問題を解決する、純粋なイベント駆動型Transformerを実装する。 | **Spikingformer** 9 | 5.1 | 計画中 |
| **P1.3** | **ハイブリッドTransformer (GPU Track)** | ANNのAttentionとSNNのFFNを組み合わせた、迅速デプロイ可能なハイブリッドTransformerを実装する。 | **HsVT** 53 | 5.2 | 計画中 |
| **P1.4** | **時空間アテンション** | SNN固有の時間情報を活用するため、スパイクベースの時空間アテンション・モジュールを実装する。 | **DTA-SNN** 10, **STAA-SNN** 11 | 2.2 | 計画中 |

### **フェーズ2: ニューロン・ダイナミクスとコーディングの強化 (効率/精度向上)**

ニューロンモデル自体と入力コーディング方式を高度化し、ネットワークの表現力と効率を向上させる。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P2.1** | **学習可能なニューロン (PLIF)** | 膜時定数($\\tau$)を学習可能なパラメータとし、各ニューロンが最適な時間スケールを自動獲得できるようにする。 | **PLIF** 7 | 3.1 | 計画中 |
| **P2.2** | **学習可能なニューロン (GLIF)** | $\\tau$や閾値を入力依存のゲートで動的に制御する、より高表現力なニューロンモデルを実装する。 | **GLIF** 8 | 3.1 | 計画中 |
| **P2.3** | **時間コーディングの最適化** | レートコーディングから、高速・高効率なTTFS（Time-to-First-Spike）コーディングへ移行する。 | **TTFS** 30, 84 | 3.2 | 計画中 |

### **フェーズ3: 省エネ化（エネルギー効率）の徹底追求**

「高精度」と「高スパース性（低スパイク率）」「低タイムステップ」を両立させ、真の省エネモデルを実現する。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P3.1** | **スパース性正則化** | 損失関数にL1正則化や平均スパイク率ペナルティを導入し、スパースな発火（省エネ）を強制する。 | スパース性正則化 12, 13 | 4.2 | 計画中 |
| **P3.2** | **量子化アウェア学習 (QAT)** | 2ビットや4ビットなどの低ビット精度で学習・推論を行い、メモリと演算コストを劇的に削減する。 | **QUEST** 14 | 4.3 | 計画中 |
| **P3.3** | **ワンショット・プルーニング** | ヘッセ行列を用いて再学習不要（ワンショット）でネットワークを剪定し、開発サイクルを高速化する。 | **SBC** (Spiking Brain Compression) 15 | 4.4 | 計画中 |

### **フェーズ4: デプロイメントと実用化 (レイテンシ/HW対応)**

ロボティクス等の実応用で不可欠な「リアルタイム性」と「ハードウェア互換性」を確保する。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P4.1** | **動的推論 (SNN Cutoff)** | 推論の途中で確信度を監視し、閾値を超えた時点で処理を早期終了させ、レイテンシを動的に削減する。 | **SNN Cutoff** 16, 58 | 6.1 | 計画中 |
| **P4.2** | **HW協調設計 (Lava)** | P1.2 (Spikingformer) や P3 の最適化済みモデルを、Intel Loihi 2 向けのLavaフレームワークへ移植する。 | **Lava Framework** 61 | 6.2 | 計画中 |
| **P4.3** | **HW協調設計 (SpiNNaker)** | P1.2 (Spikingformer) や P3 の最適化済みモデルを、SpiNNaker 向けのsPyNNakerフレームワークへ移植する。 | **sPyNNaker** 69 | 6.2 | 計画中 |

### **フェーズ5: ベンチマークと基盤整備 (開発効率)**

開発基盤をSOTAフレームワークに準拠させ、客観的な評価体制を構築する。

| ID | タスク | 概要 | SOTA技術 | 参照(強化案) | ステータス |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **P5.1** | **開発基盤の移行** | プロジェクトの開発・評価基盤として、SpikingJellyフレームワークを全面的に採用する。 | **SpikingJelly** 24 | 7.1 | 計画中 |
| **P5.2** | **ニューロモーフィック・データセット対応** | 静的データセットに加え、CIFAR10-DVS, SHD, N-Caltech101 等のイベントベースデータセットを標準でサポートする。 | DVS/SHD Datasets 74 | 7.1 | 計画中 |
| **P5.3** | **統合ベンチマークの確立** | 評価指標として「精度」に加え、「SynOps (シナプス演算数)」と「スパイクレート」を必須とし、CI/CDに統合する。 | **SNNBench** 79 | 7.1 | 計画中 |

