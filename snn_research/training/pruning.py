# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/pruning.py
# (æ”¹ä¿®: SBC/OBC é‡ã¿è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯ã®è¿½åŠ  v5)
# Title: æ§‹é€ çš„ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (SBC & Spatio-Temporal)
# Description:
# - doc/SNNé–‹ç™ºï¼šåŸºæœ¬è¨­è¨ˆæ€æƒ³.md (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.4, å¼•ç”¨[15]) ã«åŸºã¥ãã€
#   ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSBCï¼‰ã‚’å®Ÿè£…ã™ã‚‹ã€‚
# - SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
#   æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã®ã‚¹ã‚¿ãƒ–ã‚’è¿½åŠ ã€‚
#
# æ”¹å–„ (SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1 å¯¾å¿œ):
# - _calculate_temporal_redundancy ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…ã‚’ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã«
#   åŸºã¥ãé£½å’Œåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã®ã‚¹ã‚¿ãƒ–ï¼‰ã«æ”¹å–„ã€‚
#
# æ”¹å–„ (v2):
# - SBC (å¼•ç”¨[15]) ã®æ ¸å¿ƒã§ã‚ã‚‹ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®è¨ˆç®—ã¨é‡ã¿è£œæ­£ã®
#   ã€Œãƒ€ãƒŸãƒ¼å®Ÿè£…ã€ã‚’ã€Œè¿‘ä¼¼å®Ÿè£… (Optimal Brain Damage)ã€ã«æ”¹å–„ã€‚
#
# ä¿®æ­£ (v3):
# - mypy [name-defined] ã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€SNNCore ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚
#
# æ”¹å–„ (v4):
# - _compute_hessian_diag ã®ãƒ€ãƒŸãƒ¼å®Ÿè£…ï¼ˆFisherè¿‘ä¼¼ï¼‰ã‚’ã€
#   ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®ã€ŒçœŸã®å¯¾è§’æˆåˆ† (d^2 L / dw^2)ã€ã®è¨ˆç®—ã«ç½®ãæ›ãˆã€‚
#
# æ”¹å–„ (v5):
# - _prune_and_update_weights ã«ã€SBC/OBCã®ã€Œé‡ã¿è£œæ­£ã€ãƒ­ã‚¸ãƒƒã‚¯ã‚’
#   å¯¾è§’è¿‘ä¼¼ã§å®Ÿè£…ã€‚ (ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— P3.3ã€Œå®Ÿè£…ãŒã‚ã¾ã„ã€ç‚¹ã®è§£æ¶ˆ)

import torch
import torch.nn as nn
# --- â–¼ ä¿®æ­£: å¿…è¦ãªå‹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import List, Tuple, Dict, Any, cast, Optional, Type, Iterator
import logging 
# --- â–² ä¿®æ­£ â–² ---
# --- â–¼ ä¿®æ­£: SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ 4.1 å¯¾å¿œ â–¼ ---
from snn_research.core.neurons import AdaptiveLIFNeuron, IzhikevichNeuron
import torch.nn.functional as F
# --- â–² ä¿®æ­£ â–² ---
from snn_research.core.snn_core import SNNCore


logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- â–¼â–¼â–¼ æ”¹å–„ (v4): SBC ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®è§£æ¶ˆ â–¼â–¼â–¼ ---
def _get_model_input_keys(model: nn.Module) -> List[str]:
    """ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚¿ã‚¤ãƒ—ã‹ã‚‰å…¥åŠ›ã‚­ãƒ¼ã‚’æ¨æ¸¬ã™ã‚‹ (ç°¡æ˜“ç‰ˆ)"""
    config_model: Any = None
    if isinstance(model, SNNCore):
        config_model = model.config
    elif hasattr(model, 'config'):
        config_model = model.config # type: ignore[attr-defined]

    if config_model is not None and hasattr(config_model, 'architecture_type'):
        arch_type = config_model.architecture_type
        if arch_type in ["spiking_cnn", "sew_resnet", "hybrid_cnn_snn"]:
            return ["input_images"]
        if arch_type == "tskips_snn":
            return ["input_sequence"]
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ (Transformer, SSM, RWKV ãªã©)
    return ["input_ids"]

def _compute_hessian_diag(
    model: nn.Module, 
    loss_fn: nn.Module, 
    dataloader: Any,
    max_samples: int = 64 # ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®è¨ˆç®—ã«ä½¿ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor]]:
    """
    (æ”¹å–„ v4) ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®å¯¾è§’æˆåˆ† (H_ii = d^2 L / d w_i^2) ã¨ã€
    ä¸€æ¬¡å‹¾é… (dL/dw) ã‚’è¿‘ä¼¼è¨ˆç®—ã™ã‚‹ã€‚
    SBC (å¼•ç”¨[15]) ã«åŸºã¥ãã€‚
    """
    logger.info("Computing Hessian matrix diagonal (True Diag Approx.)...")
    
    # 1. è¨ˆç®—å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (é‡ã¿) ã‚’ç‰¹å®š
    params_to_compute: List[nn.Parameter] = []
    param_names: List[str] = []
    for name, param in model.named_parameters():
        if "weight" in name and param.requires_grad and param.dim() > 1:
            params_to_compute.append(param)
            param_names.append(name)
            
    if not params_to_compute:
        logger.warning("No parameters found for Hessian computation.")
        return {}, {}

    # 2. æå¤±ã®å‹¾é… (dL/dw) ã‚’è¨ˆç®— (autograd.grad ã‚’ä½¿ã†ãŸã‚)
    input_keys: List[str] = _get_model_input_keys(model)
    
    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å°‘æ•°ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—
    data_iterator: Iterator = iter(dataloader)
    
    # å¹³å‡å‹¾é… (dL/dw) ã‚’è“„ç©
    grad_avg: Dict[str, torch.Tensor] = {
        name: torch.zeros_like(param, device=param.device) 
        for name, param in zip(param_names, params_to_compute)
    }
    # å¹³å‡ãƒ˜ãƒƒã‚»è¡Œåˆ—å¯¾è§’ (d^2 L / dw^2) ã‚’è“„ç©
    hessian_diag_avg: Dict[str, torch.Tensor] = {
        name: torch.zeros_like(param, device=param.device) 
        for name, param in zip(param_names, params_to_compute)
    }
    samples_processed: int = 0
    device: torch.device = next(model.parameters()).device

    while samples_processed < max_samples:
        try:
            batch: Any = next(data_iterator)
            
            if not isinstance(batch, dict) or "labels" not in batch:
                logger.warning("Skipping batch: Invalid data format for Hessian computation.")
                continue
                
            labels: torch.Tensor = batch["labels"].to(device)
            inputs: Dict[str, torch.Tensor] = {
                k: v.to(device) for k, v in batch.items() if k in input_keys
            }

            if not inputs:
                logger.warning("Skipping batch: No valid input keys found.")
                continue

            current_batch_size: int = labels.shape[0]
            
            for i in range(current_batch_size):
                if samples_processed >= max_samples:
                    break
                
                sample_inputs: Dict[str, torch.Tensor] = {
                    k: v[i].unsqueeze(0) for k, v in inputs.items()
                }
                sample_label: torch.Tensor = labels[i].unsqueeze(0)

                # --- æå¤± L ã‚’è¨ˆç®— ---
                model.zero_grad()
                outputs: Tuple[torch.Tensor, ...] = model(**sample_inputs)
                logits: torch.Tensor = outputs[0] if isinstance(outputs, tuple) else outputs
                
                loss: torch.Tensor
                if logits.dim() == 3:
                    loss = loss_fn(logits.view(-1, logits.size(-1)), sample_label.view(-1))
                else:
                    loss = loss_fn(logits, sample_label)

                # --- 1æ¬¡å‹¾é… (dL/dw) ã‚’è¨ˆç®— ---
                first_grads: Tuple[Optional[torch.Tensor], ...] = torch.autograd.grad( # type: ignore[assignment]
                    loss, params_to_compute, create_graph=True
                )
                
                # --- 2æ¬¡å‹¾é… (H_ii) ã‚’è¨ˆç®— ---
                # (H_ii = d/dw_i (dL/dw_i))
                
                for j, (name, param) in enumerate(zip(param_names, params_to_compute)):
                    g_i: Optional[torch.Tensor] = first_grads[j]
                    if g_i is None:
                        continue
                        
                    # 1æ¬¡å‹¾é…ã®å¹³å‡ã‚’è“„ç©
                    grad_avg[name] += g_i.detach()
                    
                    # 2æ¬¡å‹¾é…ï¼ˆå¯¾è§’æˆåˆ†ï¼‰ã‚’è¨ˆç®—
                    # (dL/dw_i) * 1.0 ã®å‹¾é…ã‚’å†åº¦ w_i ã«ã¤ã„ã¦è¨ˆç®—
                    H_ii_unsummed: Optional[torch.Tensor] = torch.autograd.grad( # type: ignore[assignment]
                        g_i, param, grad_outputs=torch.ones_like(g_i), retain_graph=True
                    )[0]
                    
                    if H_ii_unsummed is not None:
                        # (dL/dw_i)^2 ã§ã¯ãªãã€(d^2 L / dw_i^2)
                        hessian_diag_avg[name] += H_ii_unsummed.detach()
                    else:
                        # Fisherè¿‘ä¼¼ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        hessian_diag_avg[name] += (g_i.detach() ** 2)

                samples_processed += 1
                
        except StopIteration:
            break
        except Exception as e:
            logger.error(f"Error during Hessian computation: {e}", exc_info=True)
            break

    if samples_processed == 0:
        logger.error("Hessian computation failed: No samples processed.")
        return {}, {}

    # ã‚µãƒ³ãƒ—ãƒ«æ•°ã§å¹³å‡
    for name in hessian_diag_avg:
        grad_avg[name] /= samples_processed
        hessian_diag_avg[name] /= samples_processed
        # å®‰å®šåŒ– (H_ii ãŒè² ã«ãªã‚‹ã“ã¨ã‚‚è¨±å®¹ã™ã‚‹ãŒã€ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹)
        hessian_diag_avg[name] = hessian_diag_avg[name].abs() + 1e-8

    logger.info(f"Hessian diagonal (True Diag) computed for {len(hessian_diag_avg)} layers (using {samples_processed} samples).")
    return grad_avg, hessian_diag_avg

# --- â–²â–²â–² æ”¹å–„ (v4): SBC ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®è§£æ¶ˆ â–²â–²â–² ---

def _compute_saliency(
    param: torch.Tensor, 
    hessian_diag: torch.Tensor
) -> torch.Tensor:
    """
    (æ”¹å–„ v4) Optimal Brain Damage (OBD) ã«åŸºã¥ã Saliency ã‚’è¨ˆç®—ã€‚
    Saliency = (1/2) * (H_ii) * (w^2)
    """
    return 0.5 * hessian_diag * (param.data ** 2)

@torch.no_grad()
# --- â–¼â–¼â–¼ æ”¹å–„ (v5): SBC é‡ã¿è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£… â–¼â–¼â–¼ ---
def _prune_and_update_weights(
    module: nn.Module,
    param_name: str,
    saliency: torch.Tensor,
    grad: torch.Tensor,       # 1æ¬¡å‹¾é… (dL/dw)
    hessian_diag: torch.Tensor, # ãƒ˜ãƒƒã‚»è¡Œåˆ—å¯¾è§’ (H_ii)
    amount: float
) -> Tuple[int, int]:
    """
    (æ”¹å–„ v5) Saliency (OBD) ã«åŸºã¥ããƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€
    å¯¾è§’è¿‘ä¼¼ (Diagonal OBC) ã«åŸºã¥ãé‡ã¿è£œæ­£ã‚’è¡Œã†ã€‚
    """
    param: torch.Tensor = getattr(module, param_name)
    
    # 1. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã‚’æ±ºå®š
    num_to_prune = int(param.numel() * amount)
    if num_to_prune == 0:
        return 0, param.numel()
        
    # Saliency ãŒ *æœ€å°* ã®ã‚‚ã®ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã¨ã™ã‚‹
    threshold = torch.kthvalue(saliency.view(-1), k=num_to_prune).values
    
    # Saliency > threshold ã®é‡ã¿ã‚’ *æ®‹ã™* (ãƒã‚¹ã‚¯)
    mask_keep = saliency > threshold
    mask_prune = ~mask_keep

    # --- æ”¹å–„ v5: é‡ã¿è£œæ­£ (Weight Update) ã®å®Ÿè£… ---
    # ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—[15] (SBC/OBC) ã«åŸºã¥ãã€‚
    # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (w_q -> 0) ã«ã‚ˆã‚‹æå¤±ã®å¢—åŠ ã‚’ã€
    # æ®‹ã‚‹é‡ã¿ (w_p) ã®å¤‰æ›´ (delta_w_p) ã§è£œå„Ÿã™ã‚‹ã€‚
    #
    # å¯¾è§’è¿‘ä¼¼ (H_ij = 0 if i!=j) ã®ä¸‹ã§ã® Optimal Brain Cutting (OBC) ã¯ã€
    # å„é‡ã¿ã‚’ç‹¬ç«‹ã«æ‰±ã„ã¾ã™ã€‚
    #
    # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã‚‹é‡ã¿ (w_q) ãŒã€ã‚‚ã— 0 ã§ã¯ãªã
    # åˆ¥ã®å€¤ w_q' = - g_q / H_qq ã«è¨­å®šã•ã‚Œã‚‹ãªã‚‰ã°ã€
    # æå¤±é–¢æ•° L(w) ã®2æ¬¡ãƒ†ã‚¤ãƒ©ãƒ¼å±•é–‹ L(w_q) ~= L(0) + g_q*w_q + 0.5*H_qq*w_q^2
    # ã¯ w_q = -g_q / H_qq ã§æœ€å°å€¤ã‚’å–ã‚‹ (dL/dw_q = g_q + H_qq*w_q = 0)ã€‚
    #
    # ã¤ã¾ã‚Šã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®é‡ã¿ã‚’ 0 ã«ã™ã‚‹ä»£ã‚ã‚Šã«ã€
    # ã“ã®ã€Œæœ€é©è£œæ­£å€¤ã€ã«è¨­å®šã™ã‚‹ã“ã¨ã§ã€æå¤±ã®å¢—åŠ ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
    
    # è£œæ­£é …: delta_w_q = - g_q / H_qq
    # (H_ii ãŒ 0 ã«è¿‘ã„ã¨ç™ºæ•£ã™ã‚‹ãŸã‚ã€å®‰å®šåŒ–æ¸ˆã¿ã® hessian_diag ã‚’ä½¿ç”¨)
    correction_term: torch.Tensor = - (grad / hessian_diag)
    
    # 3. é‡ã¿è£œæ­£ã®é©ç”¨
    # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ (mask_prune) ã®é‡ã¿ã‚’ 0 ã§ã¯ãªãã€è£œæ­£å€¤ (correction_term) ã«è¨­å®š
    # ãŸã ã—ã€å…ƒã®é‡ã¿ã«åŠ ç®—ã™ã‚‹ã®ã§ã¯ãªãã€ã“ã®å€¤ã«ã€Œè¨­å®šã€ã™ã‚‹
    # (w_q + delta_w_q ã§ã¯ãªãã€w_q' = delta_w_q ã¨ã™ã‚‹)
    #
    # è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯:
    #   w_new = w_old (if mask_keep)
    #   w_new = w_old + delta_w (if mask_prune) 
    #   -> ã“ã“ã§ delta_w ã¯ w_q' = 0 ã«ã™ã‚‹ãŸã‚ã® delta_w = -w_q ã§ã¯ãªãã€
    #      w_q' = w_q_optimal ã«ã™ã‚‹ãŸã‚ã® delta_w = w_q_optimal - w_q
    #      w_q_optimal = -g_q / H_qq
    #   -> delta_w = (-g_q / H_qq) - w_q
    
    # è£œæ­£å€¤ã®è¨ˆç®— (w_q ã‚’ 0 ã«ã™ã‚‹ã®ã§ã¯ãªãã€-g_q/H_qq ã«ã™ã‚‹)
    delta_w: torch.Tensor = correction_term - param.data
    
    # ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®é‡ã¿ã«ã®ã¿è£œæ­£ã‚’é©ç”¨
    param.data[mask_prune] += delta_w[mask_prune]
    
    logger.debug(f"  - [SBC] Applied diagonal OBC correction to {mask_prune.sum()} weights.")

    # 4. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒã‚¹ã‚¯ã‚’é©ç”¨)
    # è£œæ­£æ¸ˆã¿ã®é‡ã¿ã«å¯¾ã—ã€æ®‹ã™é‡ã¿ (mask_keep) ã ã‘ã‚’ä¿æŒ
    param.data *= mask_keep.float()
    
    original_count = param.numel()
    pruned_count = original_count - mask_keep.sum().item()
    return int(pruned_count), original_count
# --- â–²â–²â–² æ”¹å–„ (v5): SBC é‡ã¿è£œæ­£ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£… â–²â–²â–² ---

def apply_sbc_pruning(
    model: nn.Module,
    amount: float,
    dataloader_stub: Any, # ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)
    loss_fn_stub: nn.Module # æå¤±é–¢æ•° (ã‚¹ã‚¿ãƒ–)
) -> nn.Module:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«ã€SBC (Spiking Brain Compression) ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒ»ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã€‚
    (æ”¹å–„ v5: OBD (Saliencyè¨ˆç®—) + OBC (é‡ã¿è£œæ­£) ç›¸å½“)

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã€‚
        amount (float): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹é‡ã¿ã®å‰²åˆ (0.0ã‹ã‚‰1.0ã®é–“)ã€‚
        dataloader_stub (Any): ãƒ˜ãƒƒã‚»è¡Œåˆ—è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã€‚
        loss_fn_stub (nn.Module): æå¤±é–¢æ•°ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    if not (0.0 < amount < 1.0):
        logger.warning(f"ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°é‡ãŒç„¡åŠ¹ã§ã™ ({amount})ã€‚0.0ã‹ã‚‰1.0ã®é–“ã®å€¤ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return model

    logger.info(f"--- ğŸ§  Spiking Brain Compression (SBC/OBC) é–‹å§‹ (Amount: {amount:.1%}) ---")

    # 1. ãƒ˜ãƒƒã‚»è¡Œåˆ—ï¼ˆå¯¾è§’æˆåˆ†ï¼‰ã¨ä¸€æ¬¡å‹¾é…ã‚’è¨ˆç®— (æ”¹å–„ v4)
    grads_avg, hessian_diagonals = _compute_hessian_diag(
        model, loss_fn_stub, dataloader_stub
    )
    
    if not hessian_diagonals:
        logger.error("--- âŒ SBC å¤±æ•—: ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®è¨ˆç®—ã«å¤±æ•—ã—ã¾ã—ãŸ ---")
        return model
    
    total_pruned = 0
    total_params = 0

    # 2. å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ
    target_modules: List[Tuple[nn.Module, str]] = []
    
    # (SNNCoreãƒ©ãƒƒãƒ‘ãƒ¼ã‚’è€ƒæ…®ã—ã€å†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—)
    model_to_prune: nn.Module = model
    if isinstance(model, SNNCore) and hasattr(model, 'model'):
        model_to_prune = model.model
    
    for module in model_to_prune.modules():
        if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
            if hasattr(module, 'weight'): # weight ãŒã‚ã‚‹ã‹ç¢ºèª
                target_modules.append((module, 'weight'))

    if not target_modules:
        logger.warning("ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return model

    logger.info(f"SBCå¯¾è±¡ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {len(target_modules)}")
    
    for module, param_name in target_modules:
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åã‚’å–å¾— (mypyäº’æ›ã®ãŸã‚ãƒ«ãƒ¼ãƒ—ã‚’ä½¿ç”¨)
        full_param_name: str = ""
        for name, mod in model_to_prune.named_modules(): # model_to_prune ã‚’æ¢ç´¢
             if mod is module:
                 full_param_name = f"{name}.{param_name}"
                 break
        
        if not full_param_name:
             logger.warning(f"  - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ (Module: {type(module)})")
             continue

        if full_param_name in hessian_diagonals:
            param: torch.Tensor = getattr(module, param_name)
            hessian_diag = hessian_diagonals[full_param_name]
            grad = grads_avg[full_param_name]
            
            # 3. é‡è¦åº¦ã‚’è¨ˆç®— (OBD)
            saliency = _compute_saliency(param, hessian_diag)
            
            # 4. ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (é‡ã¿è£œæ­£ã‚ã‚Š) (æ”¹å–„ v5)
            pruned, total = _prune_and_update_weights(
                module, param_name, saliency, grad, hessian_diag, amount
            )
            total_pruned += pruned
            total_params += total
            logger.info(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': {pruned}/{total} ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (OBCãƒ™ãƒ¼ã‚¹)ã€‚")
        else:
            logger.warning(f"  - ãƒ¬ã‚¤ãƒ¤ãƒ¼ '{full_param_name}': ãƒ˜ãƒƒã‚»è¡Œåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãšã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"--- âœ… SBC (OBC) å®Œäº† ---")
        logger.info(f"  - åˆè¨ˆãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ç‡: {actual_sparsity:.2%} ({total_pruned} / {total_params})")
    else:
        logger.error("--- âŒ SBC å¤±æ•—: å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ0ã§ã—ãŸ ---")

    return model

# --- â–¼â–¼â–¼ SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆã«åŸºã¥ãè¿½åŠ å®Ÿè£… â–¼â–¼â–¼ ---

@torch.no_grad()
def _calculate_temporal_redundancy(
    model: nn.Module, 
    dataloader: Any, 
    time_steps: int,
    target_layer_names: Optional[List[str]] = None, # ç›£è¦–å¯¾è±¡ã®LIFå±¤ãªã©
    kl_threshold: float = 0.01 # é£½å’Œã¨ã¿ãªã™KLç™ºæ•£ã®é–¾å€¤
) -> Dict[str, int]:
    """
    (æ”¹å–„) SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€‚
    KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã‚’ç›£è¦–ã—ã€æƒ…å ±ãŒé£½å’Œã—ãŸå†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹ã€‚
    
    (æ³¨: å®Ÿéš›ã®KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ã¯è¤‡é›‘ãªãƒ•ãƒƒã‚¯ã¨ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ä¼´ã†ãŸã‚ã€
     ã“ã“ã§ã¯ãã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œçµæœã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹æ”¹å–„ã•ã‚ŒãŸã‚¹ã‚¿ãƒ–ã‚’å®Ÿè£…ã—ã¾ã™)

    Returns:
        Dict[str, int]: ãƒ¬ã‚¤ãƒ¤ãƒ¼åã¨ã€ãã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§å‰Šæ¸›å¯èƒ½ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®è¾æ›¸ã€‚
    """
    logger.info(f"Calculating temporal redundancy (KL divergence method, threshold={kl_threshold})...")
    
    # --- (ãƒ€ãƒŸãƒ¼å®Ÿè£…ã®æ”¹å–„) ---
    # å®Ÿéš›ã«ã¯ã“ã“ã§ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã—ã€ãƒ•ãƒƒã‚¯ã‚’ä½¿ã£ã¦
    # å„ `target_layer_names` ã®ã‚¹ãƒ‘ã‚¤ã‚¯å‡ºåŠ› (T, B, F) ã‚’åé›†ã™ã‚‹ã€‚
    #
    # for t in range(time_steps - 1):
    #   p_t = spike_history[t].mean(dim=(0, 1)) # (F,)
    #   p_t_plus_1 = spike_history[t+1].mean(dim=(0, 1))
    #   # ã‚¼ãƒ­ã‚’é¿ã‘ã‚‹ãŸã‚ã®ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    #   p_t = (p_t + 1e-6) / (1.0 + 1e-6 * F)
    #   p_t_plus_1 = (p_t_plus_1 + 1e-6) / (1.0 + 1e-6 * F)
    #   
    #   kl_div = F.kl_div(p_t_plus_1.log(), p_t, reduction='sum')
    #   
    #   if kl_div < kl_threshold:
    #       redundant_start_step = t + 1
    #       break
    
    # (ã“ã“ã§ã¯ã€ãã®ãƒ­ã‚¸ãƒƒã‚¯ã®ã€Œçµæœã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹)
    
    # KLé–¾å€¤ãŒå°ã•ã„ã»ã©ã€é£½å’Œæ¤œå‡ºãŒå³ã—ããªã‚Šã€å†—é•·ã‚¹ãƒ†ãƒƒãƒ—ã¯å°‘ãªããªã‚‹
    # KLé–¾å€¤ãŒå¤§ãã„ã»ã©ã€é£½å’Œæ¤œå‡ºãŒç·©ããªã‚Šã€å†—é•·ã‚¹ãƒ†ãƒƒãƒ—ã¯å¤šããªã‚‹
    
    # å†—é•·ãªé–‹å§‹ã‚¹ãƒ†ãƒƒãƒ—ã‚’é–¾å€¤ã«åŸºã¥ã„ã¦ç°¡æ˜“çš„ã«è¨ˆç®—
    # (kl_threshold=0.01 -> 0.8), (kl_threshold=0.1 -> 0.6)
    redundancy_start_ratio = min(0.9, max(0.5, 1.0 - kl_threshold * 3.0))
    
    redundant_start_step = int(time_steps * redundancy_start_ratio)
    redundant_steps = time_steps - redundant_start_step
    
    redundancy_report: Dict[str, int] = {}
    
    # ç›£è¦–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’ç‰¹å®š
    if target_layer_names is None:
        target_layer_names = [name for name, mod in model.named_modules() if isinstance(mod, (AdaptiveLIFNeuron, IzhikevichNeuron))]
        if not target_layer_names:
             target_layer_names = [name for name, mod in model.named_modules() if isinstance(mod, (nn.Linear, nn.Conv2d))]

    for name in target_layer_names:
        redundancy_report[name] = redundant_steps

    logger.info(f"Temporal redundancy calculated (KL method stub). Proposing {redundant_steps} steps reduction (from T={redundant_start_step}).")
    return redundancy_report

@torch.no_grad()
def apply_spatio_temporal_pruning(
    model: nn.Module,
    dataloader: Any,
    time_steps: int,
    spatial_amount: float, # ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®å‰²åˆ
    kl_threshold: float = 0.01 # KLé–¾å€¤ã‚’è¿½åŠ 
) -> nn.Module:
    """
    SNN5æ”¹å–„ãƒ¬ãƒãƒ¼ãƒˆ (ã‚»ã‚¯ã‚·ãƒ§ãƒ³4.1, å¼•ç”¨[19]) ã«åŸºã¥ãã€
    æ™‚ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatio-Temporal Pruning) ã‚’é©ç”¨ã™ã‚‹ (ã‚¹ã‚¿ãƒ–)ã€‚

    Args:
        model (nn.Module): ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°å¯¾è±¡ã®SNNãƒ¢ãƒ‡ãƒ«ã€‚
        dataloader (Any): KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹è¨ˆç®—ç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ (ã‚¹ã‚¿ãƒ–)ã€‚
        time_steps (int): å…ƒã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã€‚
        spatial_amount (float): ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆé‡ã¿å‰Šé™¤ï¼‰ã®å‰²åˆã€‚
        kl_threshold (float): æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã®é£½å’Œåˆ¤å®šã«ä½¿ç”¨ã™ã‚‹KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹é–¾å€¤ã€‚

    Returns:
        nn.Module: ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé©ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã€‚
    """
    logger.info(f"--- âš¡ï¸ Spatio-Temporal Pruning é–‹å§‹ (Spatial Amount: {spatial_amount:.1%}, KL Threshold: {kl_threshold}) ---")
    
    # --- 1. æ™‚é–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Temporal Pruning) ---
    # å¼•ç”¨[19]ã«åŸºã¥ãã€å†—é•·ãªã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç‰¹å®šã™ã‚‹
    redundancy_report = _calculate_temporal_redundancy(
        model, dataloader, time_steps, kl_threshold=kl_threshold
    )
    
    avg_redundant_steps: int = 0
    if redundancy_report:
        avg_redundant_steps = int(sum(redundancy_report.values()) / len(redundancy_report))

    new_time_steps = time_steps - avg_redundant_steps
    
    logger.info(f"  [Temporal Pruning (Stub)]: æ¨å®šå‰Šæ¸›å¯èƒ½ã‚¹ãƒ†ãƒƒãƒ—æ•°: {avg_redundant_steps}. (T={time_steps} -> T={new_time_steps})")
    
    # (ã‚¹ã‚¿ãƒ–: å®Ÿéš›ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«å†…ã® time_steps ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã™ã‚‹)
    # (ä¾‹: model.time_steps = new_time_steps)
    if hasattr(model, 'time_steps'):
         logger.info(f"  [Temporal Pruning (Stub)]: Updating model.time_steps to {new_time_steps}")
         # (æ³¨: ã“ã®æ“ä½œã¯ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…ã«å¼·ãä¾å­˜ã™ã‚‹ãŸã‚æ³¨æ„)
         # model.time_steps = new_time_steps
    
    # --- 2. ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚° (Spatial Pruning) ---
    # å¼•ç”¨[19]ã®LAMPSãƒ™ãƒ¼ã‚¹ã€ã¾ãŸã¯å˜ç´”ãªMagnitudeãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°
    logger.info("  [Spatial Pruning (Magnitude Stub)]: é‡ã¿ã®ç©ºé–“ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œä¸­...")
    
    total_pruned = 0
    total_params = 0
    
    for module in model.modules():
        if isinstance(module, (nn.Linear, nn.Conv2d)):
            if hasattr(module, 'weight'): # é‡ã¿ãŒã‚ã‚‹ã‹ç¢ºèª
                param: torch.Tensor = module.weight
                
                num_to_prune = int(param.numel() * spatial_amount)
                if num_to_prune == 0:
                    continue
                
                # å˜ç´”ãª Magnitude Pruning (ã‚¹ã‚¿ãƒ–)
                threshold = torch.kthvalue(param.data.abs().view(-1), k=num_to_prune).values
                mask = param.data.abs() > threshold
                param.data *= mask.float()
                
                pruned_count = param.numel() - mask.sum().item()
                total_pruned += int(pruned_count)
                total_params += param.numel()

    if total_params > 0:
        actual_sparsity = total_pruned / total_params
        logger.info(f"  [Spatial Pruning]: {actual_sparsity:.2%} ({total_pruned} / {total_params}) ã®é‡ã¿ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã—ãŸã€‚")
    
    logger.info("--- âœ… Spatio-Temporal Pruning å®Œäº† (Stub) ---")
    return model
