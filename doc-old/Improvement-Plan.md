# **çµ±åˆæ”¹å–„ææ¡ˆ**

## **æ”¹å–„æ¡ˆ1ï¼šç·åˆçš„æ”¹å–„ææ¡ˆ**

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç¾çŠ¶åˆ†æã¨æœ€æ–°ã®SNNç ”ç©¶å‹•å‘ã‚’è¸ã¾ãˆã€ä»¥ä¸‹ã®æ”¹å–„ææ¡ˆã‚’è¡Œã„ã¾ã™ã€‚

### **å„ªå…ˆåº¦HIGHï¼šå³åº§ã«å®Ÿè£…ã™ã¹ãæ”¹å–„**

#### **1\. å®Ÿè£…ã®å®Ÿè¨¼æ€§ã¨å†ç¾æ€§ã®å‘ä¸Š**

**å•é¡Œç‚¹:**

* READMEã¯å£®å¤§ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚’èªã£ã¦ã„ã‚‹ãŒã€å®Ÿéš›ã«ã‚³ãƒ¼ãƒ‰ãŒå‹•ä½œã™ã‚‹ã‹ã©ã†ã‹ã®è¨¼æ‹ ãŒä¸è¶³  
* ã€Œäººå·¥è„³ã€ã€Œãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã€ã¨ã„ã†æ¦‚å¿µçš„ãªè¡¨ç¾ãŒå¤šãã€å…·ä½“çš„ãªå®Ÿè£…ãŒè¦‹ãˆã«ãã„

**æ”¹å–„ç­–:**

**A) å‹•ä½œå®Ÿç¸¾ã®æ˜ç¤º**

##### **âœ… æ¤œè¨¼æ¸ˆã¿æ©Ÿèƒ½**

###### **å®Ÿè£…å®Œäº†ãƒ»å‹•ä½œç¢ºèªæ¸ˆã¿**

* \[x\] SpikingCNN (CIFAR-10ã§ç²¾åº¦XX%)  
* \[x\] åŸºæœ¬çš„ãªSTDPå­¦ç¿’  
* \[x\] ANN-SNNå¤‰æ› (BatchNorm Foldingå®Ÿè£…æ¸ˆã¿)

###### **å®Ÿè£…ä¸­ãƒ»å®Ÿé¨“æ®µéš**

* \[ \] Spiking Transformer (ç²¾åº¦æ”¹å–„ä¸­)  
* \[ \] Spiking Mamba (ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆä¸­)  
* \[ \] å®Œå…¨ãªè‡ªå·±é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ  (ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—æ®µéš)

###### **æœªå®Ÿè£…ãƒ»è¨ˆç”»ä¸­**

* \[ \] çµ±åˆã•ã‚ŒãŸäººå·¥è„³ã‚·ã‚¹ãƒ†ãƒ   
* \[ \] Webå­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ

**B) ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®å…¬é–‹**

æœ€æ–°ã®SNNç ”ç©¶ã§ã¯ImageNet-1Kã§77-79%ã®ç²¾åº¦ã‚’é”æˆã—ã¦ãŠã‚Šã€ç‰©ä½“æ¤œå‡ºã§ã¯MS-COCOã§mAP 47.6%ãŒå ±å‘Šã•ã‚Œã¦ã„ã¾ã™ PLOSFrontiersã€‚ã‚ãªãŸã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿç¸¾ã‚’ç¤ºã™ã“ã¨ãŒé‡è¦ã§ã™ã€‚

##### **ğŸ“Š ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ**

| ãƒ¢ãƒ‡ãƒ« | ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ | ç²¾åº¦ | æ¨è«–æ™‚é–“ | ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ |
| :---- | :---- | :---- | :---- | :---- |
| SpikingCNN | CIFAR-10 | 89.2% | 12ms | 2.3mJ |
| ANN Baseline | CIFAR-10 | 91.5% | 8ms | 45mJ |
| SpikingTransformer | SST-2 | å®Ÿé¨“ä¸­ | \- | \- |

#### **2\. æœ€æ–°ã®SNNæŠ€è¡“ã®çµ±åˆ**

2024-2025å¹´ã®ç ”ç©¶ã§ã¯ã€Spike-Driven Self-Attention (SDSA)ãŒå¾“æ¥ã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã¨æ¯”è¼ƒã—ã¦87.2å€ä½ã„è¨ˆç®—ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å®Ÿç¾ã—ã€ImageNetã§77.1%ã®ç²¾åº¦ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

**å®Ÿè£…ã™ã¹ãæœ€æ–°æŠ€è¡“:**

**A) Bistable Integrate-and-Fire (BIF) ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«**

BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ç‰©ä½“æ¤œå‡ºã‚¿ã‚¹ã‚¯ã§36-61msã®æ¤œå‡ºæ™‚é–“ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

\# snn\_research/core/neurons/bif\_neuron.py  
class BistableIFNeuron(nn.Module):  
    """  
    åŒå®‰å®šç©åˆ†ç™ºç«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³  
    å¾“æ¥ã®LIFã‚ˆã‚Šé«˜é€Ÿãªæ™‚é–“åæŸã‚’å®Ÿç¾  
    """  
    def \_\_init\_\_(self, threshold\_high=1.0, threshold\_low=-0.5):  
        super().\_\_init\_\_()  
        self.v\_th\_high \= threshold\_high  
        self.v\_th\_low \= threshold\_low

    def forward(self, x, membrane\_potential):  
        \# BIFã®åŒå®‰å®šãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹å®Ÿè£…  
        spike \= (membrane\_potential \>= self.v\_th\_high).float()  
        reset\_mask \= (membrane\_potential \<= self.v\_th\_low).float()  
        \# ... å®Ÿè£…è©³ç´°

**B) Spike-Driven Self-Attention (SDSA)**

\# snn\_research/architectures/spiking\_transformer\_v2.py  
class SpikeDrivenSelfAttention(nn.Module):  
    """  
    ä¹—ç®—ãªã—ã®ãƒã‚¹ã‚¯ã¨åŠ ç®—ã®ã¿ã§å‹•ä½œã™ã‚‹è¶…ä½ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³  
    è¨ˆç®—è¤‡é›‘åº¦: O(N) (å¾“æ¥ã®O(N^2)ã‹ã‚‰æ”¹å–„)  
    """  
    def forward(self, spike\_input):  
        \# ãƒã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ã®ç·šå½¢è¤‡é›‘åº¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³  
        \# ä¹—ç®—ã‚’ä½¿ã‚ãªã„ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ™ãƒ¼ã‚¹å®Ÿè£…  
        pass

#### **3\. æ˜ç¢ºãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ®µéšã®è¨­å®š**

**ææ¡ˆã™ã‚‹æ§‹é€ :**

##### **ğŸ—ºï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**

###### **Phase 1: åŸºç›¤æ§‹ç¯‰ (ç¾åœ¨) âœ…**

* åŸºæœ¬çš„ãªSNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å®Ÿè£…  
* ANN-SNNå¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³  
* ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒæ•´å‚™

###### **Phase 2: æ€§èƒ½æœ€é©åŒ– (é€²è¡Œä¸­) ğŸ”„**

* æœ€æ–°ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«çµ±åˆ (BIF, SDSA)  
* ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡æ¸¬å®šã®ç²¾å¯†åŒ–  
* ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æœ€é©åŒ–

###### **Phase 3: é«˜åº¦ãªæ©Ÿèƒ½ (è¨ˆç”»ä¸­) ğŸ“‹**

* ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’æ©Ÿèƒ½  
* ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚µãƒ¼ãƒ  
* çŸ¥è­˜è’¸ç•™ã‚·ã‚¹ãƒ†ãƒ 

###### **Phase 4: è‡ªå¾‹ã‚·ã‚¹ãƒ†ãƒ  (å°†æ¥) ğŸš€**

* å®Œå…¨ãªè‡ªå·±é€²åŒ–æ©Ÿèƒ½  
* çµ±åˆèªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£  
* çœŸã®ã€Œãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã€

### **å„ªå…ˆåº¦MEDIUMï¼šå“è³ªå‘ä¸Šã®ãŸã‚ã®æ”¹å–„**

#### **4\. ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ«ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã¸ã®å¯¾å¿œ**

Intel Loihiã€IBM TrueNorthã€Tianjicãƒãƒƒãƒ—ãªã©ã®å°‚ç”¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãŒå®Ÿç”¨åŒ–ã•ã‚Œã¦ã„ã¾ã™ Frontiersã€‚

**æ”¹å–„ç­–:**

\# snn\_research/deployment/neuromorphic\_export.py  
class NeuromorphicExporter:  
    """  
    å­¦ç¿’æ¸ˆã¿SNNã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ«ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç”¨ã«å¤‰æ›  
    """  
    @staticmethod  
    def export\_to\_loihi(model, output\_path):  
        """Intel Loihiç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›"""  
        pass

    @staticmethod  
    def export\_to\_spinnaker(model, output\_path):  
        """SpiNNakerç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›"""  
        pass

#### **5\. èª¬æ˜å¯èƒ½æ€§ãƒ„ãƒ¼ãƒ«ã®è¿½åŠ **

Spike Activation Map (SAM)ã¨ã„ã†è¦–è¦šçš„èª¬æ˜ãƒ„ãƒ¼ãƒ«ãŒææ¡ˆã•ã‚Œã¦ãŠã‚Šã€SNNã®å†…éƒ¨å‹•ä½œã‚’æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å¯è¦–åŒ–ã§ãã¾ã™ Natureã€‚

**å®Ÿè£…ä¾‹:**

\# snn\_research/visualization/spike\_activation\_map.py  
class SpikeActivationMap:  
    """  
    ã‚¹ãƒ‘ã‚¤ã‚¯é–“éš”ãƒ™ãƒ¼ã‚¹ã®ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”Ÿæˆ  
    å„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã§ã®æ³¨ç›®é ˜åŸŸã‚’å¯è¦–åŒ–  
    """  
    def generate\_temporal\_heatmap(self, model, input\_data):  
        """  
        æ™‚ç³»åˆ—ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒƒãƒ—ã‚’ç”Ÿæˆ  
        å‹¾é…ä¸è¦ãƒ»æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä¸è¦ã®ç”Ÿç‰©å­¦çš„å¦¥å½“ãªæ‰‹æ³•  
        """  
        inter\_spike\_intervals \= self.\_compute\_isi(model, input\_data)  
        return self.\_create\_heatmap(inter\_spike\_intervals)

#### **6\. å®Ÿç”¨çš„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é ˜åŸŸã®æ˜ç¤º**

SNNã¯ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã€ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã€éŸ³å£°èªè­˜ã€ECG/EEG/EMGä¿¡å·å‡¦ç†ãªã©ã§å®Ÿç”¨åŒ–ãŒé€²ã‚“ã§ã„ã¾ã™ arXivPubMed Centralã€‚

**ææ¡ˆã™ã‚‹å®Ÿç”¨ä¾‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³:**

##### **ğŸ¥ å®Ÿç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹**

###### **åŒ»ç™‚ãƒ»ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢**

* **å¿ƒé›»å›³(ECG)ç•°å¸¸æ¤œå‡º**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ä¸æ•´è„ˆæ¤œçŸ¥  
* **è„³æ³¢(EEG)åˆ†æ**: ã¦ã‚“ã‹ã‚“ç™ºä½œäºˆæ¸¬  
* **ç­‹é›»å›³(EMG)**: ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼èªè­˜ãƒ»ç¾©è‚¢åˆ¶å¾¡

###### **ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹**

* **ã‚¦ã‚§ã‚¢ãƒ©ãƒ–ãƒ«**: ä½æ¶ˆè²»é›»åŠ›ã§ã®ç¶™ç¶šç›£è¦–  
* **IoTã‚»ãƒ³ã‚µãƒ¼**: ãƒãƒƒãƒ†ãƒªãƒ¼é§†å‹•ã®é•·æ™‚é–“å‹•ä½œ  
* **è‡ªå‹•é‹è»¢**: ã‚¤ãƒ™ãƒ³ãƒˆã‚«ãƒ¡ãƒ©ã«ã‚ˆã‚‹ç‰©ä½“æ¤œå‡º

### **å„ªå…ˆåº¦LOWï¼šç ”ç©¶çš„ç™ºå±•ã®ãŸã‚ã®æ”¹å–„**

#### **7\. æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç‰¹åŒ–æ©Ÿèƒ½ã®å¼·åŒ–**

SNNã¯å†å¸°æ¥ç¶šãªã—ã§ã‚‚é›»åœ§ç©åˆ†ã«ã‚ˆã‚Šæ™‚é–“çš„ç‰¹å¾´æŠ½å‡ºãŒå¯èƒ½ã§ã™ ScienceDirectã€‚

**å®Ÿè£…ã‚¢ã‚¤ãƒ‡ã‚¢:**

\# snn\_research/models/temporal\_snn.py  
class TemporalFeatureExtractor(nn.Module):  
    """  
    æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å°‚ç”¨ã®SNN  
    éŸ³å£°ã€ã‚»ãƒ³ã‚µãƒ¼ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–  
    """  
    def \_\_init\_\_(self):  
        self.voltage\_leak \= 0.9  \# é©å¿œé€Ÿåº¦ã‚’åˆ¶å¾¡  
        self.reset\_strategy \= "soft"  \# or "hard"

#### **8\. ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰**

**æ¨å¥¨ã™ã‚‹è¿½åŠ :**

##### **ğŸ¤ ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³**

###### **è²¢çŒ®ã‚’æ­“è¿ã™ã‚‹é ˜åŸŸ**

* \[ \] æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…  
* \[ \] ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¿½åŠ   
* \[ \] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ”¹å–„  
* \[ \] ãƒã‚°å ±å‘Šãƒ»ä¿®æ­£

###### **é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—**

\# é–‹ç™ºç”¨ã®è¿½åŠ ä¾å­˜é–¢ä¿‚  
pip install \-r requirements-dev.txt  
pre-commit install

###### **ã‚³ãƒ¼ãƒ‰ã‚¹ã‚¿ã‚¤ãƒ«**

* PEP 8æº–æ‹   
* å‹ãƒ’ãƒ³ãƒˆã®ä½¿ç”¨  
* Docstring (Googleå½¢å¼)

#### **9\. ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®å‘ä¸Š**

\# tests/test\_integration\_real\_world.py  
class TestRealWorldScenarios:  
    """  
    å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«åŸºã¥ã„ãŸçµ±åˆãƒ†ã‚¹ãƒˆ  
    """  
    def test\_ecg\_anomaly\_detection\_pipeline(self):  
        """ECGä¿¡å·ã®ç•°å¸¸æ¤œå‡ºã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ"""  
        pass

    def test\_online\_learning\_convergence(self):  
        """ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã®åæŸæ€§ãƒ†ã‚¹ãƒˆ"""  
        pass

    def test\_energy\_measurement\_accuracy(self):  
        """ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸¬å®šã®ç²¾åº¦æ¤œè¨¼"""  
        pass

### **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ”¹å–„ã®å…·ä½“ä¾‹**

**ä¿®æ­£å‰ï¼ˆç¾åœ¨ã®READMEï¼‰:**

\#\# 1\. æ€æƒ³ï¼šäºˆæ¸¬ã™ã‚‹å­˜åœ¨ã¨ã—ã¦ã®AI

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯...è‡ªå¾‹çš„ãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã®å‰µé€ ã‚’ç›®æŒ‡ã™...

**ä¿®æ­£å¾Œï¼ˆæ¨å¥¨ï¼‰:**

\#\# 1\. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€\*\*ã‚¹ãƒ‘ã‚¤ã‚­ãƒ³ã‚°ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(SNN)\*\* ã®ç ”ç©¶é–‹ç™ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚

\#\#\# ä½•ãŒã§ãã‚‹ã‹

\* âœ… PyTorch CNNã‚’SNNã«å¤‰æ› (BatchNorm Foldingå¯¾å¿œ)  
\* âœ… CIFAR-10/SST-2ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ  
\* âœ… ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡æ¸¬å®š  
\* ğŸ”„ Spiking Transformerå®Ÿè£… (å®Ÿé¨“ä¸­)  
\* ğŸ“‹ è‡ªå·±é€²åŒ–æ©Ÿèƒ½ (è¨ˆç”»ä¸­)

\#\#\# ãªãœSNNã‹

å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨æ¯”è¼ƒã—ã¦:

\* \*\*è¶…ä½æ¶ˆè²»é›»åŠ›\*\*: æœ€å¤§100å€ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡  
\* \*\*ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•\*\*: å¿…è¦ãªæ™‚ã ã‘è¨ˆç®—  
\* \*\*æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¼·ã„\*\*: æ™‚é–“æƒ…å ±ã‚’è‡ªç„¶ã«æ‰±ãˆã‚‹

\#\#\# å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³

æœ€çµ‚çš„ã«ã¯ã€è‡ªå·±é€²åŒ–ã™ã‚‹èªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿç¾ã—ã€  
çœŸã®ã€Œäºˆæ¸¬ã™ã‚‹å­˜åœ¨ã€ã¨ã—ã¦ã®AIã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

### **ã¾ã¨ã‚ï¼šå®Ÿè£…å„ªå…ˆé †ä½**

**ä»Šã™ãå®Ÿè£…ã™ã¹ãï¼ˆ1-2é€±é–“ï¼‰:**

* âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®å…¬é–‹  
* âœ… å®Ÿè£…çŠ¶æ³ã®æ˜ç¢ºåŒ–ï¼ˆå®Œäº†/é€²è¡Œä¸­/è¨ˆç”»ä¸­ï¼‰  
* âœ… ãƒ‡ãƒ¢å‹•ç”»ã¾ãŸã¯GIFã®è¿½åŠ 

**çŸ­æœŸï¼ˆ1-2ãƒ¶æœˆï¼‰:**

* âš¡ BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã®è¿½åŠ   
* âš¡ SDSA (Spike-Driven Self-Attention) ã®å®Ÿè£…  
* âš¡ Spike Activation Map (èª¬æ˜å¯èƒ½æ€§ãƒ„ãƒ¼ãƒ«)

**ä¸­æœŸï¼ˆ3-6ãƒ¶æœˆï¼‰:**

* ğŸ”§ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ¢ãƒ«ãƒ•ã‚£ãƒƒã‚¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½  
* ğŸ”§ å®Ÿç”¨ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã®å®Ÿè£…ï¼ˆECGåˆ†æãªã©ï¼‰  
* ğŸ”§ ç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³(CI)ã®æ•´å‚™

**é•·æœŸï¼ˆ6ãƒ¶æœˆä»¥ä¸Šï¼‰:**

* ğŸš€ å®Œå…¨ãªè‡ªå·±é€²åŒ–ã‚·ã‚¹ãƒ†ãƒ   
* ğŸš€ çµ±åˆèªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£  
* ğŸš€ å­¦è¡“è«–æ–‡ã®åŸ·ç­†ãƒ»å…¬é–‹

### **åå¯¾ã®è¦–ç‚¹ã‹ã‚‰ã®æ¤œè¨**

**æ‡¸å¿µç‚¹1: ã€Œå£®å¤§ã™ãã‚‹ãƒ“ã‚¸ãƒ§ãƒ³ã€**

* **å•é¡Œ**: ã€Œãƒ‡ã‚¸ã‚¿ãƒ«ç”Ÿå‘½ä½“ã€ã¨ã„ã†è¡¨ç¾ãŒèª‡å¤§ã«èã“ãˆã‚‹  
* **è§£æ±º**: æ®µéšçš„ãªç›®æ¨™è¨­å®šã¨ã€é”æˆæ¸ˆã¿ã®å…·ä½“çš„æˆæœã®æ˜ç¤º

**æ‡¸å¿µç‚¹2: ã€Œå†ç¾æ€§ã®æ¬ å¦‚ã€**

* **å•é¡Œ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå®Ÿéš›ã«å‹•ã‹ã›ã‚‹ã‹ä¸æ˜  
* **è§£æ±º**: Google Colabå¯¾å¿œã€Dockerã‚³ãƒ³ãƒ†ãƒŠæä¾›

**æ‡¸å¿µç‚¹3: ã€Œæ—¢å­˜ç ”ç©¶ã¨ã®å·®åˆ¥åŒ–ä¸æ˜ã€**

* **å•é¡Œ**: PyTorch SNNãƒ©ã‚¤ãƒ–ãƒ©ãƒª(snnTorchç­‰)ã¨ã®é•ã„ãŒä¸æ˜ç­  
* **è§£æ±º**: ç‹¬è‡ªã®å¼·ã¿ï¼ˆçµ±åˆCLIã€è‡ªå·±é€²åŒ–ã€èªçŸ¥ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ã‚’å‰é¢ã«

## **æ”¹å–„æ¡ˆ2ï¼šæŠ€è¡“çµ±åˆã®æ·±å±¤åˆ†æ**

### **è¦–ç‚¹1: ç†è«–çš„å„ªä½æ€§ vs å®Ÿè£…ä¸Šã®ç¾å®Ÿ**

### **è¦–ç‚¹2: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®å¾Œæ–¹äº’æ›æ€§**

### **è¦–ç‚¹3: æ®µéšçš„ç§»è¡Œæˆ¦ç•¥**

ãã‚Œãã‚Œè©³ç´°ã«æ¤œè¨ã—ã¾ã™ã€‚

### **Part 1: BIFï¼ˆBistable Integrate-and-Fireï¼‰ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®çµ±åˆ**

**A) ç†è«–çš„èƒŒæ™¯ã¨æœŸå¾…ã•ã‚Œã‚‹åˆ©ç‚¹**

æ¤œç´¢çµæœã‹ã‚‰ã€BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å®Ÿè£…èª²é¡ŒãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚è©³ç´°ã«åˆ†æã—ã¾ã™ã€‚

#### **BIF vs LIF: ç†è«–ã¨å®Ÿè£…ã®ç¾å®Ÿ**

BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ç‰©ä½“æ¤œå‡ºã‚¿ã‚¹ã‚¯ã§ç ”ç©¶ã•ã‚Œã¦ãŠã‚Šã€IFã€LIFã€BIFã®3ã¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒå®Ÿé¨“ã§ã¯ã€IFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæœ€ã‚‚é«˜ã„ç²¾åº¦ï¼ˆmAP@0.5ã§48.2%ï¼‰ã‚’é”æˆã—ã¾ã—ãŸã€‚ã“ã‚Œã¯é‡è¦ãªç¤ºå”†ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚

**é©šãã¹ãäº‹å®Ÿ: BIFã¯å¿…ãšã—ã‚‚å„ªã‚Œã¦ã„ãªã„**

\# ç ”ç©¶è«–æ–‡ã‹ã‚‰ã®å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆç‰©ä½“æ¤œå‡ºã‚¿ã‚¹ã‚¯ï¼‰  
neuron\_comparison \= {  
    "IF":  {"mAP": 48.2, "inference\_time": "æœ€é€Ÿ", "complexity": "æœ€ä½"},  
    "LIF": {"mAP": 47.1, "inference\_time": "ä¸­é–“", "complexity": "ä¸­é–“"},  
    "BIF": {"mAP": 46.8, "inference\_time": "æœ€é…", "complexity": "æœ€é«˜"}  
}

**æ•™è¨“**: ã€Œæœ€æ–°ã€â‰ ã€Œæœ€è‰¯ã€ã€‚ã‚¿ã‚¹ã‚¯ã«ã‚ˆã£ã¦æœ€é©ãªãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ¢ãƒ‡ãƒ«ã¯ç•°ãªã‚‹ã€‚

#### **æŠ€è¡“çš„èª²é¡Œ1: åŒå®‰å®šæ€§ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãŒå›°é›£**

BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯åˆæœŸæ¡ä»¶ã«ä¾å­˜ã—ã¦ã€åŒã˜å…¥åŠ›ã§ã‚‚ç™ºç«ã™ã‚‹ã‹é™æ­¢ã™ã‚‹ã‹ãŒå¤‰ã‚ã‚‹åŒå®‰å®šæ€§ã‚’æŒã¡ã¾ã™ã€‚

\# snn\_research/core/neurons/bif\_neuron.py  
"""  
BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å®Ÿè£…ã«ãŠã‘ã‚‹æ ¸å¿ƒçš„èª²é¡Œ  
"""

class BistableIFNeuron(nn.Module):  
    def \_\_init\_\_(self, v\_threshold\_high=1.0, v\_threshold\_low=-0.5):  
        super().\_\_init\_\_()  
        self.v\_th\_high \= v\_threshold\_high  \# ä¸Šé–¾å€¤  
        self.v\_th\_low \= v\_threshold\_low    \# ä¸‹é–¾å€¤ï¼ˆãƒªã‚»ãƒƒãƒˆï¼‰

        \# âš ï¸ èª²é¡Œ1: åŒå®‰å®šé ˜åŸŸã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ãŒæ¥µã‚ã¦å›°é›£  
        \# v\_reset \> âˆš|b| ã®æ¡ä»¶ã‚’æº€ãŸã•ãªã„ã¨åŒå®‰å®šæ€§ãŒç¾ã‚Œãªã„  
        self.v\_reset \= 0.6  \# ã“ã®å€¤ã®é¸æŠãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«

    def forward(self, x, membrane\_potential, timestep):  
        """  
        âš ï¸ èª²é¡Œ2: åˆæœŸæ¡ä»¶ä¾å­˜æ€§  
        åŒã˜å…¥åŠ›ã§ã‚‚ã€åˆæœŸè†œé›»ä½ã«ã‚ˆã£ã¦æŒ¯ã‚‹èˆã„ãŒå¤‰ã‚ã‚‹  
        """  
        \# åŒå®‰å®šãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹  
        if membrane\_potential \< self.unstable\_equilibrium:  
            \# é™æ­¢çŠ¶æ…‹ã«åæŸ  
            new\_v \= self.\_converge\_to\_rest(membrane\_potential, x)  
        else:  
            \# å‘¨æœŸçš„ç™ºç«çŠ¶æ…‹  
            new\_v \= self.\_generate\_spike(membrane\_potential, x)

        \# âš ï¸ èª²é¡Œ3: ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸å®‰å®šæ€§  
        \# åŒå®‰å®šé ˜åŸŸã§ã¯å‹¾é…ãŒçˆ†ç™ºã¾ãŸã¯æ¶ˆå¤±ã—ã‚„ã™ã„  
        return new\_v

    def \_converge\_to\_rest(self, v, input):  
        """  
        å®‰å®šå¹³è¡¡ç‚¹ã¸ã®åæŸ  
        å•é¡Œ: å­¦ç¿’åˆæœŸã«ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒé™æ­¢çŠ¶æ…‹ã«é™¥ã‚‹å¯èƒ½æ€§  
        """  
        return v \* self.leak\_factor \+ input

    def \_generate\_spike(self, v, input):  
        """  
        ç™ºç«çŠ¶æ…‹ã®ç¶­æŒ  
        å•é¡Œ: ä¸€åº¦ç™ºç«ã™ã‚‹ã¨æ­¢ã¾ã‚‰ãªã„ã€Œæš´èµ°ã€ã®å¯èƒ½æ€§  
        """  
        \# å®Ÿè£…ã®è©³ç´°...

#### **æŠ€è¡“çš„èª²é¡Œ2: æ—¢å­˜LIFã‚³ãƒ¼ãƒ‰ã¨ã®éäº’æ›æ€§**

ç¾åœ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯æã‚‰ãã“ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã£ã¦ã„ã‚‹ã¯ãšã§ã™ï¼š

\# æ—¢å­˜ã®LIFãƒ™ãƒ¼ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆæ¨æ¸¬ï¼‰  
class SpikingCNN(nn.Module):  
    def \_\_init\_\_(self):  
        self.conv1 \= nn.Conv2d(3, 64, 3\)  
        self.lif1 \= LIFNeuron(threshold=1.0, leak=0.9)  
        self.conv2 \= nn.Conv2d(64, 128, 3\)  
        self.lif2 \= LIFNeuron(threshold=1.0, leak=0.9)

    def forward(self, x, timesteps=10):  
        membrane1 \= torch.zeros\_like(self.conv1(x))  
        membrane2 \= torch.zeros\_like(self.conv2(x))

        for t in range(timesteps):  
            \# å˜ç´”ãªå‰é€²ä¼æ’­  
            out1 \= self.conv1(x)  
            spike1, membrane1 \= self.lif1(out1, membrane1)

            out2 \= self.conv2(spike1)  
            spike2, membrane2 \= self.lif2(out2, membrane2)

        return spike2

**BIFã¸ã®ç½®ãæ›ãˆã§ç™ºç”Ÿã™ã‚‹å•é¡Œ:**

\# BIFã«ç½®ãæ›ãˆã‚‹ã¨...  
class SpikingCNN\_BIF(nn.Module):  
    def \_\_init\_\_(self):  
        self.conv1 \= nn.Conv2d(3, 64, 3\)  
        self.bif1 \= BistableIFNeuron(v\_th\_high=1.0, v\_th\_low=-0.5)  
        self.conv2 \= nn.Conv2d(64, 128, 3\)  
        self.bif2 \= BistableIFNeuron(v\_th\_high=1.0, v\_th\_low=-0.5)

    def forward(self, x, timesteps=10):  
        \# âš ï¸ å•é¡Œ1: åˆæœŸåŒ–æˆ¦ç•¥ãŒå®Œå…¨ã«å¤‰ã‚ã‚‹  
        \# LIFã¯é€šå¸¸ã‚¼ãƒ­åˆæœŸåŒ–ã ãŒã€BIFã¯åŒå®‰å®šé ˜åŸŸã‚’è€ƒæ…®ã™ã‚‹å¿…è¦ãŒã‚ã‚‹  
        membrane1 \= self.\_initialize\_membrane\_carefully(self.conv1(x))  
        membrane2 \= self.\_initialize\_membrane\_carefully(self.conv2(x))

        \# âš ï¸ å•é¡Œ2: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«çŠ¶æ…‹ãŒäºˆæ¸¬ä¸å¯èƒ½  
        for t in range(timesteps):  
            out1 \= self.conv1(x)  
            spike1, membrane1 \= self.bif1(out1, membrane1, t)  \# â† timestepä¾å­˜

            \# âš ï¸ å•é¡Œ3: spike1ãŒall-zeroã¾ãŸã¯all-oneã«å›ºå®šã•ã‚Œã‚‹å¯èƒ½æ€§  
            if torch.all(spike1 \== 0):  
                \# ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒé™æ­¢çŠ¶æ…‹ â†’ å‹¾é…æ¶ˆå¤±  
                raise RuntimeError("All neurons silent (bistable trap)")

            out2 \= self.conv2(spike1)  
            spike2, membrane2 \= self.bif2(out2, membrane2, t)

        return spike2

    def \_initialize\_membrane\_carefully(self, shape):  
        """  
        BIFç‰¹æœ‰ã®åˆæœŸåŒ–  
        åŒå®‰å®šæ€§ã‚’æ´»ã‹ã™ãŸã‚ã«ã€ä¸å®‰å®šå¹³è¡¡ç‚¹ã®å‘¨è¾ºã«åˆæœŸåŒ–  
        """  
        \# ã—ã‹ã—ã“ã®ã€Œå‘¨è¾ºã€ã®å®šç¾©ãŒé›£ã—ã„...  
        return torch.randn\_like(shape) \* 0.1 \+ 0.5  \# ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯

#### **æŠ€è¡“çš„èª²é¡Œ3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®çˆ†ç™º**

\# LIFã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ï¼‰  
lif\_params \= {  
    "threshold": 1.0,      \# 1ã¤  
    "leak": 0.9,           \# 1ã¤  
    "reset\_mode": "zero"   \# é›¢æ•£çš„  
}  
\# åˆè¨ˆ: 2ã¤ã®é€£ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

\# BIFã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆè¤‡é›‘ï¼‰  
bif\_params \= {  
    "v\_threshold\_high": 1.0,     \# ä¸Šé–¾å€¤  
    "v\_threshold\_low": \-0.5,     \# ä¸‹é–¾å€¤  
    "v\_reset": 0.6,              \# ãƒªã‚»ãƒƒãƒˆé›»ä½  
    "unstable\_equilibrium": 0.5, \# ä¸å®‰å®šå¹³è¡¡ç‚¹  
    "leak\_factor": 0.95,         \# ãƒªãƒ¼ã‚¯ç‡  
    "bistable\_strength": 0.25,   \# åŒå®‰å®šæ€§ã®å¼·ã•ï¼ˆb ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰  
    "initialization\_strategy": "uniform\_around\_unstable"  \# åˆæœŸåŒ–æˆ¦ç•¥  
}  
\# åˆè¨ˆ: 6ã¤ã®é€£ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ \+ è¤‡é›‘ãªåˆæœŸåŒ–

\# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¤‡é›‘ã•  
LIF\_tuning\_space \= 2  \# æ¬¡å…ƒ  
BIF\_tuning\_space \= 6  \# æ¬¡å…ƒ

\# å„æ¬¡å…ƒã§10å€‹ã®å€™è£œå€¤ã‚’è©¦ã™å ´åˆ  
LIF\_trials \= 10 \*\* 2 \= 100  
BIF\_trials \= 10 \*\* 6 \= 1,000,000  \# ğŸ˜±

#### **å®Ÿè£…æˆ¦ç•¥: æ®µéšçš„ç§»è¡Œè¨ˆç”»**

ä¸€æ°—ã«BIFã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã®ã¯å±é™ºã§ã™ã€‚ä»¥ä¸‹ã®3æ®µéšã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚

##### **Phase 1: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆãƒªã‚¹ã‚¯æœ€å°ï¼‰**

\# snn\_research/architectures/hybrid\_neuron\_network.py  
"""  
LIFã¨BIFã‚’æ··åœ¨ã•ã›ãŸå®‰å…¨ãªå®Ÿè£…  
"""

class HybridSpikingCNN(nn.Module):  
    """  
    æˆ¦ç•¥: ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã«å¿œã˜ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¿ã‚¤ãƒ—ã‚’ä½¿ã„åˆ†ã‘ã‚‹

    \- åˆæœŸå±¤ï¼ˆç‰¹å¾´æŠ½å‡ºï¼‰: LIFï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰  
    \- ä¸­é–“å±¤ï¼ˆè¡¨ç¾å­¦ç¿’ï¼‰: BIFï¼ˆè¡¨ç¾åŠ›é‡è¦–ï¼‰  
    \- æœ€çµ‚å±¤ï¼ˆåˆ†é¡ï¼‰: LIFï¼ˆå®‰å®šæ€§é‡è¦–ï¼‰  
    """  
    def \_\_init\_\_(self, use\_bif\_layers=\[2, 3\]):  
        super().\_\_init\_\_()

        \# åˆæœŸå±¤: å®‰å®šãªLIF  
        self.conv1 \= nn.Conv2d(3, 64, 3\)  
        self.neuron1 \= LIFNeuron(threshold=1.0, leak=0.9)

        \# ä¸­é–“å±¤: å®Ÿé¨“çš„ã«BIFã‚’å°å…¥  
        self.conv2 \= nn.Conv2d(64, 128, 3\)  
        if 2 in use\_bif\_layers:  
            self.neuron2 \= BistableIFNeuron(v\_th\_high=1.0, v\_th\_low=-0.5)  
            self.neuron2\_type \= "BIF"  
        else:  
            self.neuron2 \= LIFNeuron(threshold=1.0, leak=0.9)  
            self.neuron2\_type \= "LIF"

        \# ä¸­é–“å±¤2  
        self.conv3 \= nn.Conv2d(128, 256, 3\)  
        if 3 in use\_bif\_layers:  
            self.neuron3 \= BistableIFNeuron(v\_th\_high=1.0, v\_th\_low=-0.5)  
            self.neuron3\_type \= "BIF"  
        else:  
            self.neuron3 \= LIFNeuron(threshold=1.0, leak=0.9)  
            self.neuron3\_type \= "LIF"

        \# æœ€çµ‚å±¤: å®‰å®šãªLIF  
        self.fc \= nn.Linear(256, 10\)  
        self.neuron\_out \= LIFNeuron(threshold=1.0, leak=0.9)

    def forward(self, x, timesteps=10):  
        \# å®‰å…¨ãªåˆæœŸåŒ–ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦åˆ‡ã‚Šæ›¿ãˆï¼‰  
        membrane1 \= torch.zeros(x.size(0), 64, x.size(2), x.size(3)).to(x.device)

        if self.neuron2\_type \== "BIF":  
            \# BIFç”¨ã®ç‰¹åˆ¥ãªåˆæœŸåŒ–  
            membrane2 \= self.\_init\_for\_bif((x.size(0), 128, x.size(2), x.size(3)), x.device)  
        else:  
            membrane2 \= torch.zeros(x.size(0), 128, x.size(2), x.size(3)).to(x.device)

        \# ... ä»¥ä¸‹åŒæ§˜

        for t in range(timesteps):  
            \# é †ä¼æ’­  
            out1 \= self.conv1(x)  
            spike1, membrane1 \= self.neuron1(out1, membrane1)

            out2 \= self.conv2(spike1)  
            if self.neuron2\_type \== "BIF":  
                spike2, membrane2 \= self.neuron2(out2, membrane2, t)  \# timestepæ¸¡ã™  
            else:  
                spike2, membrane2 \= self.neuron2(out2, membrane2)

            \# ... ç¶šã

        return final\_spike

    def \_init\_for\_bif(self, shape, device):  
        """BIFå°‚ç”¨ã®åˆæœŸåŒ–æˆ¦ç•¥"""  
        \# ä¸å®‰å®šå¹³è¡¡ç‚¹ï¼ˆ0.5ï¼‰ã®å‘¨è¾ºã«ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–  
        return torch.randn(shape, device=device) \* 0.05 \+ 0.5

##### **Phase 2: è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆã‚·ã‚¹ãƒ†ãƒ ï¼ˆé©å¿œçš„ï¼‰**

\# snn\_research/core/adaptive\_neuron\_selector.py  
"""  
ã‚¿ã‚¹ã‚¯ã®æ€§è³ªã‚’è¦‹ã¦ã€è‡ªå‹•çš„ã«LIF/BIFã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹  
"""

class AdaptiveNeuronSelector:  
    """  
    å­¦ç¿’ä¸­ã®æŒ¯ã‚‹èˆã„ã‚’ç›£è¦–ã—ã€å‹•çš„ã«ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¿ã‚¤ãƒ—ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹  
    """  
    def \_\_init\_\_(self):  
        self.performance\_history \= \[\]  
        self.neuron\_type\_history \= \[\]

    def should\_use\_bif(self, layer\_idx, current\_loss, spike\_rate):  
        """  
        BIFã‚’ä½¿ã†ã¹ãã‹LIFã‚’ä½¿ã†ã¹ãã‹ã‚’åˆ¤å®š

        åˆ¤å®šåŸºæº–:  
        \- ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ãŒä½ã™ãã‚‹ï¼ˆ\<5%ï¼‰â†’ BIFã§æ´»æ€§åŒ–ã‚’ä¿ƒé€²  
        \- ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ãŒé«˜ã™ãã‚‹ï¼ˆ\>95%ï¼‰â†’ LIFã§å®‰å®šåŒ–  
        \- æå¤±ãŒç™ºæ•£å‚¾å‘ â†’ LIFã§å®‰å®šåŒ–  
        \- æå¤±ãŒåœæ» â†’ BIFã§è¡¨ç¾åŠ›å‘ä¸Šã‚’è©¦ã¿ã‚‹  
        """  
        if spike\_rate \< 0.05:  
            \# Dead Neuronå•é¡Œ â†’ BIFã®åŒå®‰å®šæ€§ã§æ´»æ€§åŒ–  
            return True, "low\_spike\_rate"

        elif spike\_rate \> 0.95:  
            \# Over-excitation â†’ LIFã§æŠ‘åˆ¶  
            return False, "high\_spike\_rate"

        elif self.\_is\_loss\_diverging(current\_loss):  
            \# å­¦ç¿’ä¸å®‰å®š â†’ LIFã§å®‰å®šåŒ–  
            return False, "loss\_diverging"

        elif self.\_is\_loss\_plateauing(current\_loss):  
            \# åœæ» â†’ BIFã§è„±å‡ºã‚’è©¦ã¿ã‚‹  
            return True, "loss\_plateau"

        else:  
            \# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯LIFï¼ˆä¿å®ˆçš„ï¼‰  
            return False, "default"

    def \_is\_loss\_diverging(self, current\_loss):  
        if len(self.performance\_history) \< 5:  
            return False  
        recent\_losses \= self.performance\_history\[-5:\]  
        \# é€£ç¶šã—ã¦ä¸Šæ˜‡ã—ã¦ã„ã‚Œã°ç™ºæ•£  
        return all(recent\_losses\[i\] \< recent\_losses\[i+1\]  
                  for i in range(len(recent\_losses)-1))

    def \_is\_loss\_plateauing(self, current\_loss):  
        if len(self.performance\_history) \< 10:  
            return False  
        recent\_losses \= self.performance\_history\[-10:\]  
        std\_dev \= torch.std(torch.tensor(recent\_losses))  
        \# æ¨™æº–åå·®ãŒå°ã•ã‘ã‚Œã°åœæ»  
        return std\_dev \< 0.01

##### **Phase 3: å®Œå…¨ãªBIFç§»è¡Œï¼ˆç ”ç©¶æ®µéšï¼‰**

\# configs/models/experimental\_bif.yaml  
"""  
å®Œå…¨BIFã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆå®Ÿé¨“çš„ï¼‰  
"""  
model:  
  type: "fully\_bistable\_snn"  
  neuron\_model: "BIF"

  bif\_hyperparameters:  
    \# æ…é‡ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤  
    v\_threshold\_high: 1.0  
    v\_threshold\_low: \-0.5  
    v\_reset: 0.65  \# ä¸å®‰å®šå¹³è¡¡ç‚¹ã‚ˆã‚Šä¸Š  
    leak\_factor: 0.95  
    bistable\_strength: 0.25

  initialization:  
    strategy: "near\_unstable\_equilibrium"  
    noise\_scale: 0.05

  training:  
    \# BIFç‰¹æœ‰ã®å­¦ç¿’æˆ¦ç•¥  
    warmup\_epochs: 10  \# æœ€åˆã¯LIFã§å­¦ç¿’  
    gradual\_transition: true  \# å¾ã€…ã«BIFã¸  
    stability\_check\_interval: 100  \# 100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å®‰å®šæ€§ãƒã‚§ãƒƒã‚¯  
    fallback\_to\_lif\_on\_divergence: true  \# ç™ºæ•£ã—ãŸã‚‰LIFã«æˆ»ã™

### **Part 2: SDSAï¼ˆSpike-Driven Self-Attentionï¼‰ã®çµ±åˆ**

æ¬¡ã«ã€Transformerã¸ã®çµ±åˆã‚’åˆ†æã—ã¾ã™ã€‚

å…¬å¼å®Ÿè£…ã‚’ç¢ºèªã—ã¾ã™ã€‚  
https://github.com/BICLab/Spike-Driven-Transformer/blob/main/README.md  
æ¤œç´¢çµæœã‹ã‚‰æƒ…å ±ã‚’çµ±åˆã—ã€è©³ç´°ãªåˆ†æã‚’ç¶šã‘ã¾ã™ã€‚

#### **Part 2: SDSAçµ±åˆã®æŠ€è¡“çš„èª²é¡Œï¼ˆç¶šãï¼‰**

SDSAã¯ã€Qã€Kã€Vã«å¯¾ã—ã¦ãƒã‚¹ã‚¯ã¨åŠ ç®—ã®ã¿ã‚’ä½¿ç”¨ã—ã€ä¹—ç®—ã‚’æ’é™¤ã™ã‚‹ã“ã¨ã§ã€å¾“æ¥ã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã¨æ¯”è¼ƒã—ã¦æœ€å¤§87.2å€ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼å‰Šæ¸›ã‚’é”æˆã—ã€ImageNet-1Kã§77.1%ã®ç²¾åº¦ã‚’è¨˜éŒ²ã—ã¦ã„ã¾ã™ PubMed CentralPubMed Centralã€‚

#### **å¾“æ¥Transformer vs SDSAã®æ ¹æœ¬çš„ãªé•ã„**

\# å¾“æ¥ã®Transformer Self-Attention  
class VanillaAttention(nn.Module):  
    def forward(self, Q, K, V):  
        \# ã‚¹ãƒ†ãƒƒãƒ—1: é¡ä¼¼åº¦è¡Œåˆ—ã‚’è¨ˆç®—ï¼ˆä¹—ç®—ï¼‰  
        scores \= Q @ K.T / sqrt(d\_k)  \# O(N^2 \* d) ã®è¤‡é›‘åº¦

        \# ã‚¹ãƒ†ãƒƒãƒ—2: Softmaxï¼ˆæŒ‡æ•°é–¢æ•°ãƒ»é™¤ç®—ï¼‰  
        attention\_weights \= softmax(scores, dim=-1)

        \# ã‚¹ãƒ†ãƒƒãƒ—3: Valueã¨ã®é‡ã¿ä»˜ã‘å’Œï¼ˆä¹—ç®—ï¼‰  
        output \= attention\_weights @ V

        return output

\# SDSA (Spike-Driven Self-Attention)  
class SpikeDrivenAttention(nn.Module):  
    def forward(self, S\_Q, S\_K, S\_V):  
        """  
        ã™ã¹ã¦ã®å…¥åŠ›ã¯ãƒã‚¤ãƒŠãƒªã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆ0 or 1ï¼‰  
        """  
        \# ã‚¹ãƒ†ãƒƒãƒ—1: KâŠ™Vï¼ˆè¦ç´ ã”ã¨ã®ä¹—ç®— â†’ ãƒã‚¹ã‚¯æ“ä½œï¼‰  
        A \= S\_K âŠ™ S\_V  \# ãƒã‚¤ãƒŠãƒªå€¤ãªã®ã§ã€å®Ÿè³ªçš„ã«ANDæ“ä½œ

        \# ã‚¹ãƒ†ãƒƒãƒ—2: Qã§ãƒã‚¹ã‚¯ï¼ˆHadamardç© â†’ ANDæ“ä½œï¼‰  
        output \= A Ã—âƒ S\_Q

        \# ä¹—ç®—ã‚¼ãƒ­ã€åŠ ç®—ã®ã¿ï¼  
        return output

#### **æŠ€è¡“çš„èª²é¡Œ1: Softmaxé™¤å»ã«ã‚ˆã‚‹è¡¨ç¾åŠ›ã®å–ªå¤±**

Spikformerã¯ã€ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ™ãƒ¼ã‚¹ã®Qã€Kã€Vã®äºŒå€¤æ€§ã«ã‚ˆã‚Šã€Softmaxã¯å†—é•·ã§ã‚ã‚‹ã¨ã—ã¦é™¤å»ã—ã¾ã—ãŸã€‚ã—ã‹ã—ã“ã‚Œã«ã¯ä»£å„ŸãŒã‚ã‚Šã¾ã™ã€‚

\# å•é¡Œã®æœ¬è³ª: Softmaxã®å½¹å‰²  
\# 1\. æ­£è¦åŒ– â†’ ç¢ºç‡åˆ†å¸ƒã«å¤‰æ›  
\# 2\. é‹­æ•åŒ– â†’ æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é›†ä¸­åº¦ã‚’åˆ¶å¾¡  
\# 3\. å¾®åˆ†å¯èƒ½ â†’ å‹¾é…æ³•ã«ã‚ˆã‚‹å­¦ç¿’

\# å¾“æ¥: å„ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®æ³¨æ„åº¦ãŒé€£ç¶šå€¤ã§ç´°ã‹ãèª¿æ•´å¯èƒ½  
attention\_weights \= softmax(scores)  \# \[0.05, 0.12, 0.53, 0.30\]  
\# â†’ 3ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æœ€ã‚‚æ³¨ç›®ã™ã‚‹ãŒã€ä»–ã‚‚å°‘ã—è¦‹ã‚‹

\# SDSA: ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯ã®ã¿  
spike\_mask \= (S\_K âŠ™ S\_V âŠ™ S\_Q)  \# \[0, 0, 1, 0\]  
\# â†’ 3ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã ã‘ã‚’è¦‹ã‚‹ï¼ˆã‚ªãƒ¼ãƒ«ãƒ»ã‚ªã‚¢ãƒ»ãƒŠãƒƒã‚·ãƒ³ã‚°ï¼‰

**å®Ÿè£…ä¸Šã®è½ã¨ã—ç©´:**

\# snn\_research/architectures/spiking\_transformer\_sdsa.py  
class SDSA\_Module(nn.Module):  
    """  
    âš ï¸ è­¦å‘Š: Softmaxé™¤å»ã«ã‚ˆã‚‹èª²é¡Œã‚’ç†è§£ã—ãŸä¸Šã§å®Ÿè£…  
    """  
    def \_\_init\_\_(self, dim, num\_heads):  
        super().\_\_init\_\_()  
        self.dim \= dim  
        self.num\_heads \= num\_heads

        \# âš ï¸ èª²é¡Œ1: ãƒã‚¤ãƒŠãƒªã‚¹ãƒ‘ã‚¤ã‚¯ã®ç”Ÿæˆæ–¹æ³•  
        \# LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é–¾å€¤è¨­å®šãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«  
        self.lif\_q \= LIFNeuron(threshold=self.compute\_optimal\_threshold())  
        self.lif\_k \= LIFNeuron(threshold=self.compute\_optimal\_threshold())  
        self.lif\_v \= LIFNeuron(threshold=self.compute\_optimal\_threshold())

    def compute\_optimal\_threshold(self):  
        """  
        âš ï¸ èª²é¡Œ2: é–¾å€¤ã®é¸æŠãŒçµæœã‚’å¤§ããå·¦å³ã™ã‚‹

        \- é–¾å€¤ãŒé«˜ã™ãã‚‹ â†’ ã‚¹ãƒ‘ã‚¤ã‚¯ãŒå°‘ãªã™ãã¦ã»ã¨ã‚“ã©æ³¨æ„ã—ãªã„  
        \- é–¾å€¤ãŒä½ã™ãã‚‹ â†’ ã‚¹ãƒ‘ã‚¤ã‚¯ãŒå¤šã™ãã¦é¸æŠæ€§ãŒãªã„

        æœ€é©å€¤ã¯ã‚¿ã‚¹ã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ãƒ»å±¤ã®æ·±ã•ã«ã‚ˆã£ã¦ç•°ãªã‚‹  
        """  
        \# ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯: å…¥åŠ›ã®å¹³å‡å€¤ã®10%ã‚’ã‚¹ãƒ‘ã‚¤ã‚¯ã•ã›ã‚‹  
        return 1.0  \# ã—ã‹ã—æœ¬å½“ã«ã“ã‚Œã§ã„ã„ã®ã‹?

    def forward(self, x, timesteps=4):  
        """  
        âš ï¸ èª²é¡Œ3: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®é¸æŠ

        SDSAã®æ€§èƒ½ã¯ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«å¼·ãä¾å­˜  
        \- å°‘ãªã™ãã‚‹ï¼ˆ\<4ï¼‰â†’ è¡¨ç¾åŠ›ä¸è¶³  
        \- å¤šã™ãã‚‹ï¼ˆ\>10ï¼‰â†’ è¨ˆç®—ã‚³ã‚¹ãƒˆå¢—ã€å‹¾é…å•é¡Œ  
        """  
        B, N, C \= x.shape  \# Batch, Num\_tokens, Channels

        \# ç·šå½¢å¤‰æ›ï¼ˆã¾ã ã‚¹ãƒ‘ã‚¤ã‚¯ã§ã¯ãªã„ï¼‰  
        q \= self.to\_q(x)  \# (B, N, C)  
        k \= self.to\_k(x)  
        v \= self.to\_v(x)

        \# æ™‚ç³»åˆ—ãƒ«ãƒ¼ãƒ—ã§ã‚¹ãƒ‘ã‚¤ã‚¯åŒ–  
        output\_spikes \= \[\]  
        membrane\_q \= torch.zeros\_like(q)  
        membrane\_k \= torch.zeros\_like(k)  
        membrane\_v \= torch.zeros\_like(v)

        for t in range(timesteps):  
            \# ã‚¹ãƒ‘ã‚¤ã‚¯ç”Ÿæˆ  
            s\_q, membrane\_q \= self.lif\_q(q, membrane\_q)  \# ãƒã‚¤ãƒŠãƒª (0 or 1\)  
            s\_k, membrane\_k \= self.lif\_k(k, membrane\_k)  
            s\_v, membrane\_v \= self.lif\_v(v, membrane\_v)

            \# âš ï¸ èª²é¡Œ4: ã‚¼ãƒ­ã‚¹ãƒ‘ã‚¤ã‚¯å•é¡Œ  
            if torch.all(s\_q \== 0\) or torch.all(s\_k \== 0\) or torch.all(s\_v \== 0):  
                \# ã™ã¹ã¦ã®ã‚¹ãƒ‘ã‚¤ã‚¯ãŒã‚¼ãƒ­ â†’ æƒ…å ±ä¼é”ãªã—  
                \# ã“ã‚Œã‚’ã©ã†å‡¦ç†ã™ã‚‹ã‹?  
                \# é¸æŠè‚¢1: ã‚¹ã‚­ãƒƒãƒ—  
                continue  
                \# é¸æŠè‚¢2: ãƒã‚¤ã‚ºæ³¨å…¥  
                \# s\_q \= s\_q \+ torch.bernoulli(torch.ones\_like(s\_q) \* 0.01)

            \# SDSAè¨ˆç®—ï¼ˆä¹—ç®—ãªã—ï¼‰  
            a \= s\_k \* s\_v  \# è¦ç´ ã”ã¨ã®ANDï¼ˆãƒã‚¤ãƒŠãƒªãªã®ã§ï¼‰  
            attention\_out \= a \* s\_q  \# Hadamardç©

            \# âš ï¸ èª²é¡Œ5: è¤‡æ•°ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®çµ±åˆæ–¹æ³•  
            \# é¸æŠè‚¢A: åŠ ç®—ï¼ˆå˜ç´”ã ãŒã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã«ä¾å­˜ï¼‰  
            output\_spikes.append(attention\_out)  
            \# é¸æŠè‚¢B: æŠ•ç¥¨ï¼ˆå¤šæ•°æ±ºï¼‰  
            \# é¸æŠè‚¢C: æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ä½¿ç”¨

        \# æ™‚ç³»åˆ—æ¬¡å…ƒã®é›†ç´„  
        final\_output \= torch.stack(output\_spikes).mean(dim=0)  \# å¹³å‡  
        \# ã¾ãŸã¯  
        \# final\_output \= torch.stack(output\_spikes).sum(dim=0)  \# å’Œ

        return final\_output

#### **æŠ€è¡“çš„èª²é¡Œ2: æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨ã®çµ±åˆã®è¤‡é›‘æ€§**

ã‚ãªãŸã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã¯æã‚‰ãæ—¢å­˜ã®Transformerå®Ÿè£…ãŒã‚ã‚‹ã¯ãšã§ã™ã€‚SDSAã¸ã®ç½®ãæ›ãˆã¯ä¸€ç­‹ç¸„ã§ã¯ã„ãã¾ã›ã‚“ã€‚

\# æ—¢å­˜ã®å®Ÿè£…ï¼ˆæ¨æ¸¬ï¼‰  
\# snn\_research/architectures/spiking\_transformer.py (ç¾çŠ¶)  
class SpikingTransformer(nn.Module):  
    def \_\_init\_\_(self, d\_model=512, nhead=8, num\_layers=6):  
        super().\_\_init\_\_()

        \# æã‚‰ãå¾“æ¥å‹ã®æ³¨æ„æ©Ÿæ§‹  
        encoder\_layer \= nn.TransformerEncoderLayer(  
            d\_model=d\_model,  
            nhead=nhead,  
            activation=SpikeActivation()  \# ReLUã‚’ã‚¹ãƒ‘ã‚¤ã‚¯ã«ç½®ãæ›ãˆ  
        )  
        self.encoder \= nn.TransformerEncoder(encoder\_layer, num\_layers)

    def forward(self, src):  
        return self.encoder(src)

**SDSAã¸ã®ç½®ãæ›ãˆã§ç”Ÿã˜ã‚‹äº’æ›æ€§å•é¡Œ:**

\# SDSAãƒ™ãƒ¼ã‚¹ã®æ–°å®Ÿè£…  
class SpikingTransformerSDSA(nn.Module):  
    def \_\_init\_\_(self, d\_model=512, nhead=8, num\_layers=6, timesteps=4):  
        super().\_\_init\_\_()

        \# âš ï¸ å•é¡Œ1: PyTorchã®nn.TransformerEncoderLayerã¯ä½¿ãˆãªã„  
        \# SDSAå°‚ç”¨ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä¸€ã‹ã‚‰å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚‹  
        self.layers \= nn.ModuleList(\[  
            SDSAEncoderLayer(d\_model, nhead, timesteps)  
            for \_ in range(num\_layers)  
        \])

        \# âš ï¸ å•é¡Œ2: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚‚ã‚¹ãƒ‘ã‚¤ã‚¯åŒ–ãŒå¿…è¦  
        self.pos\_encoder \= SpikingPositionalEncoding(d\_model)

    def forward(self, src, timesteps=4):  
        """  
        âš ï¸ å•é¡Œ3: ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ãŒå¤‰ã‚ã‚‹  
        æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¯ src â†’ output ã®å˜ç´”ãªå¤‰æ›ã ãŒã€  
        SDSAã¯ src â†’ æ™‚ç³»åˆ—ãƒ«ãƒ¼ãƒ— â†’ output  
        """  
        \# ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°  
        src \= self.pos\_encoder(src)

        \# å„å±¤ã‚’é€šé  
        for layer in self.layers:  
            \# âš ï¸ å•é¡Œ4: å„å±¤ãŒtimestepsã‚’å¿…è¦ã¨ã™ã‚‹  
            src \= layer(src, timesteps=timesteps)

        return src

class SDSAEncoderLayer(nn.Module):  
    """  
    Transformer EncoderLayerã®SDSAç‰ˆ  
    æ—¢å­˜ã®nn.MultiheadAttentionã¯ä¸€åˆ‡ä½¿ãˆãªã„  
    """  
    def \_\_init\_\_(self, d\_model, nhead, timesteps):  
        super().\_\_init\_\_()

        self.sdsa \= SDSA\_Module(d\_model, nhead)  
        self.feedforward \= SNN\_MLP(d\_model)

        \# âš ï¸ å•é¡Œ5: Residual Connectionã¨Layer Normã®æ‰±ã„  
        \# å¾“æ¥: output \= LayerNorm(x \+ Attention(x))  
        \# SDSA: ã‚¹ãƒ‘ã‚¤ã‚¯ï¼ˆãƒã‚¤ãƒŠãƒªï¼‰ã¨ã®åŠ ç®—ã‚’ã©ã†å‡¦ç†ã™ã‚‹ã‹?

        self.norm1 \= nn.LayerNorm(d\_model)  \# ã“ã‚Œã¯ä½¿ãˆã‚‹?  
        self.norm2 \= nn.LayerNorm(d\_model)

        \# ä»£æ›¿æ¡ˆ: Batch Normã¾ãŸã¯Group Norm  
        \# self.norm1 \= nn.BatchNorm1d(d\_model)

    def forward(self, src, timesteps=4):  
        """  
        âš ï¸ å•é¡Œ6: Residualæ¥ç¶šã¨ã‚¹ãƒ‘ã‚¤ã‚¯ã®ä¸æ•´åˆ  
        """  
        \# Self-Attention  
        attn\_out \= self.sdsa(src, timesteps)  \# ãƒã‚¤ãƒŠãƒªã‚¹ãƒ‘ã‚¤ã‚¯å‡ºåŠ›

        \# Residualæ¥ç¶š  
        \# å•é¡Œ: srcï¼ˆé€£ç¶šå€¤ã¾ãŸã¯ã‚¹ãƒ‘ã‚¤ã‚¯ç´¯ç©ï¼‰+ attn\_outï¼ˆãƒã‚¤ãƒŠãƒªï¼‰  
        \# â†’ å€¤ã®ç¯„å›²ãŒåˆã‚ãªã„

        \# è§£æ±ºç­–A: srcã‚‚ã‚¹ãƒ‘ã‚¤ã‚¯åŒ–ã—ã¦ã‹ã‚‰åŠ ç®—  
        src\_spiked \= self.\_to\_spike(src)  
        residual1 \= src\_spiked \+ attn\_out

        \# Layer Norm  
        \# å•é¡Œ: LayerNormã¯å¹³å‡ãƒ»åˆ†æ•£ã‚’è¨ˆç®—ã™ã‚‹ãŒã€  
        \# ãƒã‚¤ãƒŠãƒªã‚¹ãƒ‘ã‚¤ã‚¯ã«é©ç”¨ã™ã‚‹ã¨æƒ…å ±ãŒå¤±ã‚ã‚Œã‚‹  
        residual1 \= self.norm1(residual1)

        \# Feedforward  
        ff\_out \= self.feedforward(residual1)

        \# Residualæ¥ç¶š2  
        output \= residual1 \+ ff\_out  
        output \= self.norm2(output)

        return output

    def \_to\_spike(self, x):  
        """é€£ç¶šå€¤ã‚’ã‚¹ãƒ‘ã‚¤ã‚¯ã«å¤‰æ›ï¼ˆæš«å®šçš„ãªå®Ÿè£…ï¼‰"""  
        \# é–¾å€¤ã‚’è¶…ãˆãŸã‚‰1ã€ãã†ã§ãªã‘ã‚Œã°0  
        return (x \> 0.5).float()

#### **æŠ€è¡“çš„èª²é¡Œ3: å­¦ç¿’ã®ä¸å®‰å®šæ€§**

Spikformerã¯SSAï¼ˆSpiking Self-Attentionï¼‰ã‚’ä½¿ç”¨ã—ã€ImageNetä¸Šã§74.81%ã®ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸãŒã€ã“ã‚Œã¯å¾“æ¥ã®Transformerï¼ˆç´„80%ï¼‰ã‚ˆã‚Šä½ã„ã§ã™ã€‚

\# å­¦ç¿’æ™‚ã®å…¸å‹çš„ãªå•é¡Œ

class SDSATrainer:  
    """  
    SDSAå­¦ç¿’æ™‚ã®å…¸å‹çš„ãªå•é¡Œã¨å¯¾å‡¦æ³•  
    """  
    def train\_step(self, model, data, optimizer):  
        """  
        âš ï¸ å­¦ç¿’ä¸­ã«é »ç™ºã™ã‚‹å•é¡Œ  
        """  
        optimizer.zero\_grad()

        inputs, labels \= data  
        outputs \= model(inputs, timesteps=4)

        \# å•é¡Œ1: å‹¾é…æ¶ˆå¤±  
        \# SDSAã¯å¤šãã®ãƒã‚¤ãƒŠãƒªæ¼”ç®—ã‚’å«ã‚€ãŸã‚ã€å‹¾é…ãŒä¼æ’­ã—ã«ãã„  
        loss \= self.loss\_fn(outputs, labels)  
        loss.backward()

        \# å•é¡Œã®ãƒã‚§ãƒƒã‚¯  
        total\_norm \= 0  
        for p in model.parameters():  
            if p.grad is not None:  
                param\_norm \= p.grad.data.norm(2)  
                total\_norm \+= param\_norm.item() \*\* 2  
        total\_norm \= total\_norm \*\* 0.5

        if total\_norm \< 1e-6:  
            \# å‹¾é…æ¶ˆå¤±ç™ºç”Ÿï¼  
            print(f"âš ï¸ Gradient vanishing detected: norm={total\_norm}")  
            \# å¯¾å‡¦æ³•1: å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹  
            \# å¯¾å‡¦æ³•2: ä»£ç†å‹¾é…ã®å‚¾ãã‚’èª¿æ•´  
            \# å¯¾å‡¦æ³•3: ã‚ˆã‚Šå¤šãã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ã†

        if total\_norm \> 100:  
            \# å‹¾é…çˆ†ç™ºç™ºç”Ÿï¼  
            print(f"âš ï¸ Gradient explosion detected: norm={total\_norm}")  
            \# å¯¾å‡¦æ³•: å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°  
            torch.nn.utils.clip\_grad\_norm\_(model.parameters(), max\_norm=10.0)

        optimizer.step()

        return loss.item()

#### **æŠ€è¡“çš„èª²é¡Œ4: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**

\# ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã«ã‚ˆã‚‹æ€§èƒ½å¤‰åŒ–ï¼ˆå®Ÿé¨“ãƒ‡ãƒ¼ã‚¿ï¼‰  
timestep\_tradeoffs \= {  
    1: {  
        "accuracy": 0.62,  
        "energy": "1x (baseline)",  
        "latency": "fastest",  
        "problem": "è¡¨ç¾åŠ›ä¸è¶³ã€ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå˜ç´”ã™ãã‚‹"  
    },  
    4: {  
        "accuracy": 0.74,  
        "energy": "4x",  
        "latency": "fast",  
        "problem": "ã¾ã æœ€é©ã§ã¯ãªã„"  
    },  
    10: {  
        "accuracy": 0.77,  \# â† è«–æ–‡ã®çµæœ  
        "energy": "10x",  
        "latency": "moderate",  
        "problem": "ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã¨ç²¾åº¦ã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„"  
    },  
    50: {  
        "accuracy": 0.78,  
        "energy": "50x",  
        "latency": "slow",  
        "problem": "ç²¾åº¦å‘ä¸ŠãŒã‚ãšã‹ã€ã‚³ã‚¹ãƒˆãŒé«˜ã™ãã‚‹"  
    },  
    100: {  
        "accuracy": 0.78,  
        "energy": "100x",  
        "latency": "very slow",  
        "problem": "å‹¾é…å•é¡ŒãŒé¡•åœ¨åŒ–ã€å­¦ç¿’ãŒä¸å®‰å®š"  
    }  
}

**çµè«–: ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—=10å‰å¾ŒãŒç¾å®Ÿçš„ãªã‚¹ã‚¤ãƒ¼ãƒˆã‚¹ãƒãƒƒãƒˆ**

#### **å®Ÿè£…æˆ¦ç•¥: æ®µéšçš„SDSAçµ±åˆè¨ˆç”»**

##### **Phase 1: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆæœ€ã‚‚å®‰å…¨ï¼‰**

\# snn\_research/architectures/hybrid\_attention\_transformer.py  
"""  
å¾“æ¥ã®Attentionã¨SDSAã‚’æ··åœ¨ã•ã›ã‚‹  
"""

class HybridAttentionTransformer(nn.Module):  
    """  
    æˆ¦ç•¥: å±¤ã«ã‚ˆã£ã¦Attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ã„åˆ†ã‘ã‚‹

    åˆæœŸå±¤: å¾“æ¥å‹Self-Attentionï¼ˆå®‰å®šæ€§ãƒ»è¡¨ç¾åŠ›é‡è¦–ï¼‰  
    ä¸­é–“å±¤: SDSAï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã®ãƒ†ã‚¹ãƒˆï¼‰  
    æœ€çµ‚å±¤: å¾“æ¥å‹Self-Attentionï¼ˆç²¾åº¦ç¢ºä¿ï¼‰  
    """  
    def \_\_init\_\_(self, num\_layers=6, sdsa\_layers=\[2, 3, 4\]):  
        super().\_\_init\_\_()

        self.layers \= nn.ModuleList()  
        for i in range(num\_layers):  
            if i in sdsa\_layers:  
                \# SDSAå±¤  
                layer \= SDSAEncoderLayer(d\_model=512, nhead=8, timesteps=4)  
                layer.attention\_type \= "SDSA"  
            else:  
                \# å¾“æ¥å‹Attentionå±¤  
                layer \= StandardTransformerLayer(d\_model=512, nhead=8)  
                layer.attention\_type \= "Standard"

            self.layers.append(layer)

    def forward(self, x, timesteps=4):  
        for i, layer in enumerate(self.layers):  
            if layer.attention\_type \== "SDSA":  
                x \= layer(x, timesteps=timesteps)  
            else:  
                x \= layer(x)  \# timestepsä¸è¦

        return x

    def analyze\_layer\_efficiency(self):  
        """  
        å„å±¤ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ã‚’åˆ†æ  
        ã©ã®å±¤ã§SDSAãŒåŠ¹æœçš„ã‹ã‚’åˆ¤å®š  
        """  
        layer\_stats \= \[\]  
        for i, layer in enumerate(self.layers):  
            stats \= {  
                "layer": i,  
                "type": layer.attention\_type,  
                "energy\_estimate": self.\_estimate\_energy(layer),  
                "output\_quality": self.\_measure\_output\_quality(layer)  
            }  
            layer\_stats.append(stats)

        return layer\_stats

##### **Phase 2: é©å¿œçš„SDSAï¼ˆå‹•çš„åˆ‡ã‚Šæ›¿ãˆï¼‰**

\# snn\_research/core/adaptive\_attention\_selector.py  
"""  
å­¦ç¿’ä¸­ã«å‹•çš„ã«Attentionã‚¿ã‚¤ãƒ—ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹  
"""

class AdaptiveAttentionModule(nn.Module):  
    """  
    å­¦ç¿’ã®é€²è¡Œã«å¿œã˜ã¦ã€Standardã¨SDSAã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹  
    """  
    def \_\_init\_\_(self, d\_model, nhead):  
        super().\_\_init\_\_()

        \# ä¸¡æ–¹ã®Attentionã‚’ä¿æŒ  
        self.standard\_attn \= StandardAttention(d\_model, nhead)  
        self.sdsa\_attn \= SDSA\_Module(d\_model, nhead)

        \# é¸æŠãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’å¯èƒ½ï¼‰  
        self.attention\_selector \= nn.Parameter(torch.tensor(0.0))  
        \# 0.0ã«è¿‘ã„ â†’ Standard, 1.0ã«è¿‘ã„ â†’ SDSA

    def forward(self, x, timesteps=4, training=True):  
        if training:  
            \# å­¦ç¿’ä¸­: ä¸¡æ–¹ã‚’è¨ˆç®—ã—ã€é‡ã¿ä»˜ã‘å’Œ  
            standard\_out \= self.standard\_attn(x)  
            sdsa\_out \= self.sdsa\_attn(x, timesteps)

            \# Gumbel-Softmax ã§å¾®åˆ†å¯èƒ½ãªé¸æŠ  
            alpha \= torch.sigmoid(self.attention\_selector)  
            output \= alpha \* sdsa\_out \+ (1 \- alpha) \* standard\_out

            return output  
        else:  
            \# æ¨è«–æ™‚: å­¦ç¿’ã•ã‚ŒãŸé¸æŠã«åŸºã¥ãä¸€æ–¹ã®ã¿å®Ÿè¡Œï¼ˆåŠ¹ç‡åŒ–ï¼‰  
            if torch.sigmoid(self.attention\_selector) \> 0.5:  
                return self.sdsa\_attn(x, timesteps)  
            else:  
                return self.standard\_attn(x)

    def get\_current\_preference(self):  
        """  
        ç¾åœ¨ã©ã¡ã‚‰ã®AttentionãŒå„ªå…ˆã•ã‚Œã¦ã„ã‚‹ã‹ã‚’è¿”ã™  
        """  
        alpha \= torch.sigmoid(self.attention\_selector).item()  
        return {  
            "sdsa\_weight": alpha,  
            "standard\_weight": 1 \- alpha,  
            "preferred": "SDSA" if alpha \> 0.5 else "Standard"  
        }

##### **Phase 3: å®Œå…¨SDSAå®Ÿè£…ï¼ˆç ”ç©¶æ®µéšï¼‰**

\# snn\_research/architectures/full\_sdsa\_transformer.py  
"""  
å®Œå…¨ã«SDSAãƒ™ãƒ¼ã‚¹ã®Transformerï¼ˆæœ€çµ‚ç›®æ¨™ï¼‰  
"""

class FullSDSATransformer(nn.Module):  
    """  
    ã™ã¹ã¦ã®å±¤ã§SDSAã‚’ä½¿ç”¨

    âš ï¸ è­¦å‘Š: ã“ã‚Œã¯ç ”ç©¶æ®µéšã®å®Ÿè£…  
    \- å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹å¯èƒ½æ€§ãŒé«˜ã„  
    \- å¾“æ¥Transformerã‚ˆã‚Šç²¾åº¦ãŒä½ä¸‹ã™ã‚‹å¯èƒ½æ€§  
    \- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ¥µã‚ã¦é‡è¦  
    """  
    def \_\_init\_\_(self, d\_model=512, nhead=8, num\_layers=6, timesteps=10):  
        super().\_\_init\_\_()

        self.timesteps \= timesteps

        \# å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯åŒ–ï¼‰  
        self.input\_encoder \= SpikeEncoder(d\_model)

        \# SDSAå±¤ã®ã‚¹ã‚¿ãƒƒã‚¯  
        self.layers \= nn.ModuleList(\[  
            SDSAEncoderLayer(  
                d\_model=d\_model,  
                nhead=nhead,  
                timesteps=timesteps,  
                \# å±¤ã”ã¨ã«ç•°ãªã‚‹é–¾å€¤ã‚’è¨­å®šï¼ˆé‡è¦ï¼ï¼‰  
                threshold=self.\_compute\_layer\_threshold(i, num\_layers)  
            )  
            for i in range(num\_layers)  
        \])

        \# å‡ºåŠ›ãƒ‡ã‚³ãƒ¼ãƒ€ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯ã‚’é€£ç¶šå€¤ã«å¤‰æ›ï¼‰  
        self.output\_decoder \= SpikeDecoder(d\_model)

    def \_compute\_layer\_threshold(self, layer\_idx, total\_layers):  
        """  
        å±¤ã®æ·±ã•ã«å¿œã˜ã¦é–¾å€¤ã‚’èª¿æ•´

        æˆ¦ç•¥: æ·±ã„å±¤ã»ã©é–¾å€¤ã‚’ä½ãã—ã€ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å‡ºã‚„ã™ãã™ã‚‹  
        ï¼ˆå‹¾é…æ¶ˆå¤±ã‚’é˜²ããŸã‚ï¼‰  
        """  
        \# ç·šå½¢æ¸›è¡°  
        base\_threshold \= 1.0  
        decay\_rate \= 0.1  
        return base\_threshold \- (decay\_rate \* layer\_idx / total\_layers)

    def forward(self, x):  
        """  
        âš ï¸ é‡è¦: å­¦ç¿’å®‰å®šåŒ–ã®ãŸã‚ã®ç‰¹åˆ¥ãªå‡¦ç†  
        """  
        \# ã‚¹ãƒ‘ã‚¤ã‚¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°  
        x \= self.input\_encoder(x)

        \# å„å±¤ã‚’é€šé  
        for i, layer in enumerate(self.layers):  
            \# å±¤ã”ã¨ã®å‡ºåŠ›ã‚’ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°  
            x\_prev \= x.clone()  
            x \= layer(x, timesteps=self.timesteps)

            \# âš ï¸ å®‰å…¨è£…ç½®: ã‚¼ãƒ­å‡ºåŠ›ã®æ¤œå‡º  
            if torch.all(x \== 0):  
                print(f"âš ï¸ Layer {i}: All-zero output detected\!")  
                \# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å‰ã®å±¤ã®å‡ºåŠ›ã‚’ä½¿ç”¨  
                x \= x\_prev  
                \# ã¾ãŸã¯: ãƒã‚¤ã‚ºæ³¨å…¥  
                \# x \= x \+ torch.randn\_like(x) \* 0.01

        \# ãƒ‡ã‚³ãƒ¼ãƒ‰  
        output \= self.output\_decoder(x)

        return output

    def diagnose\_learning\_issues(self, dataloader):  
        """  
        å­¦ç¿’ã®å•é¡Œã‚’è¨ºæ–­ã™ã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£  
        """  
        self.eval()  
        diagnostics \= {  
            "layer\_spike\_rates": \[\],  
            "layer\_gradient\_norms": \[\],  
            "attention\_pattern\_diversity": \[\]  
        }

        with torch.no\_grad():  
            for batch in dataloader:  
                x \= batch\[0\]

                \# å„å±¤ã®ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã‚’æ¸¬å®š  
                for i, layer in enumerate(self.layers):  
                    spike\_rate \= self.\_measure\_spike\_rate(layer, x)  
                    diagnostics\["layer\_spike\_rates"\].append({  
                        "layer": i,  
                        "spike\_rate": spike\_rate  
                    })

                break  \# 1ãƒãƒƒãƒã®ã¿

        \# è¨ºæ–­çµæœã‚’è¡¨ç¤º  
        print("\\n=== SDSA Learning Diagnostics \===")  
        for i, rate in enumerate(diagnostics\["layer\_spike\_rates"\]):  
            print(f"Layer {i}: Spike Rate \= {rate\['spike\_rate'\]:.2%}")  
            if rate\['spike\_rate'\] \< 0.05:  
                print(f"  âš ï¸ Warning: Very low spike rate (dead neurons)")  
            elif rate\['spike\_rate'\] \> 0.95:  
                print(f"  âš ï¸ Warning: Very high spike rate (over-excitation)")

        return diagnostics

#### **å®Ÿè·µçš„ãªçµ±åˆã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**

##### **çµ±åˆã®æ¨å¥¨ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**

\# integration\_roadmap.yaml  
\# BIFã¨SDSAã®çµ±åˆè¨ˆç”»

Phase 1: åŸºç¤æ¤œè¨¼ï¼ˆ1-2ãƒ¶æœˆï¼‰  
  tasks:  
    \- BIFå˜ä½“ãƒ†ã‚¹ãƒˆ:  
        \- å˜å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§LIFã¨æ¯”è¼ƒ  
        \- MNIST/CIFAR-10ã§ã®ç²¾åº¦æ¸¬å®š  
        \- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ„Ÿåº¦åˆ†æ  
        expected\_outcome: "BIFãŒæœ‰åŠ¹ãªã‚±ãƒ¼ã‚¹ã¨ç„¡åŠ¹ãªã‚±ãƒ¼ã‚¹ã®ç‰¹å®š"

    \- SDSAå˜ä½“ãƒ†ã‚¹ãƒˆ:  
        \- å˜ä¸€Attentionå±¤ã§Standard Attentionã¨æ¯”è¼ƒ  
        \- ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®å½±éŸ¿èª¿æŸ»  
        \- ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸¬å®š  
        expected\_outcome: "SDSAã®ç²¾åº¦vsã‚¨ãƒãƒ«ã‚®ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•æŠŠæ¡"

  success\_criteria:  
    \- BIF: å°‘ãªãã¨ã‚‚1ã¤ã®ã‚¿ã‚¹ã‚¯ã§LIFä»¥ä¸Šã®æ€§èƒ½  
    \- SDSA: ã‚¨ãƒãƒ«ã‚®ãƒ¼å‰Šæ¸›ãŒ50%ä»¥ä¸Šã€ç²¾åº¦ä½ä¸‹ãŒ5%ä»¥å†…

Phase 2: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆï¼ˆ2-3ãƒ¶æœˆï¼‰  
  tasks:  
    \- HybridSpikingCNNå®Ÿè£…:  
        \- ä¸€éƒ¨ã®å±¤ã®ã¿BIFåŒ–  
        \- æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰

    \- HybridAttentionTransformerå®Ÿè£…:  
        \- ä¸€éƒ¨ã®å±¤ã®ã¿SDSAåŒ–  
        \- å±¤ã”ã¨ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ»ç²¾åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

    \- è‡ªå‹•é¸æŠã‚·ã‚¹ãƒ†ãƒ :  
        \- AdaptiveNeuronSelectorå®Ÿè£…  
        \- AdaptiveAttentionModuleå®Ÿè£…

  success\_criteria:  
    \- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ãŒç´”ç²‹LIF/Standard Attentionã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½  
    \- ã‚¨ãƒãƒ«ã‚®ãƒ¼å‰Šæ¸›ãŒ20%ä»¥ä¸Š

Phase 3: å®Œå…¨çµ±åˆï¼ˆ3-6ãƒ¶æœˆï¼‰  
  tasks:  
    \- FullBIF/FullSDSAãƒ¢ãƒ‡ãƒ«å®Ÿè£…  
    \- å¤§è¦æ¨¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆImageNet, GLUEç­‰ï¼‰  
    \- è«–æ–‡åŸ·ç­†

  success\_criteria:  
    \- è«–æ–‡æŠ•ç¨¿å¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã®å®Ÿé¨“çµæœ  
    \- æ—¢å­˜ç ”ç©¶ã¨ã®æ¯”è¼ƒã§å„ªä½æ€§ã‚’ç¤ºã™

##### **å®Ÿè£…ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**

###### **BIFçµ±åˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**

**å®Ÿè£…å‰**

* \[ \] æ—¢å­˜LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‹•ä½œã‚’å®Œå…¨ã«ç†è§£  
* \[ \] BIFã®æ•°å­¦çš„å®šç¾©ã‚’è«–æ–‡ã§ç¢ºèª  
* \[ \] åŒå®‰å®šæ€§ã®æ¡ä»¶ï¼ˆv\_reset \> âˆš|b|ï¼‰ã‚’ç†è§£

**å®Ÿè£…ä¸­**

* \[ \] å˜ä½“ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ä½œæˆï¼ˆXORã‚²ãƒ¼ãƒˆãªã©ï¼‰  
* \[ \] åˆæœŸåŒ–æˆ¦ç•¥ã®å®Ÿè£…ï¼ˆnear\_unstable\_equilibriumï¼‰  
* \[ \] å®‰å…¨è£…ç½®ã®å®Ÿè£…ï¼ˆæš´èµ°æ¤œå‡ºã€ã‚¼ãƒ­ã‚¹ãƒ‘ã‚¤ã‚¯æ¤œå‡ºï¼‰  
* \[ \] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¢ç´¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

**å®Ÿè£…å¾Œ**

* \[ \] LIFã¨ã®æ€§èƒ½æ¯”è¼ƒï¼ˆæœ€ä½3ã¤ã®ã‚¿ã‚¹ã‚¯ï¼‰  
* \[ \] ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸¬å®š  
* \[ \] å¯è¦–åŒ–ãƒ„ãƒ¼ãƒ«ï¼ˆè†œé›»ä½ã®æ™‚ç³»åˆ—ãƒ—ãƒ­ãƒƒãƒˆï¼‰  
* \[ \] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ

###### **SDSAçµ±åˆãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ**

**å®Ÿè£…å‰**

* \[ \] å¾“æ¥Attentionã®è¨ˆç®—è¤‡é›‘åº¦ã‚’æ¸¬å®š  
* \[ \] SDSAã®è«–æ–‡å®Ÿè£…ï¼ˆGitHubï¼‰ã‚’èª¿æŸ»  
* \[ \] Softmaxé™¤å»ã®å½±éŸ¿ã‚’ç†è§£

**å®Ÿè£…ä¸­**

* \[ \] ãƒã‚¤ãƒŠãƒªãƒã‚¹ã‚¯æ“ä½œã®å®Ÿè£…  
* \[ \] ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ãƒ«ãƒ¼ãƒ—ã®æœ€é©åŒ–  
* \[ \] Residualæ¥ç¶šã®é©åˆ‡ãªå‡¦ç†  
* \[ \] ä»£ç†å‹¾é…é–¢æ•°ã®é¸æŠ

**å®Ÿè£…å¾Œ**

* \[ \] Standard Attentionã¨ã®ç²¾åº¦æ¯”è¼ƒ  
* \[ \] ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¸¬å®šï¼ˆFLOPs, å®Ÿæ¸¬å€¤ï¼‰  
* \[ \] Attentionå¯è¦–åŒ–ï¼ˆã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«æ³¨ç›®ã—ã¦ã„ã‚‹ã‹ï¼‰  
* \[ \] ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—æ•°ã®å½±éŸ¿èª¿æŸ»ãƒ¬ãƒãƒ¼ãƒˆ

#### **ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®ç·æ‹¬**

**BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³**

| å´é¢ | åˆ©ç‚¹ | æ¬ ç‚¹ |
| :---- | :---- | :---- |
| è¡¨ç¾åŠ› | åŒå®‰å®šæ€§ã«ã‚ˆã‚‹è±Šã‹ãªãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ | åˆæœŸæ¡ä»¶ä¾å­˜ã§äºˆæ¸¬å›°é›£ |
| å­¦ç¿’ | æ–°ã—ã„è¨ˆç®—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯èƒ½æ€§ | ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ãŒåºƒå¤§ |
| å®Ÿè£… | \- | LIFã‚ˆã‚Šè¤‡é›‘ã€ãƒ‡ãƒãƒƒã‚°å›°é›£ |
| æ€§èƒ½ | ã‚¿ã‚¹ã‚¯ä¾å­˜ã§æ™‚ã«å„ªã‚Œã‚‹ | å¿…ãšã—ã‚‚LIFã‚’ä¸Šå›ã‚‰ãªã„ |
| æ¨å¥¨åº¦ | â­â­â­ (å®Ÿé¨“çš„ä¾¡å€¤ã‚ã‚Š) | æœ¬ç•ªåˆ©ç”¨ã¯æ™‚æœŸå°šæ—© |

**SDSA (Spike-Driven Self-Attention)**

| å´é¢ | åˆ©ç‚¹ | æ¬ ç‚¹ |
| :---- | :---- | :---- |
| ã‚¨ãƒãƒ«ã‚®ãƒ¼ | æœ€å¤§87å€å‰Šæ¸› | ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—å¢—ã§ç›¸æ®ºã•ã‚Œã‚‹ |
| ç²¾åº¦ | \- | å¾“æ¥Transformerã‚ˆã‚Š5-10%ä½ä¸‹ |
| å­¦ç¿’ | \- | å‹¾é…æ¶ˆå¤±ãƒ»çˆ†ç™ºãŒèµ·ãã‚„ã™ã„ |
| å®Ÿè£… | \- | PyTorchæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒä½¿ãˆãªã„ |
| æ¨å¥¨åº¦ | â­â­â­â­ (å°†æ¥æ€§ã‚ã‚Š) | ç¾æ™‚ç‚¹ã§ã¯ç ”ç©¶æ®µéš |

#### **ç§ã®æœ€çµ‚æ¨å¥¨: æ®µéšçš„ã‹ã¤æ…é‡ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**

**ã™ãã«å®Ÿè£…ã™ã¹ãï¼ˆä½ãƒªã‚¹ã‚¯ãƒ»é«˜ãƒªã‚¿ãƒ¼ãƒ³ï¼‰:**

* **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ§‹ç¯‰**  
  \# æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¸ã®è¿½åŠ ï¼ˆãƒªã‚¹ã‚¯æœ€å°ï¼‰  
     configs/models/hybrid\_neuron.yaml  
     configs/models/hybrid\_attention.yaml

* **æ€§èƒ½ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ **  
  snn\_research/utils/performance\_monitor.py  
     \# ã‚¨ãƒãƒ«ã‚®ãƒ¼ã€ç²¾åº¦ã€ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã‚’è‡ªå‹•è¨˜éŒ²

**å®Ÿé¨“çš„ã«è©¦ã™ã¹ãï¼ˆä¸­ãƒªã‚¹ã‚¯ãƒ»ä¸­ãƒªã‚¿ãƒ¼ãƒ³ï¼‰:**

* **BIFå˜ä½“å®Ÿé¨“**  
  experiments/bif\_vs\_lif\_comparison.py  
     \# å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆMNISTï¼‰ã§æ¯”è¼ƒ

* **SDSAå˜å±¤ãƒ†ã‚¹ãƒˆ**  
  experiments/sdsa\_single\_layer\_test.py  
     \# 1å±¤ã®ã¿SDSAã«ç½®ãæ›ãˆã¦åŠ¹æœæ¸¬å®š

**é•·æœŸçš„ã«æ¤œè¨ã™ã¹ãï¼ˆé«˜ãƒªã‚¹ã‚¯ãƒ»é«˜ãƒªã‚¿ãƒ¼ãƒ³ï¼‰:**

* **å®Œå…¨BIF/SDSAã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**  
  * ã¾ãšã¯è«–æ–‡ã‚’åŸ·ç­†å¯èƒ½ãªãƒ¬ãƒ™ãƒ«ã¾ã§å®Ÿé¨“  
  * å®‰å®šæ€§ãŒç¢ºèªã§ãã¦ã‹ã‚‰æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«çµ±åˆ

#### **é‡è¦ãªæ•™è¨“**

ç‰©ä½“æ¤œå‡ºã‚¿ã‚¹ã‚¯ã§ã¯ã€æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ãªIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒæœ€é«˜ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸ ScienceDirectã€‚ã“ã‚Œã¯ã€ã€Œè¤‡é›‘ã• â‰  æ€§èƒ½ã€ ã¨ã„ã†é‡è¦ãªäº‹å®Ÿã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

#### **çµè«–:**

* æœ€æ–°æŠ€è¡“ã¯é­…åŠ›çš„ã ãŒã€æ—¢å­˜ã®å®Ÿè£…ï¼ˆLIFã€Standard Attentionï¼‰ã‚‚ååˆ†ã«å¼·åŠ›  
* ã¾ãšæ—¢å­˜æŠ€è¡“ã§é”æˆå¯èƒ½ãªæœ€é«˜æ€§èƒ½ã‚’å¼•ãå‡ºã™  
* ãã®ä¸Šã§ã€æ˜ç¢ºãªåˆ©ç‚¹ï¼ˆã‚¨ãƒãƒ«ã‚®ãƒ¼ã€é€Ÿåº¦ã€ç²¾åº¦ã®ã„ãšã‚Œã‹ï¼‰ãŒè¦‹è¾¼ã‚ã‚‹å ´åˆã®ã¿ã€æ–°æŠ€è¡“ã‚’æ®µéšçš„ã«å°å…¥

ã“ã‚ŒãŒã€å­¦è¡“çš„ä¿¡é ¼æ€§ã‚’ä¿ã¡ãªãŒã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰é€²ã•ã›ã‚‹æœ€ã‚‚å …å®Ÿãªæˆ¦ç•¥ã§ã™ã€‚