# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/learning_rules/causal_trace.py
# ã‚³ãƒ¼ãƒ‰ã®æœ€ã‚‚æœ€åˆã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¤ºã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã€æ©Ÿèƒ½ã®èª¬æ˜ã‚’è©³ç´°ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ ä¿®æ­£å†…å®¹ã¯è¨˜è¼‰ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
# Title: é€²åŒ–ç‰ˆ å› æœè¿½è·¡ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²ã‚Šå½“ã¦å­¦ç¿’å‰‡ (V2)
# Description:
# - CausalTraceCreditAssignmentEnhanced ã‚’åŸºç›¤ã¨ã—ã€ã•ã‚‰ãªã‚‹æ©Ÿèƒ½å‘ä¸Šã‚’ç›®æŒ‡ã—ãŸå®Ÿè£…
# - æ–‡è„ˆä¾å­˜ã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå¤‰èª¿ã€ãƒ‘ã‚¹ä¾å­˜æ€§ï¼ˆç°¡æ˜“ç‰ˆï¼‰ã€ç«¶åˆçš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå‰²ã‚Šå½“ã¦ã€é«˜ãƒ¬ãƒ™ãƒ«å› æœé€£æºã®æ¦‚å¿µã‚’å–ã‚Šå…¥ã‚Œã‚‹

import torch
from typing import Dict, Any, Optional, Tuple
import math

from .reward_modulated_stdp import RewardModulatedSTDP

class CausalTraceCreditAssignmentEnhancedV2(RewardModulatedSTDP):
    """
    æ–‡è„ˆå¤‰èª¿ã€ç«¶åˆã€é«˜ãƒ¬ãƒ™ãƒ«å› æœé€£æºã‚’å°å…¥ã—ãŸã€ã•ã‚‰ã«é€²åŒ–ã—ãŸå› æœå­¦ç¿’å‰‡ã€‚
    """
    def __init__(self, learning_rate: float, a_plus: float, a_minus: float,
                 tau_trace: float, tau_eligibility: float, dt: float = 1.0,
                 credit_time_decay: float = 0.95,
                 dynamic_lr_factor: float = 2.0,
                 modulate_eligibility_tau: bool = True, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹åŒ–
                 min_eligibility_tau: float = 10.0,
                 max_eligibility_tau: float = 200.0,
                 # --- â–¼ V2 æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â–¼ ---
                 context_modulation_strength: float = 0.5, # æ–‡è„ˆã«ã‚ˆã‚‹å¤‰èª¿å¼·åº¦
                 competition_k_ratio: float = 0.1,        # ç«¶åˆã§æ›´æ–°ã‚’é©ç”¨ã™ã‚‹ã‚·ãƒŠãƒ—ã‚¹ã®å‰²åˆ (ä¸Šä½10%)
                 rule_based_lr_factor: float = 3.0       # é«˜ãƒ¬ãƒ™ãƒ«ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹å­¦ç¿’ç‡å¢—åŠ ä¿‚æ•°
                 # --- â–² V2 æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ â–² ---
                 ):
        super().__init__(learning_rate, a_plus, a_minus, tau_trace, tau_eligibility, dt)
        self.causal_contribution: Optional[torch.Tensor] = None
        self.base_learning_rate = learning_rate
        self.credit_time_decay = credit_time_decay # Note: V2ã§ã¯ç›´æ¥ä½¿ã‚ãšã€tau_eligibilityå¤‰èª¿ã§ä»£æ›¿
        self.dynamic_lr_factor = dynamic_lr_factor
        self.modulate_eligibility_tau = modulate_eligibility_tau
        self.min_eligibility_tau = min_eligibility_tau
        self.max_eligibility_tau = max_eligibility_tau
        self.base_tau_eligibility = tau_eligibility
        # --- â–¼ V2 æ–°å±æ€§ â–¼ ---
        self.context_modulation_strength = context_modulation_strength
        self.competition_k_ratio = competition_k_ratio
        self.rule_based_lr_factor = rule_based_lr_factor
        # --- â–² V2 æ–°å±æ€§ â–² ---
        print("ğŸ§  V2 Enhanced Causal Trace Credit Assignment rule initialized.")
        print(f"   - Context Modulation Strength: {self.context_modulation_strength}")
        print(f"   - Competition Ratio (Top K%): {self.competition_k_ratio * 100:.1f}%")
        print(f"   - Rule-based LR Factor: {self.rule_based_lr_factor}")

    def _initialize_contribution_trace(self, weight_shape: tuple, device: torch.device):
        """å› æœçš„è²¢çŒ®åº¦ã‚’è¨˜éŒ²ã™ã‚‹ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚"""
        # (å¤‰æ›´ãªã—)
        self.causal_contribution = torch.zeros(weight_shape, device=device)

    def _apply_context_modulation(self, backward_credit: torch.Tensor, optional_params: Dict[str, Any]) -> torch.Tensor:
        """Global Workspace ã‚„ Memory ã‹ã‚‰ã®æ–‡è„ˆæƒ…å ±ã§ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’å¤‰èª¿ã™ã‚‹ã€‚"""
        modulated_credit = backward_credit.clone() # å…ƒã®ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’ã‚³ãƒ”ãƒ¼

        workspace_context = optional_params.get("global_workspace_context") # {'type': 'emotion', 'valence': -0.8,...} ãªã©
        memory_context = optional_params.get("memory_context") # é–¢é€£ã™ã‚‹éå»ã®è¨˜æ†¶æƒ…å ±ãªã©

        modulation_factor = 1.0

        # ä¾‹: ãƒã‚¬ãƒ†ã‚£ãƒ–ãªæ„Ÿæƒ…ãŒå¼·ã„å ´åˆã€é–¢é€£ã™ã‚‹ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å¢—å¹…ï¼ˆå›é¿å­¦ç¿’ã®ãŸã‚ï¼‰
        if workspace_context and isinstance(workspace_context, dict) and workspace_context.get("type") == "emotion":
            valence = workspace_context.get("valence", 0.0)
            if valence < -0.5: # å¼·ã„ãƒã‚¬ãƒ†ã‚£ãƒ–æ„Ÿæƒ…
                modulation_factor += self.context_modulation_strength * abs(valence)

        # ä¾‹: é¡ä¼¼ã—ãŸå¤±æ•—è¨˜æ†¶ãŒæƒ³èµ·ã•ã‚ŒãŸå ´åˆã€é–¢é€£ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚’å¢—å¹…
        if memory_context and isinstance(memory_context, list) and len(memory_context) > 0:
            # memory_context ã«å¤±æ•—çµŒé¨“ãŒå«ã¾ã‚Œã‚‹ã‹ãƒã‚§ãƒƒã‚¯ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ (ç°¡æ˜“ç‰ˆ)
            if any("FAILURE" in str(mem.get("result")) for mem in memory_context):
                 modulation_factor += self.context_modulation_strength * 0.5 # å¤±æ•—è¨˜æ†¶ã«ã‚ˆã‚‹å¢—å¹…

        return modulated_credit * modulation_factor

    def _apply_competition(self, dw: torch.Tensor, eligibility_trace: torch.Tensor) -> torch.Tensor:
        """ç«¶åˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é©ç”¨ã—ã€æ›´æ–°å¯¾è±¡ã®ã‚·ãƒŠãƒ—ã‚¹ã‚’é¸æŠã™ã‚‹ã€‚"""
        if self.competition_k_ratio >= 1.0: # ç«¶åˆãªã—
            return dw

        num_synapses = dw.numel()
        k = max(1, int(num_synapses * self.competition_k_ratio))

        # é©æ ¼åº¦ãƒˆãƒ¬ãƒ¼ã‚¹ã®çµ¶å¯¾å€¤ãŒå¤§ãã„ä¸Šä½kå€‹ã®ã‚·ãƒŠãƒ—ã‚¹ã‚’é¸æŠ
        # eligibility_trace ã¯å­¦ç¿’ã®ã€Œãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã€ã‚’ç¤ºã™ãŸã‚ã€dwè‡ªä½“ã‚ˆã‚Šé©åˆ‡
        abs_eligibility = torch.abs(eligibility_trace)
        top_k_values, _ = torch.topk(abs_eligibility.view(-1), k)
        threshold = top_k_values[-1] # ä¸Šä½kå€‹ã®æœ€å°å€¤

        # é–¾å€¤ã‚ˆã‚Šå°ã•ã„æ›´æ–°ã¯ã‚¼ãƒ­ã«ã™ã‚‹ãƒã‚¹ã‚¯ã‚’ä½œæˆ
        mask = abs_eligibility >= threshold

        return dw * mask.float() # ãƒã‚¹ã‚¯ã‚’é©ç”¨

    def _apply_high_level_rules(self, dynamic_lr: torch.Tensor, optional_params: Dict[str, Any], weights: torch.Tensor) -> torch.Tensor:
        """CausalInferenceEngineã‹ã‚‰ã®æŠ½è±¡ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ãå­¦ç¿’ç‡ã‚’èª¿æ•´ã™ã‚‹ã€‚"""
        rule = optional_params.get("abstract_causal_rule") # ä¾‹: {'condition': 'context_A', 'cause': 'neuron_X', 'effect': 'neuron_Y', 'increase_lr': True}

        if rule and isinstance(rule, dict):
            # ã“ã®ãƒ«ãƒ¼ãƒ«ãŒç¾åœ¨ã®æ¥ç¶š (weights) ã«é–¢é€£ã™ã‚‹ã‹åˆ¤å®šã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦
            # (ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€ç‰¹å®šã®ãƒ«ãƒ¼ãƒ«ãŒæ¥ãŸã‚‰å…¨ä½“ã®å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹)
            if rule.get("increase_lr"):
                print(f"   - Applying high-level rule: Increasing LR for relevant synapses.")
                # æœ¬æ¥ã¯ãƒ«ãƒ¼ãƒ«ã«é–¢é€£ã™ã‚‹ã‚·ãƒŠãƒ—ã‚¹ã®ã¿ã‚’é¸æŠçš„ã«å¤‰æ›´ã™ã¹ã
                return dynamic_lr * self.rule_based_lr_factor

        return dynamic_lr


    def update(
        self,
        pre_spikes: torch.Tensor,
        post_spikes: torch.Tensor,
        weights: torch.Tensor,
        optional_params: Optional[Dict[str, Any]] = None
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        V2: æ–‡è„ˆå¤‰èª¿ã€ç«¶åˆã€ãƒ«ãƒ¼ãƒ«é€£æºã‚’å«ã‚€æ›´æ–°ãƒ—ãƒ­ã‚»ã‚¹ã€‚
        """
        if optional_params is None: optional_params = {}

        # --- 1. ãƒˆãƒ¬ãƒ¼ã‚¹åˆæœŸåŒ–ã¨æ›´æ–° ---
        if self.pre_trace is None or self.post_trace is None or self.pre_trace.shape[0] != pre_spikes.shape[0] or self.post_trace.shape[0] != post_spikes.shape[0]:
            self._initialize_traces(pre_spikes.shape[0], post_spikes.shape[0], pre_spikes.device)
        self._update_traces(pre_spikes, post_spikes)

        if self.eligibility_trace is None or self.eligibility_trace.shape != weights.shape:
            self._initialize_eligibility_trace(weights.shape, weights.device)

        if self.causal_contribution is None or self.causal_contribution.shape != weights.shape:
            self._initialize_contribution_trace(weights.shape, weights.device)

        assert self.pre_trace is not None and self.post_trace is not None and self.eligibility_trace is not None and self.causal_contribution is not None

        # --- 2. é©æ ¼åº¦ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ›´æ–° (æ™‚å®šæ•°å¤‰èª¿ä»˜ã) ---
        potential_dw = self.a_plus * torch.outer(post_spikes, self.pre_trace) - self.a_minus * torch.outer(pre_spikes, self.post_trace).T
        self.eligibility_trace += potential_dw

        # æ™‚å®šæ•°å¤‰èª¿ (Path Dependency ã®ç°¡æ˜“ç‰ˆ)
        if self.modulate_eligibility_tau:
            contrib_norm = torch.sigmoid(self.causal_contribution * 10 - 5)
            current_tau_eligibility = self.min_eligibility_tau + (self.max_eligibility_tau - self.min_eligibility_tau) * contrib_norm
            eligibility_decay = (self.eligibility_trace / current_tau_eligibility.clamp(min=1e-6)) * self.dt # ã‚¼ãƒ­é™¤ç®—é˜²æ­¢
        else:
            eligibility_decay = (self.eligibility_trace / self.base_tau_eligibility) * self.dt
        self.eligibility_trace -= eligibility_decay

        # --- 3. å ±é…¬/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã®å‡¦ç† ---
        reward = optional_params.get("reward", 0.0)
        causal_credit_signal = optional_params.get("causal_credit", 0.0)
        effective_reward_signal = reward + causal_credit_signal

        dw = torch.zeros_like(weights)
        if abs(effective_reward_signal) > 1e-6: # å ±é…¬/ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãŒã‚ã‚‹å ´åˆã®ã¿
            # --- 4. å‹•çš„å­¦ç¿’ç‡ã®è¨ˆç®— ---
            contrib_norm = torch.sigmoid(self.causal_contribution * 10 - 5)
            dynamic_lr = self.base_learning_rate * (1 + self.dynamic_lr_factor * contrib_norm)

            # --- 5. é«˜ãƒ¬ãƒ™ãƒ«ãƒ«ãƒ¼ãƒ«ã«ã‚ˆã‚‹å­¦ç¿’ç‡èª¿æ•´ ---
            dynamic_lr = self._apply_high_level_rules(dynamic_lr, optional_params, weights)

            # --- 6. é‡ã¿å¤‰åŒ–é‡ã®è¨ˆç®— ---
            dw = dynamic_lr * effective_reward_signal * self.eligibility_trace

            # --- 7. ç«¶åˆçš„å‰²ã‚Šå½“ã¦ ---
            dw = self._apply_competition(dw, self.eligibility_trace)

            # --- 8. é•·æœŸè²¢çŒ®åº¦ã®æ›´æ–° ---
            self.causal_contribution = self.causal_contribution * 0.99 + torch.abs(dw) * 0.01

            # --- 9. é©æ ¼åº¦ãƒˆãƒ¬ãƒ¼ã‚¹ã®ãƒªã‚»ãƒƒãƒˆ ---
            self.eligibility_trace *= 0.0

        # --- 10. ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã®é€†æ–¹å‘ä¼æ’­ (æ–‡è„ˆå¤‰èª¿ä»˜ã) ---
        if self.eligibility_trace is not None: # eligibility_trace ã¯æ›´æ–°å¾Œã‚‚å­˜åœ¨ã™ã‚‹
            # eligibility_trace ã¯ãã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã®ã€Œå¤‰åŒ–ã®å¯èƒ½æ€§ã€ã‚’ç¤ºã™
            # backward_credit ã¯ã€ãã®å¯èƒ½æ€§ãŒã©ã‚Œã ã‘å¾Œæ®µã®ä¿¡å· (reward/credit) ã«å½±éŸ¿ã‚’ä¸ãˆãŸã‹
            # effective_reward_signal ã‚’ä½¿ã†ã¹ãã‹ï¼Ÿ -> No, ãã‚Œã¯å±€æ‰€çš„ãªæ›´æ–°ã«ä½¿ã†
            # backward_credit ã¯ã€ã“ã®å±¤ã®ã€ŒçŠ¶æ…‹å¤‰åŒ– (eligibility)ã€ãŒå¾Œæ®µã«ã©ã‚Œã ã‘å½±éŸ¿ã—ã†ã‚‹ã‹ã€ã‚’ç¤ºã™ã¹ã
            # => eligibility_trace ã‚’ãã®ã¾ã¾ä½¿ã„ã€é‡ã¿ã§é€†ä¼æ’­ã•ã›ã‚‹ã®ãŒåŸºæœ¬

            credit_contribution = self.eligibility_trace # (N_post, N_pre)
            raw_backward_credit = torch.einsum('ij,ij->i', weights, credit_contribution) # (N_pre,)

            # æ–‡è„ˆå¤‰èª¿ã‚’é©ç”¨
            backward_credit = self._apply_context_modulation(raw_backward_credit, optional_params)

        else: # é€šå¸¸èµ·ã“ã‚‰ãªã„ã¯ãš
            backward_credit = torch.zeros_like(pre_spikes)

        return dw, backward_credit
        
    def get_causal_contribution(self) -> Optional[torch.Tensor]:
        """é•·æœŸçš„ãªå› æœçš„è²¢çŒ®åº¦ã‚’è¿”ã™ã€‚"""
        return self.causal_contribution
}
