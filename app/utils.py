# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: app/utils.py
# (Gradio 4.20.0 äº’æ›ä¿®æ­£ v3)
# Gradioã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
#
# æ©Ÿèƒ½:
# - ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³é–“ã§å…±æœ‰ã•ã‚Œã‚‹å®šæ•°ã‚„é–¢æ•°ã‚’å®šç¾©ã™ã‚‹ã€‚
# - å…±é€šã®Gradio UIãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹é–¢æ•°ã‚’æä¾›ã™ã‚‹ã€‚
# - ä¿®æ­£: `type="messages"` ã¨ `avatar_images` ã‚’å‰Šé™¤ã€‚
# - ä¿®æ­£: `queue=False` ã‚’ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚¹ãƒŠãƒ¼ã«è¿½åŠ ã€‚
# - ä¿®æ­£: `stream_fn` ã®å‹ãƒ’ãƒ³ãƒˆã‚’ `List[List[Optional[str]]]` ã«ä¿®æ­£ã€‚
# - ä¿®æ­£: `.queue().launch()` ã‚’ `.launch()` ã«ä¿®æ­£ã€‚
#
# ä¿®æ­£ (v4):
# - mypyã‚¨ãƒ©ãƒ¼ [import-untyped] ã‚’è§£æ¶ˆã€‚
#
# ä¿®æ­£ (v5):
# - å¾ªç’°ã‚¤ãƒ³ãƒãƒ¼ãƒˆè§£æ¶ˆã®ãŸã‚ã€collate_fn ã‚’ train.py ã‹ã‚‰ç§»å‹•ã€‚

import gradio as gr  # type: ignore[import-untyped]
# --- â–¼ ä¿®æ­£: Callable, Iterator, Tuple, List, Optional, Any ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import Callable, Iterator, Tuple, List, Optional, Any
# --- â–² ä¿®æ­£ â–² ---
import torch
# --- â–¼ ä¿®æ­£ (v5): å¿…è¦ãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ  â–¼ ---
from transformers import PreTrainedTokenizerBase
# --- â–² ä¿®æ­£ (v5) â–² ---


def get_auto_device() -> str:
    """å®Ÿè¡Œç’°å¢ƒã«æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’è‡ªå‹•çš„ã«é¸æŠã™ã‚‹ã€‚"""
    if torch.cuda.is_available(): return "cuda"
    if torch.backends.mps.is_available(): return "mps"
    return "cpu"

def get_avatar_svgs():
    """
    Gradioãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆç”¨ã®ã‚¢ãƒã‚¿ãƒ¼SVGã‚¢ã‚¤ã‚³ãƒ³ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã€‚
    (æ³¨: å¤ã„Gradioãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã¯ä½¿ç”¨ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚‹)
    """
    user_avatar_svg = r"""
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-user"><path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg>
    """
    assistant_avatar_svg = r"""
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-zap"><polygon points="13 2 3 14 12 14 11 22 21 10 12 10 13 2"></polygon></svg>
    """
    return user_avatar_svg, assistant_avatar_svg

# --- â–¼ ä¿®æ­£ (v5): collate_fn ã‚’ train.py ã‹ã‚‰ç§»å‹• â–¼ ---
def collate_fn(tokenizer: PreTrainedTokenizerBase, is_distillation: bool) -> Callable[[List[Any]], Any]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ç”¨ã® Collate é–¢æ•°ã€‚
    (train.py ã‹ã‚‰ app/utils.py ã«ç§»å‹•)
    """
    def collate(batch: List[Any]) -> Any:
        padding_val = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0
        inputs: List[torch.Tensor] = []
        targets: List[torch.Tensor] = []
        logits: List[torch.Tensor] = [] # Only used if is_distillation

        # Handle different batch item types (dict from HF, tuple from SNNBaseDataset)
        for item in batch:
            if isinstance(item, dict):
                # Ensure keys exist and are tensors or tensor-like
                inp = item.get('input_ids')
                tgt = item.get('labels') # Assuming 'labels' key
                if inp is None or tgt is None: continue # Skip invalid items
                inputs.append(torch.tensor(inp) if not isinstance(inp, torch.Tensor) else inp)
                targets.append(torch.tensor(tgt) if not isinstance(tgt, torch.Tensor) else tgt)
                if is_distillation:
                    lg = item.get('teacher_logits')
                    if lg is not None: logits.append(torch.tensor(lg) if not isinstance(lg, torch.Tensor) else lg)
                    else: logits.append(torch.empty(0)) # Placeholder if missing

            elif isinstance(item, tuple) and len(item) >= 2:
                # Ensure elements are tensors or tensor-like
                inp = item[0]
                tgt = item[1]
                if not isinstance(inp, (torch.Tensor, list, tuple)) or not isinstance(tgt, (torch.Tensor, list, tuple)): continue
                inputs.append(torch.tensor(inp) if not isinstance(inp, torch.Tensor) else inp)
                targets.append(torch.tensor(tgt) if not isinstance(tgt, torch.Tensor) else tgt)
                if is_distillation:
                    if len(item) >= 3:
                         lg = item[2]
                         if lg is not None: logits.append(torch.tensor(lg) if not isinstance(lg, torch.Tensor) else lg)
                         else: logits.append(torch.empty(0))
                    else: logits.append(torch.empty(0))
            else:
                print(f"Warning: Skipping unsupported batch item type: {type(item)}")
                continue # Skip unsupported item types

        if not inputs or not targets: # If batch becomes empty after filtering
            # Return empty structures that match expected types
            if is_distillation:
                return torch.empty((0, 0), dtype=torch.long), torch.empty((0, 0), dtype=torch.long), torch.empty((0, 0), dtype=torch.long), torch.empty((0, 0, 0), dtype=torch.float32)
            else:
                # è¾æ›¸å½¢å¼ã‚’è¿”ã™ (æ¨™æº–ã®collate_fnãŒæœŸå¾…ã™ã‚‹å½¢å¼)
                return {
                    "input_ids": torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=padding_val),
                    "attention_mask": torch.nn.utils.rnn.pad_sequence([torch.ones_like(i) for i in inputs], batch_first=True, padding_value=0),
                    "labels": torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)
                }

        # --- æ¨™æº– (éè’¸ç•™) ã® collate ãƒ­ã‚¸ãƒƒã‚¯ (è¾æ›¸ã‚’è¿”ã™) ---
        if not is_distillation:
            padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=padding_val)
            padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)
            attention_mask = torch.ones_like(padded_inputs)
            attention_mask[padded_inputs == padding_val] = 0
            return {
                "input_ids": padded_inputs,
                "attention_mask": attention_mask,
                "labels": padded_targets
            }
        
        # --- è’¸ç•™ (ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™) ---
        padded_inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=padding_val)
        padded_targets = torch.nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=-100)
        padded_logits = torch.nn.utils.rnn.pad_sequence(logits, batch_first=True, padding_value=0.0)
        attention_mask = torch.ones_like(padded_inputs)
        attention_mask[padded_inputs == padding_val] = 0
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãƒ­ã‚¸ãƒƒãƒˆã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’åˆã‚ã›ã‚‹
        seq_len = padded_inputs.shape[1]
        if padded_targets.shape[1] < seq_len:
            pad = torch.full((padded_targets.shape[0], seq_len - padded_targets.shape[1]), -100, dtype=padded_targets.dtype, device=padded_targets.device)
            padded_targets = torch.cat([padded_targets, pad], dim=1)
        if padded_logits.shape[1] < seq_len:
            pad = torch.full((padded_logits.shape[0], seq_len - padded_logits.shape[1], padded_logits.shape[2]), 0.0, dtype=padded_logits.dtype, device=padded_logits.device)
            padded_logits = torch.cat([padded_logits, pad], dim=1)
            
        return padded_inputs, attention_mask, padded_targets, padded_logits
    
    return collate
# --- â–² ä¿®æ­£ (v5) â–² ---


def build_gradio_ui(
    # --- â–¼ ä¿®æ­£: å‹ãƒ’ãƒ³ãƒˆã‚’ List[List[Optional[str]]] ã«å¤‰æ›´ â–¼ ---
    stream_fn: Callable[[str, List[List[Optional[str]]]], Iterator[Tuple[List[List[Optional[str]]], str]]],
    # --- â–² ä¿®æ­£ â–² ---
    title: str,
    description: str,
    chatbot_label: str,
    theme: gr.themes.Base
) -> gr.Blocks:
    """
    å…±é€šã®Gradio Blocks UIã‚’æ§‹ç¯‰ã™ã‚‹ (Gradio 4.20.0 äº’æ›)ã€‚

    Args:
        stream_fn: ãƒãƒ£ãƒƒãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã—ã€å¿œç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã™ã‚‹é–¢æ•°ã€‚
        title: UIã®ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã€‚
        description: UIã®èª¬æ˜æ–‡ã€‚
        chatbot_label: ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ©ãƒ™ãƒ«ã€‚
        theme:é©ç”¨ã™ã‚‹Gradioãƒ†ãƒ¼ãƒã€‚

    Returns:
        gr.Blocks: æ§‹ç¯‰ã•ã‚ŒãŸGradio UIã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    """
    # --- â–¼ ä¿®æ­£: ã‚¢ãƒã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ãªã„ â–¼ ---
    # user_avatar, assistant_avatar = get_avatar_svgs()
    # --- â–² ä¿®æ­£ â–² ---

    with gr.Blocks(theme=theme) as demo:
        gr.Markdown(f"# {title}\n{description}")
        
        initial_stats_md = """
        **Inference Time:** `N/A`
        **Tokens/Second:** `N/A`
        ---
        **Total Spikes:** `N/A`
        **Spikes/Second:** `N/A`
        """

        with gr.Row():
            with gr.Column(scale=2):
                # --- â–¼ ä¿®æ­£: type="messages" ã¨ avatar_images ã‚’å‰Šé™¤ â–¼ ---
                chatbot = gr.Chatbot(label=chatbot_label, height=500)
                # --- â–² ä¿®æ­£ â–² ---
            with gr.Column(scale=1):
                stats_display = gr.Markdown(value=initial_stats_md, label="ğŸ“Š Inference Stats")

        with gr.Row():
            msg_textbox = gr.Textbox(
                show_label=False,
                placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›...",
                container=False,
                scale=6,
            )
            submit_btn = gr.Button("Send", variant="primary", scale=1)
            clear_btn = gr.Button("Clear", scale=1)

        gr.Markdown("<footer><p>Â© 2025 SNN System Design Project. All rights reserved.</p></footer>")

        def clear_all():
            return [], "", initial_stats_md

        # `submit` ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®šç¾©
        submit_event = msg_textbox.submit(
            fn=stream_fn,
            inputs=[msg_textbox, chatbot],
            outputs=[chatbot, stats_display],
            queue=False # --- â–¼ ä¿®æ­£: queue=False ã‚’è¿½åŠ  â–¼ ---
        )
        # --- â–² ä¿®æ­£ â–² ---
        submit_event.then(fn=lambda: "", inputs=None, outputs=msg_textbox)
        
        button_submit_event = submit_btn.click(
            fn=stream_fn,
            inputs=[msg_textbox, chatbot],
            outputs=[chatbot, stats_display],
            queue=False # --- â–¼ ä¿®æ­£: queue=False ã‚’è¿½åŠ  â–¼ ---
        )
        # --- â–² ä¿®æ­£ â–² ---
        button_submit_event.then(fn=lambda: "", inputs=None, outputs=msg_textbox)

        # `clear` ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å®šç¾©
        clear_btn.click(
            fn=clear_all,
            inputs=None,
            outputs=[chatbot, msg_textbox, stats_display],
            queue=False
        )
    
    return demo
