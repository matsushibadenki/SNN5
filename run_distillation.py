# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: run_distillation.py
# ã‚³ãƒ¼ãƒ‰ã®æœ€ã‚‚æœ€åˆã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¤ºã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã€æ©Ÿèƒ½ã®èª¬æ˜ã‚’è©³ç´°ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ ä¿®æ­£å†…å®¹ã¯è¨˜è¼‰ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
# Title: çŸ¥è­˜è’¸ç•™å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: KnowledgeDistillationManagerã‚’ä½¿ç”¨ã—ã¦ã€çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ã€‚
#              è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
#              mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: Containerã‚’TrainingContainerã«ä¿®æ­£ã€‚
# æ”¹å–„ç‚¹: argparseã‚’è¿½åŠ ã—ã€asyncio.runã§å®Ÿè¡Œã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚
# æ”¹å–„ç‚¹(snn_4_ann_parity_plan):
# - ANNæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã€AutoModelForCausalLMã®ä»£ã‚ã‚Šã«å…·ä½“çš„ãªANNBaselineModelã‚’
#   ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã€ã‚ˆã‚Šç®¡ç†ã•ã‚ŒãŸè’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿç¾ã€‚
# æ”¹å–„ç‚¹(v2): torchvisionã®ãƒ¢ãƒ‡ãƒ«ã‚’æ•™å¸«ã¨ã—ã¦ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾å¿œã€‚
# æ”¹å–„ç‚¹(v3): ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã—ã€å­¦ç¿’ã‚’ä¿ƒé€²ã€‚
#
# ä¿®æ­£ (v4): HPO (run_hpo.py) ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ã‚ˆã†ã«ã€--override_config å¼•æ•°ã‚’
#             å—ã‘å–ã‚Œã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚

import argparse
import asyncio
import torch
import torchvision.models as models  # type: ignore[import-untyped]
from torch.utils.data import DataLoader
# --- â–¼ ä¿®æ­£: HPOé€£æºã®ãŸã‚ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from omegaconf import OmegaConf, DictConfig
from typing import Any, List, Optional
# --- â–² ä¿®æ­£ â–² ---

from app.containers import TrainingContainer
from snn_research.distillation.knowledge_distillation_manager import KnowledgeDistillationManager
from snn_research.benchmark import TASK_REGISTRY


async def main() -> None:
    parser = argparse.ArgumentParser(description="SNN Knowledge Distillation Runner")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="Base config file path")
    parser.add_argument("--model_config", type=str, default="configs/cifar10_spikingcnn_config.yaml", help="SNN model architecture config file path")
    parser.add_argument("--task", type=str, default="cifar10", help="The benchmark task to distill.")
    parser.add_argument("--teacher_model", type=str, default="resnet18", help="The torchvision teacher model to use.")
    parser.add_argument("--epochs", type=int, default=15, help="Number of distillation epochs.")
    # --- â–¼ ä¿®æ­£: HPOé€£æºã®ãŸã‚ --override_config ã‚’è¿½åŠ  â–¼ ---
    parser.add_argument(
        "--override_config",
        type=str,
        action='append',
        help="Override config (e.g., 'training.epochs=5')"
    )
    # --- â–² ä¿®æ­£ â–² ---
    args = parser.parse_args()

    # DIã‚³ãƒ³ãƒ†ãƒŠã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    container = TrainingContainer()
    container.config.from_yaml(args.config)
    container.config.from_yaml(args.model_config)
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¸Šæ›¸ã
    container.config.training.epochs.from_value(args.epochs)
    
    # --- â–¼ ä¿®æ­£: HPOé€£æºã®ãŸã‚ --override_config ã‚’é©ç”¨ â–¼ ---
    if args.override_config:
        print(f"Applying {len(args.override_config)} overrides from command line...")
        for override in args.override_config:
            try:
                keys, value_str = override.split('=', 1)
                # å‹ã‚’æ¨è«–
                value: Any
                try:
                    value = int(value_str)
                except ValueError:
                    try:
                        value = float(value_str)
                    except ValueError:
                        if value_str.lower() == 'true':
                            value = True
                        elif value_str.lower() == 'false':
                            value = False
                        else:
                            value = value_str  # æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒ

                OmegaConf.update(container.config(), keys, value, merge=True)
                print(f"  - Applied: {keys} = {value}")
            except Exception as e:
                print(f"Error applying override '{override}': {e}")
    # --- â–² ä¿®æ­£ â–² ---

    # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ­£ã—ã„é †åºã§å–å¾—ãƒ»æ§‹ç¯‰
    device = container.device()
    # vocab_sizeã¯ç”»åƒã‚¿ã‚¹ã‚¯ã§ã¯ã‚¯ãƒ©ã‚¹æ•°ã¨ã—ã¦ä½¿ç”¨
    student_model = container.snn_model(vocab_size=10).to(device)
    optimizer = container.optimizer(params=student_model.parameters())
    scheduler = container.scheduler(optimizer=optimizer) if container.config.training.gradient_based.use_scheduler() else None

    # --- æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ ---
    print(f"ğŸ§  Initializing ANN teacher model ({args.teacher_model})...")
    if args.teacher_model == "resnet18":
        teacher_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        # CIFAR-10ç”¨ã«æœ€çµ‚å±¤ã‚’å¤‰æ›´
        num_ftrs = teacher_model.fc.in_features
        teacher_model.fc = torch.nn.Linear(num_ftrs, 10)
    else:
        raise ValueError(f"Unsupported teacher model: {args.teacher_model}")
    teacher_model = teacher_model.to(device)
    teacher_model.eval()

    distillation_trainer = container.distillation_trainer(
        model=student_model,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        rank=-1
    )
    model_registry = container.model_registry()

    manager = KnowledgeDistillationManager(
        student_model=student_model,
        teacher_model=teacher_model,
        trainer=distillation_trainer,
        tokenizer_name=container.config.data.tokenizer_name(), # tokenizerã¯CIFARã‚¿ã‚¹ã‚¯ã§ã¯ä½¿ã‚ã‚Œãªã„ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãŸã‚æ¸¡ã™
        model_registry=model_registry,
        device=device,
        config=container.config() # ä¿®æ­£: config ã‚’æ¸¡ã™
    )

    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ---
    TaskClass = TASK_REGISTRY.get(args.task)
    if not TaskClass:
        raise ValueError(f"Task '{args.task}' not found.")
    task = TaskClass(tokenizer=container.tokenizer.provided, device=device, hardware_profile={})
    train_dataset, val_dataset = task.prepare_data()

    # çŸ¥è­˜è’¸ç•™ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—
    train_loader, val_loader = manager.prepare_dataset(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        collate_fn=task.get_collate_fn(),
        batch_size=container.config.training.batch_size()
    )

    # è’¸ç•™ã®å®Ÿè¡Œ
    await manager.run_distillation(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=container.config.training.epochs(), # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å–å¾—
        model_id=f"{args.task}_distilled_from_{args.teacher_model}",
        task_description=f"An expert SNN for {args.task}, distilled from {args.teacher_model}.",
        student_config=container.config.model.to_dict()
    )

if __name__ == "__main__":
    asyncio.run(main())
