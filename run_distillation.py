# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: run_distillation.py
# ã‚³ãƒ¼ãƒ‰ã®æœ€ã‚‚æœ€åˆã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¤ºã—ãŸã‚¿ã‚¤ãƒˆãƒ«ã€æ©Ÿèƒ½ã®èª¬æ˜ã‚’è©³ç´°ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚ ä¿®æ­£å†…å®¹ã¯è¨˜è¼‰ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
# Title: çŸ¥è­˜è’¸ç•™å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: KnowledgeDistillationManagerã‚’ä½¿ç”¨ã—ã¦ã€çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ã€‚
#              è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
#
# ä¿®æ­£ (v_async_fix):
# - KnowledgeDistillationManager.prepare_dataset ãŒ async def ã«å¤‰æ›´ã•ã‚ŒãŸã“ã¨ã«ä¼´ã„ã€
#   main() å†…ã§ã®å‘¼ã³å‡ºã—æ™‚ã« await ã‚’è¿½åŠ  (L183)ã€‚
#
# ä¿®æ­£ (v_hpo_fix_attr_error):
# - HPOå®Ÿè¡Œæ™‚ã« KnowledgeDistillationManager (L162) ã«æ¸¡ã•ã‚Œã‚‹ config ãŒ
#   dict ã ã£ãŸãŸã‚ã€OmegaConf.create() ã§ DictConfig ã«å¤‰æ›ã™ã‚‹ã‚ˆã†ä¿®æ­£ã€‚
#
# ä¿®æ­£ (v_hpo_fix_tensor_size_mismatch):
# - OOM (Trial 224, 226) ã®æ ¹æœ¬åŸå› ã§ã‚ã‚‹ã€cifar10 (32x32) ã¨
#   ViT (224x224) ã®ãƒŸã‚¹ãƒãƒƒãƒã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€
#   ã‚¿ã‚¹ã‚¯ãŒ 'cifar10' ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãŒå‚ç…§ã™ã‚‹
#   `config.data.img_size` ã‚‚ 32 ã«ä¸Šæ›¸ãã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’è¿½åŠ  (L129)ã€‚
#
# ä¿®æ­£ (v_hpo_fix_type_error):
# - TypeError (unexpected keyword argument 'img_size') ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã€
#   img_size ã‚’ task.prepare_data() ã§ã¯ãªãã€
#   TaskClass() ã®ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã«æ¸¡ã™ã‚ˆã†ä¿®æ­£ (L226-L233)ã€‚


import argparse
import asyncio
import torch
import torchvision.models as models  # type: ignore[import-untyped]
from torch.utils.data import DataLoader
from omegaconf import OmegaConf, DictConfig
from typing import Any, List, Optional, cast, Dict
import sys 
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ  (run_hpo.py ã¨åŒã˜ä¿®æ­£)
project_root: str = os.path.abspath(os.path.dirname(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from app.containers import TrainingContainer
from snn_research.distillation.knowledge_distillation_manager import KnowledgeDistillationManager
from snn_research.benchmark import TASK_REGISTRY


async def main() -> None:
    parser = argparse.ArgumentParser(description="SNN Knowledge Distillation Runner")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="Base config file path")
    parser.add_argument("--model_config", type=str, default="configs/cifar10_spikingcnn_config.yaml", help="SNN model architecture config file path")
    parser.add_argument("--task", type=str, default="cifar10", help="The benchmark task to distill.")
    parser.add_argument("--teacher_model", type=str, default="resnet18", help="The torchvision teacher model to use.")
    parser.add_argument("--epochs", type=int, default=15, help="Number of distillation epochs.")
    parser.add_argument(
        "--override_config",
        type=str,
        action='append',
        help="Override config (e.g., 'training.epochs=5')"
    )
    args = parser.parse_args()


    container = TrainingContainer()
    
    # 2. åŸºæœ¬è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰
    container.config.from_yaml(args.config)

    # 3. ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ãƒ­ãƒ¼ãƒ‰ (AttributeError ä¿®æ­£)
    #    cifar10_spikingcnn_config.yaml ã«ã¯ 'model:' ã‚­ãƒ¼ãŒãªã„ãŸã‚ã€
    #    'model' ãƒãƒ¼ãƒ‰é…ä¸‹ã«ãƒãƒ¼ã‚¸ã™ã‚‹
    try:
        # --- â–¼ ä¿®æ­£ (v_hpo_fix_3): ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ã¨Exceptãƒ–ãƒ­ãƒƒã‚¯ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ â–¼ ---
        cfg_raw = OmegaConf.load(args.model_config)
        
        # ãƒ­ãƒ¼ãƒ‰ã—ãŸ config ãŒ 'model:' ã‚­ãƒ¼ã‚’ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ã«æŒã£ã¦ã„ã‚‹ã‹ç¢ºèª
        if isinstance(cfg_raw, DictConfig) and 'model' in cfg_raw:
            # æ—¢ã« 'model' ã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆ (spiking_transformer.yaml ãªã©)
            # .model ãƒãƒ¼ãƒ‰ã‚’ç›´æ¥ãƒãƒ¼ã‚¸ã™ã‚‹
            container.config.model.from_dict(
                cast(Dict[str, Any], OmegaConf.to_container(cfg_raw.model, resolve=True))
            )
        elif isinstance(cfg_raw, DictConfig):
            # 'model' ã‚­ãƒ¼ãŒãªã„å ´åˆ (cifar10_spikingcnn_config.yaml ãªã©)
            # è¾æ›¸å…¨ä½“ã‚’ 'model' ã‚­ãƒ¼ã§ãƒ©ãƒƒãƒ—ã—ã¦ãƒãƒ¼ã‚¸ã™ã‚‹
            model_config_dict = OmegaConf.to_container(cfg_raw, resolve=True)
            if isinstance(model_config_dict, dict):
                container.config.from_dict({'model': model_config_dict})
            else:
                 # --- â–¼ ä¿®æ­£: ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ä¿®æ­£ (21 -> 20 spaces) â–¼ ---
                 raise TypeError(f"Model config loaded from {args.model_config} is not a dictionary.")
        else:
             # --- â–¼ ä¿®æ­£: ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ä¿®æ­£ (17 -> 16 spaces) â–¼ ---
             raise TypeError(f"Model config loaded from {args.model_config} is not a dictionary.")
            
    except Exception as e:
        print(f"Warning: Could not load or merge model config '{args.model_config}': {e}")
        # --- â–¼ ä¿®æ­£: å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ except ãƒ–ãƒ­ãƒƒã‚¯å†…ã«å¾©å…ƒ â–¼ ---
        # 'model' ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ç©ºã®è¾æ›¸ã‚’ãƒãƒ¼ã‚¸ã—ã¦ãŠã
        container.config.from_dict({'model': {}})
        # --- â–² ä¿®æ­£ â–² ---


    # 4. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¸Šæ›¸ã
    #    (override_config ã‚ˆã‚Šã‚‚å…ˆã«é©ç”¨)
    container.config.training.epochs.from_value(args.epochs)
    
    # 5. HPOã‹ã‚‰ã® --override_config ã‚’é©ç”¨
    if args.override_config:
        print(f"Applying {len(args.override_config)} overrides from command line...")
        for override in args.override_config:
            try:
                keys, value_str = override.split('=', 1)
                # å‹ã‚’æ¨è«–
                value: Any
                try:
                    value = int(value_str)
                except ValueError:
                    try:
                        value = float(value_str)
                    except ValueError:
                        if value_str.lower() == 'true':
                            value = True
                        elif value_str.lower() == 'false':
                            value = False
                        else:
                            value = value_str  # æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒ

                # ä¿®æ­£: dependency-injector ã® provider API ã‚’ä½¿ã£ã¦ä¸Šæ›¸ã
                key_parts = keys.split('.')
                config_provider = container.config
                for part in key_parts:
                    # providerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¾¿ã‚‹
                    config_provider = getattr(config_provider, part)
                
                # æœ€çµ‚çš„ãª provider ã« .from_value() ã§å€¤ã‚’è¨­å®š
                config_provider.from_value(value)
                print(f"  - Applied: {keys} = {value}")
            except Exception as e:
                print(f"Error applying override '{override}': {e}")
    # --- â–² ä¿®æ­£ â–² ---

    # --- â–¼ ä¿®æ­£ (v_hpo_fix_tensor_size_mismatch) â–¼ ---
    # HPO (spiking_transformer.yaml) ã¨ cifar10 ã‚¿ã‚¹ã‚¯ã®ãƒŸã‚¹ãƒãƒƒãƒã‚’ä¿®æ­£
    if args.task == 'cifar10':
        print("INFO: Overriding data/model config for CIFAR-10 (img_size=32, patch_size=4).")
        
        # 1. ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ•ã‚£ã‚° (SNNCoreãŒèª­ã¿å–ã‚‹) ã‚’ä¸Šæ›¸ã
        try:
            container.config.model.img_size.from_value(32)
            container.config.model.patch_size.from_value(4)
        except Exception as e:
            print(f"Warning: Could not override config.model: {e}")

        # 2. ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒ•ã‚£ã‚° (CIFAR10TaskãŒèª­ã¿å–ã‚‹) ã‚’ä¸Šæ›¸ã
        try:
            if container.config.data.img_size.provided:
                container.config.data.img_size.from_value(32)
            else:
                # å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ (base_configã«data.img_sizeãŒãªã„å ´åˆ)
                container.config.data.from_dict({'img_size': 32})
                
            if container.config.data.patch_size.provided:
                container.config.data.patch_size.from_value(4)
            else:
                # å­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                container.config.data.from_dict({'patch_size': 4})
                
        except Exception as e:
            print(f"Warning: Could not override config.data: {e}")
    # --- â–² ä¿®æ­£ (v_hpo_fix_tensor_size_mismatch) â–² ---


    # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ­£ã—ã„é †åºã§å–å¾—ãƒ»æ§‹ç¯‰
    device = container.device()
    
    # (cfg.model ãŒæ­£ã—ãè¨­å®šã•ã‚ŒãŸãŸã‚ã€SNNCoreã®åˆæœŸåŒ–ãŒæˆåŠŸã™ã‚‹ã¯ãš)
    # --- â–¼ ä¿®æ­£(v7): [arg-type] vocab_size=10 ã‚’æ˜ç¤ºçš„ã«æ¸¡ã™ â–¼ ---
    # (ã‚¿ã‚¹ã‚¯ãŒ "cifar10" ã§ã‚ã‚‹ã“ã¨ãŒå‰æ)
    student_model = container.snn_model(vocab_size=10).to(device)
    # --- â–² ä¿®æ­£(v7) â–² ---
    optimizer = container.optimizer(params=student_model.parameters())
    scheduler = container.scheduler(optimizer=optimizer) if container.config.training.gradient_based.use_scheduler() else None

    # --- æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ ---
    print(f"ğŸ§  Initializing ANN teacher model ({args.teacher_model})...")
    if args.teacher_model == "resnet18":
        teacher_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        # CIFAR-10ç”¨ã«æœ€çµ‚å±¤ã‚’å¤‰æ›´
        num_ftrs = teacher_model.fc.in_features
        teacher_model.fc = torch.nn.Linear(num_ftrs, 10)
    else:
        raise ValueError(f"Unsupported teacher model: {args.teacher_model}")
    teacher_model = teacher_model.to(device)
    teacher_model.eval()

    distillation_trainer = container.distillation_trainer(
        model=student_model,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        rank=-1
    )
    model_registry = container.model_registry()

    # --- â–¼ ä¿®æ­£ (v_hpo_fix_attr_error): dict ã‚’ DictConfig ã«å¤‰æ› â–¼ ---
    # Managerã®åˆæœŸåŒ–ã«å¿…è¦ãªconfigã‚’å–å¾—
    manager_config_dict: Dict[str, Any] = container.config() # ã“ã‚Œã¯ dict ã‚’è¿”ã™
    manager_config_omegaconf: DictConfig = OmegaConf.create(manager_config_dict) # dict -> DictConfig

    manager = KnowledgeDistillationManager(
        student_model=student_model,
        teacher_model=teacher_model,
        trainer=distillation_trainer,
        tokenizer_name=container.config.data.tokenizer_name(), # tokenizerã¯CIFARã‚¿ã‚¹ã‚¯ã§ã¯ä½¿ã‚ã‚Œãªã„ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãŸã‚æ¸¡ã™
        model_registry=model_registry,
        device=device,
        config=manager_config_omegaconf # ä¿®æ­£: DictConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
    )
    # --- â–² ä¿®æ­£ (v_hpo_fix_attr_error) â–² ---

    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ---
    TaskClass = TASK_REGISTRY.get(args.task)
    if not TaskClass:
        raise ValueError(f"Task '{args.task}' not found.")
        
    # --- â–¼ ä¿®æ­£ (v_hpo_fix_type_error): img_size ã‚’ __init__ ã«æ¸¡ã™ â–¼ ---
    task_init_kwargs: Dict[str, Any] = {
        "tokenizer": container.tokenizer(),
        "device": device,
        "hardware_profile": {}
    }
    if args.task == 'cifar10':
        # CIFAR10Task ãŒ img_size ã‚’ __init__ ã§å—ã‘å–ã‚‹ã“ã¨ã‚’æœŸå¾…
        task_init_kwargs['img_size'] = container.config.data.img_size()

    task = TaskClass(**task_init_kwargs)
    # --- â–² ä¿®æ­£ (v_hpo_fix_type_error) â–² ---
    
    
    # --- â–¼ ä¿®æ­£(v_hpo_fix_type_error): kwargs ã‚’å‰Šé™¤ â–¼ ---
    # data_kwargs = {} # å‰Šé™¤
    train_dataset, val_dataset = task.prepare_data(data_dir="data")
    # --- â–² ä¿®æ­£(v_hpo_fix_type_error) â–² ---

    # çŸ¥è­˜è’¸ç•™ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—
    # --- â–¼ ä¿®æ­£ (v_async_fix): await ã‚’è¿½åŠ  â–¼ ---
    train_loader, val_loader = await manager.prepare_dataset(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        collate_fn=task.get_collate_fn(),
        batch_size=container.config.training.batch_size()
    )
    # --- â–² ä¿®æ­£ (v_async_fix) â–² ---

    # è’¸ç•™ã®å®Ÿè¡Œ
    await manager.run_distillation(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=container.config.training.epochs(), # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å–å¾—
        model_id=f"{args.task}_distilled_from_{args.teacher_model}",
        task_description=f"An expert SNN for {args.task}, distilled from {args.teacher_model}.",
        # ä¿®æ­£: model.to_dict() ã§ã¯ãªãã€ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰è¾æ›¸ã‚’å–å¾—
        student_config=container.config.model.to_dict()
    )

if __name__ == "__main__":
    asyncio.run(main())