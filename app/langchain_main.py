# matsushibadenki/snn4/app/langchain_main.py
# LangChainã¨é€£æºã—ãŸSNNãƒãƒ£ãƒƒãƒˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
#
# æ©Ÿèƒ½:
# - ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ— ãƒ•ã‚§ãƒ¼ã‚º2ã€Œ2.4. ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–‹ç™ºã€ã«å¯¾å¿œã€‚
# - DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰SNNLangChainAdapterã‚’å–å¾—ã€‚
# - LangChain Expression Language (LCEL) ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ€ãƒ³ãªãƒã‚§ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã€‚
# - å…±é€šUIãƒ“ãƒ«ãƒ€ãƒ¼é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦UIã‚’æ§‹ç¯‰ãƒ»èµ·å‹•ã™ã‚‹ã€‚
# - --model_config å¼•æ•°ã‚’è¿½åŠ ã—ã€ãƒ™ãƒ¼ã‚¹è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’åˆ†ã‘ã¦èª­ã¿è¾¼ã‚ã‚‹ã‚ˆã†ã«ã—ãŸã€‚
# - ä¿®æ­£: Gradio 4.20.0 ã¨ã®äº’æ›æ€§ã®ãŸã‚ã« .queue() ã‚’å‰Šé™¤ã—ã€app/utils.py çµŒç”±ã§ queue=False ã‚’è¨­å®š
#
# ä¿®æ­£ (v5):
# - mypyã‚¨ãƒ©ãƒ¼ [import-untyped] ã‚’è§£æ¶ˆã€‚

import gradio as gr  # type: ignore[import-untyped]
import argparse
import sys
import time
from pathlib import Path
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
# --- â–¼ ä¿®æ­£: List[List[Optional[str]]] ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ â–¼ ---
from typing import Iterator, Tuple, List, Dict, Optional 
# --- â–² ä¿®æ­£ â–² ---

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent))

from app.containers import AppContainer
from app.utils import build_gradio_ui

def main():
    parser = argparse.ArgumentParser(description="SNN + LangChain é€£æºAIãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")
    parser.add_argument("--config", type=str, default="configs/base_config.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--model_config", type=str, default="configs/models/small.yaml", help="ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹")
    parser.add_argument("--model_path", type=str, help="ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ (è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸Šæ›¸ã)")
    args = parser.parse_args()

    # DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆæœŸåŒ–
    container = AppContainer()
    container.config.from_yaml(args.config)
    container.config.from_yaml(args.model_config)

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ã€è¨­å®šã‚’ä¸Šæ›¸ã
    if args.model_path:
        container.config.model.path.from_value(args.model_path)

    # ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰LangChainã‚¢ãƒ€ãƒ—ã‚¿ã‚’å–å¾—
    snn_llm = container.langchain_adapter()
    print(f"Loading SNN model from: {container.config.model.path()}")
    print("âœ… SNN model loaded and wrapped for LangChain successfully.")

    # LangChain Expression Language (LCEL) ã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ã‚¤ãƒ³ã‚’æ§‹ç¯‰
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ã‚ãªãŸã¯ã€ç°¡æ½”ã§å½¹ç«‹ã¤ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"),
        ("user", "{question}")
    ])
    output_parser = StrOutputParser()
    chain = prompt | snn_llm | output_parser

    # --- â–¼ ä¿®æ­£: history ã®å‹ãƒ’ãƒ³ãƒˆã‚’ List[List[Optional[str]]] ã«å¤‰æ›´ â–¼ ---
    def stream_response(message: str, history: List[List[Optional[str]]]) -> Iterator[Tuple[List[List[Optional[str]]], str]]:
    # --- â–² ä¿®æ­£ â–² ---
        """Gradioã®Blocks UIã®ãŸã‚ã«ã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨çµ±è¨ˆæƒ…å ±ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆã™ã‚‹ã€‚"""
        history.append([message, ""])
        
        print("-" * 30)
        print(f"Input question to LCEL Chain: {message}")
        
        start_time = time.time()
        full_response = ""
        token_count = 0
        
        # LCELãƒã‚§ã‚¤ãƒ³ã®streamãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
        for chunk in chain.stream({"question": message}):
            full_response += chunk
            token_count += 1
            history[-1][1] = full_response
            
            duration = time.time() - start_time
            stats = snn_llm.snn_engine.last_inference_stats
            total_spikes = stats.get("total_spikes", 0)
            spikes_per_second = total_spikes / duration if duration > 0 else 0
            tokens_per_second = token_count / duration if duration > 0 else 0

            stats_md = f"""
            **Inference Time:** `{duration:.2f} s`
            **Tokens/Second:** `{tokens_per_second:.2f}`
            ---
            **Total Spikes:** `{total_spikes:,.0f}`
            **Spikes/Second:** `{spikes_per_second:,.0f}`
            """
            
            yield history, stats_md

        duration = time.time() - start_time
        stats = snn_llm.snn_engine.last_inference_stats
        total_spikes = stats.get("total_spikes", 0)
        print(f"\nGenerated response: {full_response.strip()}")
        print(f"Inference time: {duration:.4f} seconds")
        print(f"Total spikes: {total_spikes:,.0f}")
        print("-" * 30)

    # å…±é€šUIãƒ“ãƒ«ãƒ€ãƒ¼ã‚’ä½¿ç”¨ã—ã¦UIã‚’æ§‹ç¯‰
    demo = build_gradio_ui(
        stream_fn=stream_response,
        title="ğŸ¤– SNN + LangChain Prototype (LCEL)",
        description="""
        SNNãƒ¢ãƒ‡ãƒ«ã‚’LangChain Expression Language (LCEL)çµŒç”±ã§åˆ©ç”¨ã™ã‚‹ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã€‚
        å³å´ã®ãƒ‘ãƒãƒ«ã«ã¯ã€æ¨è«–æ™‚é–“ã‚„ç·ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ãªã©ã®çµ±è¨ˆæƒ…å ±ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
        """,
        chatbot_label="SNN+LangChain Chat",
        theme=gr.themes.Soft(primary_hue="green", secondary_hue="lime")
    )
    
    # Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•
    server_port = container.config.app.server_port() + 1 # ãƒãƒ¼ãƒˆãŒè¡çªã—ãªã„ã‚ˆã†ã«+1ã™ã‚‹
    print("\nStarting Gradio web server for LangChain app...")
    print(f"Please open http://{container.config.app.server_name()}:{server_port} in your browser.")
    # --- â–¼ ä¿®æ­£: .queue() ã‚’å‰Šé™¤ â–¼ ---
    demo.launch(
        server_name=container.config.app.server_name(),
        server_port=server_port,
    )
    # --- â–² ä¿®æ­£ â–² ---

if __name__ == "__main__":
    main()