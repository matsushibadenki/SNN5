# snn_research/bio_models/simple_network.py
# (ä¿®æ­£)
# ä¿®æ­£: learning_rule.update ãŒã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã‚ˆã†ã«ãªã£ãŸãŸã‚ã€
#       æˆ»ã‚Šå€¤ã‚’æ­£ã—ãã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã—ã¦ä½¿ç”¨ã™ã‚‹ã€‚
# ä¿®æ­£: CausalTraceCreditAssignmentEnhancedV2 ã«å¯¾å¿œ
#
# æ”¹å–„ (v2):
# - doc/The-flow-of-brain-behavior.md ãŠã‚ˆã³ doc/ãƒ•ã‚šãƒ­ã‚·ã‚™ã‚§ã‚¯ãƒˆå¼·åŒ–æ¡ˆã®èª¿æŸ».md (ã‚»ã‚¯ã‚·ãƒ§ãƒ³2.3) ã«åŸºã¥ãã€
#   å˜ä¸€ã®å­¦ç¿’å‰‡ã—ã‹æŒã¦ãªã‹ã£ãŸåˆ¶ç´„ã‚’è§£æ¶ˆã€‚
# - ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ãƒ«ãƒ¼ãƒ« (synaptic_rule) ã¨ æ’å¸¸æ€§ç¶­æŒãƒ«ãƒ¼ãƒ« (homeostatic_rule) ã‚’
#   åˆ¥ã€…ã«å—ã‘å–ã‚Šã€ä¸¡æ–¹ã‚’é©ç”¨ã§ãã‚‹ã‚ˆã†ã« __init__ ã¨ update_weights ã‚’å¤‰æ›´ã€‚

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, Tuple, List

from .lif_neuron import BioLIFNeuron
from snn_research.learning_rules.base_rule import BioLearningRule
# --- â–¼ ä¿®æ­£ â–¼ ---
# V2 ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.learning_rules.causal_trace import CausalTraceCreditAssignmentEnhancedV2
# --- â–² ä¿®æ­£ â–² ---


class BioSNN(nn.Module):
    # ... (init, forward ã¯å¤‰æ›´ãªã—) ...
    # --- â–¼ æ”¹å–„ (v2): __init__ ã®ã‚·ã‚°ãƒãƒãƒ£ã‚’å¤‰æ›´ â–¼ ---
    def __init__(
        self, 
        layer_sizes: List[int], 
        neuron_params: dict, 
        synaptic_rule: BioLearningRule, # learning_rule -> synaptic_rule ã«åå‰å¤‰æ›´
        homeostatic_rule: Optional[BioLearningRule] = None, # å®‰å®šåŒ–ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ 
        sparsification_config: Optional[Dict[str, Any]] = None
    ):
    # --- â–² æ”¹å–„ (v2) â–² ---
        super().__init__()
        self.layer_sizes = layer_sizes
        # --- â–¼ æ”¹å–„ (v2): 2ç¨®é¡ã®ãƒ«ãƒ¼ãƒ«ã‚’ä¿æŒ â–¼ ---
        self.synaptic_rule = synaptic_rule
        self.homeostatic_rule = homeostatic_rule
        # --- â–² æ”¹å–„ (v2) â–² ---
        self.sparsification_enabled = sparsification_config.get("enabled", False) if sparsification_config else False
        self.contribution_threshold = sparsification_config.get("contribution_threshold", 0.0) if sparsification_config else 0.0
        if self.sparsification_enabled:
            print(f"ğŸ§¬ é©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ–ãŒæœ‰åŠ¹ã§ã™ (è²¢çŒ®åº¦é–¾å€¤: {self.contribution_threshold})")
        if self.homeostatic_rule:
            print(f"âš–ï¸ æ’å¸¸æ€§ç¶­æŒãƒ«ãƒ¼ãƒ« ({type(self.homeostatic_rule).__name__}) ãŒæœ‰åŠ¹ã§ã™ã€‚")

        self.layers = nn.ModuleList()
        self.weights = nn.ParameterList()
        for i in range(len(layer_sizes) - 1):
            self.layers.append(BioLIFNeuron(layer_sizes[i+1], neuron_params))
            weight = nn.Parameter(torch.rand(layer_sizes[i+1], layer_sizes[i]) * 0.5)
            self.weights.append(weight)

    def forward(self, input_spikes: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:
        hidden_spikes_history = []
        current_spikes = input_spikes
        for i, layer in enumerate(self.layers):
            current = torch.matmul(self.weights[i], current_spikes)
            current_spikes = layer(current)
            hidden_spikes_history.append(current_spikes)
        return current_spikes, hidden_spikes_history


    def update_weights(
        self,
        all_layer_spikes: List[torch.Tensor],
        optional_params: Optional[Dict[str, Any]] = None
    ):
        if not self.training:
            return

        backward_credit: Optional[torch.Tensor] = None
        current_params = optional_params.copy() if optional_params else {}

        for i in reversed(range(len(self.weights))):
            # å…¥åŠ›ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å–å¾—ï¼ˆi=0ã®å ´åˆã¯all_layer_spikes[0]=input_spikesã‚’ä½¿ç”¨ï¼‰
            pre_spikes = all_layer_spikes[i]
            # å‡ºåŠ›ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’å–å¾—ï¼ˆiã«å¯¾å¿œã™ã‚‹å±¤ã®å‡ºåŠ›ã¯ all_layer_spikes[i+1]ï¼‰
            post_spikes = all_layer_spikes[i+1]


            if backward_credit is not None:
                # éšå±¤çš„ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’é©ç”¨
                reward_signal = current_params.get("reward", 0.0)
                # ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã®ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼ˆä¾‹ï¼‰
                modulated_reward = reward_signal + backward_credit.mean().item() * 0.1
                current_params["reward"] = modulated_reward
                # ã¾ãŸã¯ causal_creditã¨ã—ã¦æ¸¡ã™
                # current_params["causal_credit"] = backward_credit.mean().item()

            # --- â–¼ æ”¹å–„ (v2): 2ç¨®é¡ã®å­¦ç¿’å‰‡ã‚’é©ç”¨ â–¼ ---
            
            # 1. ã‚·ãƒŠãƒ—ã‚¹å¯å¡‘æ€§ãƒ«ãƒ¼ãƒ« (STDP, R-STDP, CausalTrace ãªã©)
            dw_synaptic, backward_credit_new = self.synaptic_rule.update(
                pre_spikes=pre_spikes,
                post_spikes=post_spikes,
                weights=self.weights[i],
                optional_params=current_params
            )
            # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã®ãŸã‚ã«ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆä¿¡å·ã‚’æ›´æ–°
            backward_credit = backward_credit_new
            
            # 2. æ’å¸¸æ€§ç¶­æŒãƒ«ãƒ¼ãƒ« (BCM ãªã©)
            dw_homeostasis = torch.zeros_like(self.weights[i].data)
            if self.homeostatic_rule:
                # BCMãªã©ã¯å ±é…¬ä¿¡å·ã‚’å¿…è¦ã¨ã—ãªã„ãŸã‚ã€å…ƒã® optional_params ã‚’æ¸¡ã™
                dw_homeo, _ = self.homeostatic_rule.update(
                    pre_spikes=pre_spikes,
                    post_spikes=post_spikes,
                    weights=self.weights[i],
                    optional_params=optional_params 
                )
                dw_homeostasis = dw_homeo

            # æœ€çµ‚çš„ãªé‡ã¿å¤‰åŒ–é‡ = å¯å¡‘æ€§ + æ’å¸¸æ€§
            dw = dw_synaptic + dw_homeostasis
            # --- â–² æ”¹å–„ (v2) â–² ---

            # --- â–¼ ä¿®æ­£ â–¼ ---
            # é©å¿œçš„å› æœã‚¹ãƒ‘ãƒ¼ã‚¹åŒ– (è²¢çŒ®åº¦ã«åŸºã¥ã)
            # V2 ã‚¯ãƒ©ã‚¹åã«å¤‰æ›´
            if self.sparsification_enabled and isinstance(self.synaptic_rule, CausalTraceCreditAssignmentEnhancedV2):
                # get_causal_contribution ãƒ¡ã‚½ãƒƒãƒ‰ã¯ V2 ã§ã‚‚å­˜åœ¨ã™ã‚‹ã¨ä»®å®š
                causal_contribution = self.synaptic_rule.get_causal_contribution()
                if causal_contribution is not None:
                    # è²¢çŒ®åº¦ãŒé–¾å€¤ä»¥ä¸‹ã®æ¥ç¶šã«å¯¾å¿œã™ã‚‹é‡ã¿æ›´æ–°ã‚’ã‚¼ãƒ­ã«ã™ã‚‹
                    contribution_mask = causal_contribution > self.contribution_threshold
                    dw = dw * contribution_mask
            # --- â–² ä¿®æ­£ â–² ---

            self.weights[i].data += dw
            self.weights[i].data.clamp_(min=0) # ä¾‹: éè² åˆ¶ç´„