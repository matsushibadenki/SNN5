# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: run_distillation.py
# Title: çŸ¥è­˜è’¸ç•™å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# Description: KnowledgeDistillationManagerã‚’ä½¿ç”¨ã—ã¦ã€çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹ã—ã¾ã™ã€‚
#              è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

import argparse
import asyncio
import torch
# ... (ä»–ã®importã¯çœç•¥) ...


async def main() -> None:
    # ... (argparseã€configãƒ­ãƒ¼ãƒ‰ã€override_configé©ç”¨ã¯çœç•¥) ...

    # DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ­£ã—ã„é †åºã§å–å¾—ãƒ»æ§‹ç¯‰
    device = container.device()
    
    # (cfg.model ãŒæ­£ã—ãè¨­å®šã•ã‚ŒãŸãŸã‚ã€SNNCoreã®åˆæœŸåŒ–ãŒæˆåŠŸã™ã‚‹ã¯ãš)
    # --- â–¼ ä¿®æ­£(v7): [arg-type] vocab_size=10 ã‚’æ˜ç¤ºçš„ã«æ¸¡ã™ â–¼ ---
    # (ã‚¿ã‚¹ã‚¯ãŒ "cifar10" ã§ã‚ã‚‹ã“ã¨ãŒå‰æ)
    student_model = container.snn_model(vocab_size=10).to(device)
    # --- â–² ä¿®æ­£(v7) â–² ---
    
    # --- â–¼â–¼â–¼ ã€æœ€å„ªå…ˆä¿®æ­£ã€‘é‡ã¿åˆæœŸåŒ–ã®å¼·åˆ¶ (spike_rate=0ã®æœ€çµ‚é˜²è¡›ç·š) â–¼â–¼â–¼ ---
    def aggressive_init(m: torch.nn.Module):
        """ã™ã¹ã¦ã®Conv/Linearå±¤ã«XavieråˆæœŸåŒ–ã‚’é©ç”¨ã—ã€ç¢ºå®Ÿã«é›»æµã‚’æµã™ã€‚"""
        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):
            # Glorot (Xavier) Uniform initializationã‚’é©ç”¨
            torch.nn.init.xavier_uniform_(m.weight)
            if m.bias is not None:
                torch.nn.init.constant_(m.bias, 0)
    
    print("ğŸ”¥ Forcing aggressive Xavier weight initialization to ensure initial spike activity.")
    student_model.apply(aggressive_init)
    # --- â–²â–²â–² ã€æœ€å„ªå…ˆä¿®æ­£ã€‘é‡ã¿åˆæœŸåŒ–ã®å¼·åˆ¶ (spike_rate=0ã®æœ€çµ‚é˜²è¡›ç·š) â–¼â–¼â–¼ ---
    
    optimizer = container.optimizer(params=student_model.parameters())
    scheduler = container.scheduler(optimizer=optimizer) if container.config.training.gradient_based.use_scheduler() else None
    
    # --- æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ ---
    print(f"ğŸ§  Initializing ANN teacher model ({args.teacher_model})...")
    if args.teacher_model == "resnet18":
        teacher_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
        # CIFAR-10ç”¨ã«æœ€çµ‚å±¤ã‚’å¤‰æ›´
        num_ftrs = teacher_model.fc.in_features
        teacher_model.fc = torch.nn.Linear(num_ftrs, 10)
    else:
        raise ValueError(f"Unsupported teacher model: {args.teacher_model}")
    teacher_model = teacher_model.to(device)
    teacher_model.eval()

    distillation_trainer = container.distillation_trainer(
        model=student_model,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        rank=-1
    )
    model_registry = container.model_registry()

    # --- â–¼ ä¿®æ­£ (v_hpo_fix_attr_error): dict ã‚’ DictConfig ã«å¤‰æ› â–¼ ---
    # Managerã®åˆæœŸåŒ–ã«å¿…è¦ãªconfigã‚’å–å¾—
    manager_config_dict: Dict[str, Any] = container.config() # ã“ã‚Œã¯ dict ã‚’è¿”ã™
    manager_config_omegaconf: DictConfig = OmegaConf.create(manager_config_dict) # dict -> DictConfig

    manager = KnowledgeDistillationManager(
        student_model=student_model,
        teacher_model=teacher_model,
        trainer=distillation_trainer,
        tokenizer_name=container.config.data.tokenizer_name(), # tokenizerã¯CIFARã‚¿ã‚¹ã‚¯ã§ã¯ä½¿ã‚ã‚Œãªã„ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãŸã‚æ¸¡ã™
        model_registry=model_registry,
        device=device,
        config=manager_config_omegaconf # ä¿®æ­£: DictConfig ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
    )
    # --- â–² ä¿®æ­£ (v_hpo_fix_attr_error) â–² ---

    # --- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ ---
    TaskClass = TASK_REGISTRY.get(args.task)
    if not TaskClass:
        raise ValueError(f"Task '{args.task}' not found.")
        
    # --- â–¼ ä¿®æ­£ (v_hpo_fix_type_error): img_size ã‚’ __init__ ã«æ¸¡ã™ â–¼ ---
    task_init_kwargs: Dict[str, Any] = {
        "tokenizer": container.tokenizer(),
        "device": device,
        "hardware_profile": {}
    }
    if args.task == 'cifar10':
        # CIFAR10Task ãŒ img_size ã‚’ __init__ ã§å—ã‘å–ã‚‹ã“ã¨ã‚’æœŸå¾…
        task_init_kwargs['img_size'] = container.config.data.img_size()

    task = TaskClass(**task_init_kwargs)
    # --- â–² ä¿®æ­£ (v_hpo_fix_type_error) â–² ---
    
    
    # --- â–¼ ä¿®æ­£(v_hpo_fix_type_error): kwargs ã‚’å‰Šé™¤ â–¼ ---
    # data_kwargs = {} # å‰Šé™¤
    train_dataset, val_dataset = task.prepare_data(data_dir="data")
    # --- â–² ä¿®æ­£(v_hpo_fix_type_error) â–² ---

    # çŸ¥è­˜è’¸ç•™ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒƒãƒ—
    # --- â–¼ ä¿®æ­£ (v_async_fix): await ã‚’è¿½åŠ  â–¼ ---
    train_loader, val_loader = await manager.prepare_dataset(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        collate_fn=task.get_collate_fn(),
        batch_size=container.config.training.batch_size()
    )
    # --- â–² ä¿®æ­£ (v_async_fix) â–² ---

    # è’¸ç•™ã®å®Ÿè¡Œ
    await manager.run_distillation(
        train_loader=train_loader,
        val_loader=val_loader,
        epochs=container.config.training.epochs(), # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å–å¾—
        model_id=f"{args.task}_distilled_from_{args.teacher_model}",
        task_description=f"An expert SNN for {args.task}, distilled from {args.teacher_model}.",
        # ä¿®æ­£: model.to_dict() ã§ã¯ãªãã€ã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰è¾æ›¸ã‚’å–å¾—
        student_config=container.config.model.to_dict()
    )

if __name__ == "__main__":
    asyncio.run(main())
